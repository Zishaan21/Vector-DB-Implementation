{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qdrant Vectorstore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "import os\n",
    "import qdrant_client\n",
    "\n",
    "os.environ['QDRANT_HOST']= ''\n",
    "os.environ['QDRANT_API_KEY'] = ''\n",
    "os.environ['QDRANT_COLLECTION_NAME'] = ''\n",
    "\n",
    "def get_vector_store():\n",
    "    \n",
    "    \n",
    "    #create a client that will connect to Qdrant resources\n",
    "    client = qdrant_client.QdrantClient(\n",
    "        os.getenv(\"QDRANT_HOST\"),\n",
    "        api_key=os.getenv(\"QDRANT_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    #Create an OpenAIEmbedding Object \n",
    "    embeddings = OpenAIEmbeddings()\n",
    "   \n",
    "    # To create Collection\n",
    "    vectors_config = qdrant_client.http.models.VectorParams(\n",
    "        size = 1536,\n",
    "        distance = qdrant_client.http.models.Distance.COSINE\n",
    "    )\n",
    "    \n",
    "    # To Create New Collection\n",
    "    client.recreate_collection(\n",
    "        collection_name= os.getenv('QDRANT_COLLECTION_NAME'),\n",
    "        vectors_config = vectors_config,\n",
    "    )\n",
    "\n",
    "    #Create a Vector store of collection at cloud \n",
    "    vector_store = Qdrant(\n",
    "        client=client, \n",
    "        collection_name=os.getenv(\"QDRANT_COLLECTION_NAME\"), \n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "\n",
    "# get the vector store\n",
    "vector_store = get_vector_store()\n",
    "\n",
    "\n",
    "#################### create chain \n",
    "user_question = st.text_input(\"Ask a question about your PDF:\")\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "        llm=OpenAI(),\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_store.as_retriever()\n",
    "        )\n",
    "answer = qa.run(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone Vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install All the Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (0.0.350)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from langchain) (2.0.21)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from langchain) (3.9.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.2 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from langchain) (0.0.3)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from langchain) (0.1.1)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from langchain) (0.0.71)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from langchain-core<0.2,>=0.1->langchain) (3.7.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from langchain-core<0.2,>=0.1->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.3.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Collecting pinecone-client\n",
      "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from pinecone-client) (2023.11.17)\n",
      "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
      "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from pinecone-client) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from pinecone-client) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from pinecone-client) (1.26.18)\n",
      "Requirement already satisfied: colorama in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from tqdm>=4.64.1->pinecone-client) (0.4.6)\n",
      "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
      "   ---------------------------------------- 0.0/244.8 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 61.4/244.8 kB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 235.5/244.8 kB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 244.8/244.8 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
      "   ---------------------------------------- 0.0/85.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 85.4/85.4 kB 5.0 MB/s eta 0:00:00\n",
      "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Installing collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone-client\n",
      "Successfully installed pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "   ---------------------------------------- 0.0/298.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/298.0 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/298.0 kB ? eta -:--:--\n",
      "   -------- ------------------------------ 61.4/298.0 kB 656.4 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 225.3/298.0 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 298.0/298.0 kB 1.7 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install pinecone-client\n",
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from openai) (0.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from openai) (1.10.12)\n",
      "Requirement already satisfied: sniffio in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from tiktoken) (2024.7.24)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zishaananwarsayyed\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import All the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1hPQlXrX8FbaYaLypxTmeVOFNitbBMlEE\n",
      "To: /content/pdfs/yolov7paper.pdf\n",
      "100% 2.27M/2.27M [00:00<00:00, 14.6MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1vILwiv6nS2wI3chxNabMgry3qnV67TxM\n",
      "To: /content/pdfs/rachelgreecv.pdf\n",
      "100% 271k/271k [00:00<00:00, 3.62MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown 1hPQlXrX8FbaYaLypxTmeVOFNitbBMlEE -O pdfs/yolov7paper.pdf\n",
    "!gdown 1vILwiv6nS2wI3chxNabMgry3qnV67TxM -O pdfs/rachelgreecv.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the Text from the PDF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\\ndetectors\\nChien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\\n1Institute of Information Science, Academia Sinica, Taiwan\\nkinyiu@iis.sinica.edu.tw, alexeyab84@gmail.com, and liao@iis.sinica.edu.tw\\nAbstract\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS\\nand has the highest accuracy 56.8% AP among all known\\nreal-time object detectors with 30 FPS or higher on GPU\\nV100. YOLOv7-E6 object detector (56 FPS V100, 55.9%\\nAP) outperforms both transformer-based detector SWIN-\\nL Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by\\n509% in speed and 2% in accuracy, and convolutional-\\nbased detector ConvNeXt-XL Cascade-Mask R-CNN (8.6\\nFPS A100, 55.2% AP) by 551% in speed and 0.7% AP\\nin accuracy, as well as YOLOv7 outperforms: YOLOR,\\nYOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable\\nDETR, DINO-5scale-R50, ViT-Adapter-B and many other\\nobject detectors in speed and accuracy. Moreover, we train\\nYOLOv7 only on MS COCO dataset from scratch without\\nusing any other datasets or pre-trained weights. Source\\ncode is released in https://github.com/WongKinYiu/yolov7.\\n1. Introduction\\nReal-time object detection is a very important topic in\\ncomputer vision, as it is often a necessary component in\\ncomputer vision systems. For example, multi-object track-\\ning [94, 93], autonomous driving [40, 18], robotics [35, 58],\\nmedical image analysis [34, 46], etc. The computing de-\\nvices that execute real-time object detection is usually some\\nmobile CPU or GPU, as well as various neural processing\\nunits (NPU) developed by major manufacturers. For exam-\\nple, the Apple neural engine (Apple), the neural compute\\nstick (Intel), Jetson AI edge devices (Nvidia), the edge TPU\\n(Google), the neural processing engine (Qualcomm), the AI\\nprocessing unit (MediaTek), and the AI SoCs (Kneron), are\\nall NPUs. Some of the above mentioned edge devices focus\\non speeding up different operations such as vanilla convolu-\\ntion, depth-wise convolution, or MLP operations. In this pa-\\nper, the real-time object detector we proposed mainly hopes\\nthat it can support both mobile GPU and GPU devices from\\nthe edge to the cloud.\\nIn recent years, the real-time object detector is still de-\\nveloped for different edge device. For example, the devel-\\nFigure 1: Comparison with other real-time object detectors, our\\nproposed methods achieve state-of-the-arts performance.\\nopment of MCUNet [49, 48] and NanoDet [54] focused on\\nproducing low-power single-chip and improving the infer-\\nence speed on edge CPU. As for methods such as YOLOX\\n[21] and YOLOR [81], they focus on improving the infer-\\nence speed of various GPUs. More recently, the develop-\\nment of real-time object detector has focused on the de-\\nsign of efﬁcient architecture. As for real-time object de-\\ntectors that can be used on CPU [54, 88, 84, 83], their de-\\nsign is mostly based on MobileNet [28, 66, 27], ShufﬂeNet\\n[92, 55], or GhostNet [25]. Another mainstream real-time\\nobject detectors are developed for GPU [81, 21, 97], they\\nmostly use ResNet [26], DarkNet [63], or DLA [87], and\\nthen use the CSPNet [80] strategy to optimize the architec-\\nture. The development direction of the proposed methods in\\nthis paper are different from that of the current mainstream\\nreal-time object detectors. In addition to architecture op-\\ntimization, our proposed methods will focus on the opti-\\nmization of the training process. Our focus will be on some\\noptimized modules and optimization methods which may\\nstrengthen the training cost for improving the accuracy of\\nobject detection, but without increasing the inference cost.\\nWe call the proposed modules and optimization methods\\ntrainable bag-of-freebies.\\n1arXiv:2207.02696v1  [cs.CV]  6 Jul 2022', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 0}),\n",
       " Document(page_content='Recently, model re-parameterization [13, 12, 29] and dy-\\nnamic label assignment [20, 17, 42] have become important\\ntopics in network training and object detection. Mainly af-\\nter the above new concepts are proposed, the training of\\nobject detector evolves many new issues. In this paper, we\\nwill present some of the new issues we have discovered and\\ndevise effective methods to address them. For model re-\\nparameterization, we analyze the model re-parameterization\\nstrategies applicable to layers in different networks with the\\nconcept of gradient propagation path, and propose planned\\nre-parameterized model. In addition, when we discover that\\nwith dynamic label assignment technology, the training of\\nmodel with multiple output layers will generate new issues.\\nThat is: “How to assign dynamic targets for the outputs of\\ndifferent branches?” For this problem, we propose a new\\nlabel assignment method called coarse-to-ﬁne lead guided\\nlabel assignment.\\nThe contributions of this paper are summarized as fol-\\nlows: (1) we design several trainable bag-of-freebies meth-\\nods, so that real-time object detection can greatly improve\\nthe detection accuracy without increasing the inference\\ncost; (2) for the evolution of object detection methods, we\\nfound two new issues, namely how re-parameterized mod-\\nule replaces original module, and how dynamic label as-\\nsignment strategy deals with assignment to different output\\nlayers. In addition, we also propose methods to address the\\ndifﬁculties arising from these issues; (3) we propose “ex-\\ntend” and “compound scaling” methods for the real-time\\nobject detector that can effectively utilize parameters and\\ncomputation; and (4) the method we proposed can effec-\\ntively reduce about 40% parameters and 50% computation\\nof state-of-the-art real-time object detector, and has faster\\ninference speed and higher detection accuracy.\\n2. Related work\\n2.1. Real-time object detectors\\nCurrently state-of-the-art real-time object detectors are\\nmainly based on YOLO [61, 62, 63] and FCOS [76, 77],\\nwhich are [3, 79, 81, 21, 54, 85, 23]. Being able to become\\na state-of-the-art real-time object detector usually requires\\nthe following characteristics: (1) a faster and stronger net-\\nwork architecture; (2) a more effective feature integration\\nmethod [22, 97, 37, 74, 59, 30, 9, 45]; (3) a more accurate\\ndetection method [76, 77, 69]; (4) a more robust loss func-\\ntion [96, 64, 6, 56, 95, 57]; (5) a more efﬁcient label assign-\\nment method [99, 20, 17, 82, 42]; and (6) a more efﬁcient\\ntraining method. In this paper, we do not intend to explore\\nself-supervised learning or knowledge distillation methods\\nthat require additional data or large model. Instead, we will\\ndesign new trainable bag-of-freebies method for the issues\\nderived from the state-of-the-art methods associated with\\n(4), (5), and (6) mentioned above.2.2. Model re-parameterization\\nModel re-parametrization techniques [71, 31, 75, 19, 33,\\n11, 4, 24, 13, 12, 10, 29, 14, 78] merge multiple compu-\\ntational modules into one at inference stage. The model\\nre-parameterization technique can be regarded as an en-\\nsemble technique, and we can divide it into two cate-\\ngories, i.e., module-level ensemble and model-level ensem-\\nble. There are two common practices for model-level re-\\nparameterization to obtain the ﬁnal inference model. One\\nis to train multiple identical models with different train-\\ning data, and then average the weights of multiple trained\\nmodels. The other is to perform a weighted average of the\\nweights of models at different iteration number. Module-\\nlevel re-parameterization is a more popular research issue\\nrecently. This type of method splits a module into multi-\\nple identical or different module branches during training\\nand integrates multiple branched modules into a completely\\nequivalent module during inference. However, not all pro-\\nposed re-parameterized module can be perfectly applied to\\ndifferent architectures. With this in mind, we have devel-\\noped new re-parameterization module and designed related\\napplication strategies for various architectures.\\n2.3. Model scaling\\nModel scaling [72, 60, 74, 73, 15, 16, 2, 51] is a way\\nto scale up or down an already designed model and make\\nit ﬁt in different computing devices. The model scaling\\nmethod usually uses different scaling factors, such as reso-\\nlution (size of input image), depth (number of layer), width\\n(number of channel), and stage (number of feature pyra-\\nmid), so as to achieve a good trade-off for the amount of\\nnetwork parameters, computation, inference speed, and ac-\\ncuracy. Network architecture search (NAS) is one of the\\ncommonly used model scaling methods. NAS can automat-\\nically search for suitable scaling factors from search space\\nwithout deﬁning too complicated rules. The disadvantage\\nof NAS is that it requires very expensive computation to\\ncomplete the search for model scaling factors. In [15], the\\nresearcher analyzes the relationship between scaling factors\\nand the amount of parameters and operations, trying to di-\\nrectly estimate some rules, and thereby obtain the scaling\\nfactors required by model scaling. Checking the literature,\\nwe found that almost all model scaling methods analyze in-\\ndividual scaling factor independently, and even the methods\\nin the compound scaling category also optimized scaling\\nfactor independently. The reason for this is because most\\npopular NAS architectures deal with scaling factors that are\\nnot very correlated. We observed that all concatenation-\\nbased models, such as DenseNet [32] or V oVNet [39], will\\nchange the input width of some layers when the depth of\\nsuch models is scaled. Since the proposed architecture is\\nconcatenation-based, we have to design a new compound\\nscaling method for this model.\\n2', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 1}),\n",
       " Document(page_content='Figure 2: Extended efﬁcient layer aggregation networks. The proposed extended ELAN (E-ELAN) does not change the gradient transmis-\\nsion path of the original architecture at all, but use group convolution to increase the cardinality of the added features, and combine the\\nfeatures of different groups in a shufﬂe and merge cardinality manner. This way of operation can enhance the features learned by different\\nfeature maps and improve the use of parameters and calculations.\\n3. Architecture\\n3.1. Extended efﬁcient layer aggregation networks\\nIn most of the literature on designing the efﬁcient ar-\\nchitectures, the main considerations are no more than the\\nnumber of parameters, the amount of computation, and the\\ncomputational density. Starting from the characteristics of\\nmemory access cost, Ma et al. [55] also analyzed the in-\\nﬂuence of the input/output channel ratio, the number of\\nbranches of the architecture, and the element-wise opera-\\ntion on the network inference speed. Doll ´aret al. [15] addi-\\ntionally considered activation when performing model scal-\\ning, that is, to put more consideration on the number of el-\\nements in the output tensors of convolutional layers. The\\ndesign of CSPV oVNet [79] in Figure 2 (b) is a variation of\\nV oVNet [39]. In addition to considering the aforementioned\\nbasic designing concerns, the architecture of CSPV oVNet\\n[79] also analyzes the gradient path, in order to enable the\\nweights of different layers to learn more diverse features.\\nThe gradient analysis approach described above makes in-\\nferences faster and more accurate. ELAN [1] in Figure 2 (c)\\nconsiders the following design strategy – “How to design an\\nefﬁcient network?.” They came out with a conclusion: By\\ncontrolling the shortest longest gradient path, a deeper net-\\nwork can learn and converge effectively. In this paper, we\\npropose Extended-ELAN (E-ELAN) based on ELAN and\\nits main architecture is shown in Figure 2 (d).\\nRegardless of the gradient path length and the stacking\\nnumber of computational blocks in large-scale ELAN, it has\\nreached a stable state. If more computational blocks are\\nstacked unlimitedly, this stable state may be destroyed, and\\nthe parameter utilization rate will decrease. The proposedE-ELAN uses expand, shufﬂe, merge cardinality to achieve\\nthe ability to continuously enhance the learning ability of\\nthe network without destroying the original gradient path.\\nIn terms of architecture, E-ELAN only changes the archi-\\ntecture in computational block, while the architecture of\\ntransition layer is completely unchanged. Our strategy is\\nto use group convolution to expand the channel and car-\\ndinality of computational blocks. We will apply the same\\ngroup parameter and channel multiplier to all the compu-\\ntational blocks of a computational layer. Then, the feature\\nmap calculated by each computational block will be shuf-\\nﬂed into ggroups according to the set group parameter g,\\nand then concatenate them together. At this time, the num-\\nber of channels in each group of feature map will be the\\nsame as the number of channels in the original architec-\\nture. Finally, we add ggroups of feature maps to perform\\nmerge cardinality. In addition to maintaining the original\\nELAN design architecture, E-ELAN can also guide differ-\\nent groups of computational blocks to learn more diverse\\nfeatures.\\n3.2. Model scaling for concatenation-based models\\nThe main purpose of model scaling is to adjust some at-\\ntributes of the model and generate models of different scales\\nto meet the needs of different inference speeds. For ex-\\nample the scaling model of EfﬁcientNet [72] considers the\\nwidth, depth, and resolution. As for the scaled-YOLOv4\\n[79], its scaling model is to adjust the number of stages. In\\n[15], Doll ´aret al. analyzed the inﬂuence of vanilla convolu-\\ntion and group convolution on the amount of parameter and\\ncomputation when performing width and depth scaling, and\\nused this to design the corresponding model scaling method.\\n3', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 2}),\n",
       " Document(page_content='Figure 3: Model scaling for concatenation-based models. From (a) to (b), we observe that when depth scaling is performed on\\nconcatenation-based models, the output width of a computational block also increases. This phenomenon will cause the input width\\nof the subsequent transmission layer to increase. Therefore, we propose (c), that is, when performing model scaling on concatenation-\\nbased models, only the depth in a computational block needs to be scaled, and the remaining of transmission layer is performed with\\ncorresponding width scaling.\\nThe above methods are mainly used in architectures such as\\nPlainNet or ResNet. When these architectures are in execut-\\ning scaling up or scaling down, the in-degree and out-degree\\nof each layer will not change, so we can independently an-\\nalyze the impact of each scaling factor on the amount of\\nparameters and computation. However, if these methods\\nare applied to the concatenation-based architecture, we will\\nﬁnd that when scaling up or scaling down is performed on\\ndepth, the in-degree of a translation layer which is immedi-\\nately after a concatenation-based computational block will\\ndecrease or increase, as shown in Figure 3 (a) and (b).\\nIt can be inferred from the above phenomenon that\\nwe cannot analyze different scaling factors separately for\\na concatenation-based model but must be considered to-\\ngether. Take scaling-up depth as an example, such an ac-\\ntion will cause a ratio change between the input channel and\\noutput channel of a transition layer, which may lead to a de-\\ncrease in the hardware usage of the model. Therefore, we\\nmust propose the corresponding compound model scaling\\nmethod for a concatenation-based model. When we scale\\nthe depth factor of a computational block, we must also cal-\\nculate the change of the output channel of that block. Then,\\nwe will perform width factor scaling with the same amount\\nof change on the transition layers, and the result is shown\\nin Figure 3 (c). Our proposed compound scaling method\\ncan maintain the properties that the model had at the initial\\ndesign and maintains the optimal structure.\\n4. Trainable bag-of-freebies\\n4.1. Planned re-parameterized convolution\\nAlthough RepConv [13] has achieved excellent perfor-\\nmance on the VGG [68], when we directly apply it to\\nResNet [26] and DenseNet [32] and other architectures,\\nits accuracy will be signiﬁcantly reduced. We use gradi-\\nent ﬂow propagation paths to analyze how re-parameterized\\nconvolution should be combined with different network.\\nWe also designed planned re-parameterized convolution ac-\\ncordingly.\\nFigure 4: Planned re-parameterized model. In the proposed\\nplanned re-parameterized model, we found that a layer with resid-\\nual or concatenation connections, its RepConv should not have\\nidentity connection. Under these circumstances, it can be replaced\\nby RepConvN that contains no identity connections.\\nRepConv actually combines 3×3convolution, 1×1\\nconvolution, and identity connection in one convolutional\\nlayer. After analyzing the combination and correspond-\\ning performance of RepConv and different architectures,\\nwe ﬁnd that the identity connection in RepConv destroys\\nthe residual in ResNet and the concatenation in DenseNet,\\nwhich provides more diversity of gradients for different fea-\\nture maps. For the above reasons, we use RepConv with-\\nout identity connection (RepConvN) to design the architec-\\nture of planned re-parameterized convolution. In our think-\\ning, when a convolutional layer with residual or concate-\\nnation is replaced by re-parameterized convolution, there\\nshould be no identity connection. Figure 4 shows an exam-\\nple of our designed “planned re-parameterized convolution”\\nused in PlainNet and ResNet. As for the complete planned\\nre-parameterized convolution experiment in residual-based\\nmodel and concatenation-based model, it will be presented\\nin the ablation study session.\\n4', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 3}),\n",
       " Document(page_content='Figure 5: Coarse for auxiliary and ﬁne for lead head label assigner. Compare with normal model (a), the schema in (b) has auxiliary head.\\nDifferent from the usual independent label assigner (c), we propose (d) lead head guided label assigner and (e) coarse-to-ﬁne lead head\\nguided label assigner. The proposed label assigner is optimized by lead head prediction and the ground truth to get the labels of training\\nlead head and auxiliary head at the same time. The detailed coarse-to-ﬁne implementation method and constraint design details will be\\nelaborated in Apendix.\\n4.2. Coarse for auxiliary and ﬁne for lead loss\\nDeep supervision [38] is a technique that is often used\\nin training deep networks. Its main concept is to add\\nextra auxiliary head in the middle layers of the network,\\nand the shallow network weights with assistant loss as the\\nguide. Even for architectures such as ResNet [26] and\\nDenseNet [32] which usually converge well, deep supervi-\\nsion [70, 98, 67, 47, 82, 65, 86, 50] can still signiﬁcantly\\nimprove the performance of the model on many tasks. Fig-\\nure 5 (a) and (b) show, respectively, the object detector ar-\\nchitecture “without” and “with” deep supervision. In this\\npaper, we call the head responsible for the ﬁnal output as\\nthe lead head, and the head used to assist training is called\\nauxiliary head.\\nNext we want to discuss the issue of label assignment. In\\nthe past, in the training of deep network, label assignment\\nusually refers directly to the ground truth and generate hard\\nlabel according to the given rules. However, in recent years,\\nif we take object detection as an example, researchers often\\nuse the quality and distribution of prediction output by the\\nnetwork, and then consider together with the ground truth to\\nuse some calculation and optimization methods to generate\\na reliable soft label [61, 8, 36, 99, 91, 44, 43, 90, 20, 17, 42].\\nFor example, YOLO [61] use IoU of prediction of bounding\\nbox regression and ground truth as the soft label of object-\\nness. In this paper, we call the mechanism that considers\\nthe network prediction results together with the ground truth\\nand then assigns soft labels as “label assigner.”\\nDeep supervision needs to be trained on the target ob-\\njectives regardless of the circumstances of auxiliary head or\\nlead head. During the development of soft label assigner re-\\nlated techniques, we accidentally discovered a new deriva-\\ntive issue, i.e., “How to assign soft label to auxiliary head\\nand lead head ?” To the best of our knowledge, the relevant\\nliterature has not explored this issue so far. The results of\\nthe most popular method at present is as shown in Figure 5\\n(c), which is to separate auxiliary head and lead head, and\\nthen use their own prediction results and the ground truthto execute label assignment. The method proposed in this\\npaper is a new label assignment method that guides both\\nauxiliary head and lead head by the lead head prediction.\\nIn other words, we use lead head prediction as guidance to\\ngenerate coarse-to-ﬁne hierarchical labels, which are used\\nfor auxiliary head and lead head learning, respectively. The\\ntwo proposed deep supervision label assignment strategies\\nare shown in Figure 5 (d) and (e), respectively.\\nLead head guided label assigner is mainly calculated\\nbased on the prediction result of the lead head and the\\nground truth, and generate soft label through the optimiza-\\ntion process. This set of soft labels will be used as the tar-\\nget training model for both auxiliary head and lead head.\\nThe reason to do this is because lead head has a relatively\\nstrong learning capability, so the soft label generated from it\\nshould be more representative of the distribution and corre-\\nlation between the source data and the target. Furthermore,\\nwe can view such learning as a kind of generalized residual\\nlearning. By letting the shallower auxiliary head directly\\nlearn the information that lead head has learned, lead head\\nwill be more able to focus on learning residual information\\nthat has not yet been learned.\\nCoarse-to-ﬁne lead head guided label assigner also\\nused the predicted result of the lead head and the ground\\ntruth to generate soft label. However, in the process we gen-\\nerate two different sets of soft label, i.e., coarse label and\\nﬁne label, where ﬁne label is the same as the soft label gen-\\nerated by lead head guided label assigner, and coarse label\\nis generated by allowing more grids to be treated as posi-\\ntive target by relaxing the constraints of the positive sample\\nassignment process. The reason for this is that the learning\\nability of an auxiliary head is not as strong as that of a lead\\nhead, and in order to avoid losing the information that needs\\nto be learned, we will focus on optimizing the recall of aux-\\niliary head in the object detection task. As for the output\\nof lead head, we can ﬁlter the high precision results from\\nthe high recall results as the ﬁnal output. However, we must\\nnote that if the additional weight of coarse label is close to\\n5', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 4}),\n",
       " Document(page_content='Table 1: Comparison of baseline object detectors.\\nModel #Param. FLOPs Size APvalAPval\\n50APval\\n75APval\\nSAPval\\nMAPval\\nL\\nYOLOv4 [3] 64.4M 142.8G 640 49.7% 68.2% 54.3% 32.9% 54.8% 63.7%\\nYOLOR-u5 (r6.1) [81] 46.5M 109.1G 640 50.2% 68.7% 54.6% 33.2% 55.5% 63.7%\\nYOLOv4-CSP [79] 52.9M 120.4G 640 50.3% 68.6% 54.9% 34.2% 55.6% 65.1%\\nYOLOR-CSP [81] 52.9M 120.4G 640 50.8% 69.5% 55.3% 33.7% 56.0% 65.4%\\nYOLOv7 36.9M 104.7G 640 51.2% 69.7% 55.5% 35.2% 56.0% 66.7%\\nimprovement -43% -15% - +0.4 +0.2 +0.2 +1.5 = +1.3\\nYOLOR-CSP-X [81] 96.9M 226.8G 640 52.7% 71.3% 57.4% 36.3% 57.5% 68.3%\\nYOLOv7-X 71.3M 189.9G 640 52.9% 71.1% 57.5% 36.9% 57.7% 68.6%\\nimprovement -36% -19% - +0.2 -0.2 +0.1 +0.6 +0.2 +0.3\\nYOLOv4-tiny [79] 6.1 6.9 416 24.9% 42.1% 25.7% 8.7% 28.4% 39.2%\\nYOLOv7-tiny 6.2 5.8 416 35.2% 52.8% 37.3% 15.7% 38.0% 53.4%\\nimprovement +2% -19% - +10.3 +10.7 +11.6 +7.0 +9.6 +14.2\\nYOLOv4-tiny-3l [79] 8.7 5.2 320 30.8% 47.3% 32.2% 10.9% 31.9% 51.5%\\nYOLOv7-tiny 6.2 3.5 320 30.8% 47.3% 32.2% 10.0% 31.9% 52.2%\\nimprovement -39% -49% - = = = -0.9 = +0.7\\nYOLOR-E6 [81] 115.8M 683.2G 1280 55.7% 73.2% 60.7% 40.1% 60.4% 69.2%\\nYOLOv7-E6 97.2M 515.2G 1280 55.9% 73.5% 61.1% 40.6% 60.3% 70.0%\\nimprovement -19% -33% - +0.2 +0.3 +0.4 +0.5 -0.1 +0.8\\nYOLOR-D6 [81] 151.7M 935.6G 1280 56.1% 73.9% 61.2% 42.4% 60.5% 69.9%\\nYOLOv7-D6 154.7M 806.8G 1280 56.3% 73.8% 61.4% 41.3% 60.6% 70.1%\\nYOLOv7-E6E 151.7M 843.2G 1280 56.8% 74.4% 62.1% 40.8% 62.1% 70.6%\\nimprovement = -11% - +0.7 +0.5 +0.9 -1.6 +1.6 +0.7\\nthat of ﬁne label, it may produce bad prior at ﬁnal predic-\\ntion. Therefore, in order to make those extra coarse positive\\ngrids have less impact, we put restrictions in the decoder,\\nso that the extra coarse positive grids cannot produce soft\\nlabel perfectly. The mechanism mentioned above allows\\nthe importance of ﬁne label and coarse label to be dynam-\\nically adjusted during the learning process, and makes the\\noptimizable upper bound of ﬁne label always higher than\\ncoarse label.\\n4.3. Other trainable bag-of-freebies\\nIn this section we will list some trainable bag-of-\\nfreebies. These freebies are some of the tricks we used\\nin training, but the original concepts were not proposed\\nby us. The training details of these freebies will be elab-\\norated in the Appendix, including (1) Batch normalization\\nin conv-bn-activation topology: This part mainly connects\\nbatch normalization layer directly to convolutional layer.\\nThe purpose of this is to integrate the mean and variance\\nof batch normalization into the bias and weight of convolu-\\ntional layer at the inference stage. (2) Implicit knowledge\\nin YOLOR [81] combined with convolution feature map in\\naddition and multiplication manner: Implicit knowledge in\\nYOLOR can be simpliﬁed to a vector by pre-computing at\\nthe inference stage. This vector can be combined with the\\nbias and weight of the previous or subsequent convolutional\\nlayer. (3) EMA model: EMA is a technique used in mean\\nteacher [75], and in our system we use EMA model purely\\nas the ﬁnal inference model.5. Experiments\\n5.1. Experimental setup\\nWe use Microsoft COCO dataset to conduct experiments\\nand validate our object detection method. All our experi-\\nments did not use pre-trained models. That is, all models\\nwere trained from scratch. During the development pro-\\ncess, we used train 2017 set for training, and then used val\\n2017 set for veriﬁcation and choosing hyperparameters. Fi-\\nnally, we show the performance of object detection on the\\ntest 2017 set and compare it with the state-of-the-art object\\ndetection algorithms. Detailed training parameter settings\\nare described in Appendix.\\nWe designed basic model for edge GPU, normal GPU,\\nand cloud GPU, and they are respectively called YOLOv7-\\ntiny, YOLOv7, and YOLOv7-W6. At the same time, we\\nalso use basic model for model scaling for different ser-\\nvice requirements and get different types of models. For\\nYOLOv7, we do stack scaling on neck, and use the pro-\\nposed compound scaling method to perform scaling-up of\\nthe depth and width of the entire model, and use this to ob-\\ntain YOLOv7-X. As for YOLOv7-W6, we use the newly\\nproposed compound scaling method to obtain YOLOv7-E6\\nand YOLOv7-D6. In addition, we use the proposed E-\\nELAN for YOLOv7-E6, and thereby complete YOLOv7-\\nE6E. Since YOLOv7-tiny is an edge GPU-oriented archi-\\ntecture, it will use leaky ReLU as activation function. As\\nfor other models we use SiLU as activation function. We\\nwill describe the scaling factor of each model in detail in\\nAppendix.\\n6', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 5}),\n",
       " Document(page_content='Table 2: Comparison of state-of-the-art real-time object detectors.\\nModel #Param. FLOPs Size FPS APtest/APvalAPtest\\n50APtest\\n75APtest\\nSAPtest\\nMAPtest\\nL\\nYOLOX-S [21] 9.0M 26.8G 640 102 40.5% / 40.5% - - - - -\\nYOLOX-M [21] 25.3M 73.8G 640 81 47.2% / 46.9% - - - - -\\nYOLOX-L [21] 54.2M 155.6G 640 69 50.1% / 49.7% - - - - -\\nYOLOX-X [21] 99.1M 281.9G 640 58 51.5% / 51.1% - - - - -\\nPPYOLOE-S [85] 7.9M 17.4G 640 208 43.1% / 42.7% 60.5% 46.6% 23.2% 46.4% 56.9%\\nPPYOLOE-M [85] 23.4M 49.9G 640 123 48.9% / 48.6% 66.5% 53.0% 28.6% 52.9% 63.8%\\nPPYOLOE-L [85] 52.2M 110.1G 640 78 51.4% / 50.9% 68.9% 55.6% 31.4% 55.3% 66.1%\\nPPYOLOE-X [85] 98.4M 206.6G 640 45 52.2% / 51.9% 69.9% 56.5% 33.3% 56.3% 66.4%\\nYOLOv5-N (r6.1) [23] 1.9M 4.5G 640 159 - / 28.0% - - - - -\\nYOLOv5-S (r6.1) [23] 7.2M 16.5G 640 156 - / 37.4% - - - - -\\nYOLOv5-M (r6.1) [23] 21.2M 49.0G 640 122 - / 45.4% - - - - -\\nYOLOv5-L (r6.1) [23] 46.5M 109.1G 640 99 - / 49.0% - - - - -\\nYOLOv5-X (r6.1) [23] 86.7M 205.7G 640 83 - / 50.7% - - - - -\\nYOLOR-CSP [81] 52.9M 120.4G 640 106 51.1% / 50.8% 69.6% 55.7% 31.7% 55.3% 64.7%\\nYOLOR-CSP-X [81] 96.9M 226.8G 640 87 53.0% / 52.7% 71.4% 57.9% 33.7% 57.1% 66.8%\\nYOLOv7-tiny-SiLU 6.2M 13.8G 640 286 38.7% / 38.7% 56.7% 41.7% 18.8% 42.4% 51.9%\\nYOLOv7 36.9M 104.7G 640 161 51.4% / 51.2% 69.7% 55.9% 31.8% 55.5% 65.0%\\nYOLOv7-X 71.3M 189.9G 640 114 53.1% / 52.9% 71.2% 57.8% 33.8% 57.1% 67.4%\\nYOLOv5-N6 (r6.1) [23] 3.2M 18.4G 1280 123 - / 36.0% - - - - -\\nYOLOv5-S6 (r6.1) [23] 12.6M 67.2G 1280 122 - / 44.8% - - - - -\\nYOLOv5-M6 (r6.1) [23] 35.7M 200.0G 1280 90 - / 51.3% - - - - -\\nYOLOv5-L6 (r6.1) [23] 76.8M 445.6G 1280 63 - / 53.7% - - - - -\\nYOLOv5-X6 (r6.1) [23] 140.7M 839.2G 1280 38 - / 55.0% - - - - -\\nYOLOR-P6 [81] 37.2M 325.6G 1280 76 53.9% / 53.5% 71.4% 58.9% 36.1% 57.7% 65.6%\\nYOLOR-W6 [81] 79.8G 453.2G 1280 66 55.2% / 54.8% 72.7% 60.5% 37.7% 59.1% 67.1%\\nYOLOR-E6 [81] 115.8M 683.2G 1280 45 55.8% / 55.7% 73.4% 61.1% 38.4% 59.7% 67.7%\\nYOLOR-D6 [81] 151.7M 935.6G 1280 34 56.5% / 56.1% 74.1% 61.9% 38.9% 60.4% 68.7%\\nYOLOv7-W6 70.4M 360.0G 1280 84 54.9% / 54.6% 72.6% 60.1% 37.3% 58.7% 67.1%\\nYOLOv7-E6 97.2M 515.2G 1280 56 56.0% / 55.9% 73.5% 61.2% 38.0% 59.9% 68.4%\\nYOLOv7-D6 154.7M 806.8G 1280 44 56.6% / 56.3% 74.0% 61.8% 38.8% 60.1% 69.5%\\nYOLOv7-E6E 151.7M 843.2G 1280 36 56.8% / 56.8% 74.4% 62.1% 39.3% 60.5% 69.0%\\n1Our FLOPs is calaculated by rectangle input resolution like 640 ×640 or 1280 ×1280.\\n2Our inference time is estimated by using letterbox resize input image to make its long side equals to 640 or 1280.\\n5.2. Baselines\\nWe choose previous version of YOLO [3, 79] and state-\\nof-the-art object detector YOLOR [81] as our baselines. Ta-\\nble 1 shows the comparison of our proposed YOLOv7 mod-\\nels and those baseline that are trained with the same settings.\\nFrom the results we see that if compared with YOLOv4,\\nYOLOv7 has 75% less parameters, 36% less computation,\\nand brings 1.5% higher AP. If compared with state-of-the-\\nart YOLOR-CSP, YOLOv7 has 43% fewer parameters, 15%\\nless computation, and 0.4% higher AP. In the performance\\nof tiny model, compared with YOLOv4-tiny-31, YOLOv7-\\ntiny reduces the number of parameters by 39% and the\\namount of computation by 49%, but maintains the same AP.\\nOn the cloud GPU model, our model can still have a higher\\nAP while reducing the number of parameters by 19% and\\nthe amount of computation by 33%.5.3. Comparison with state-of-the-arts\\nWe compare the proposed method with state-of-the-art\\nobject detectors for general GPUs and Mobile GPUs, and\\nthe results are shown in Table 2. From the results in\\nTable 2 we know that the proposed method has the best\\nspeed-accuracy trade-off comprehensively. If we compare\\nYOLOv7-tiny-SiLU with YOLOv5-N (r6.1), our method\\nis 127 fps faster and 10.7% more accurate on AP. In ad-\\ndition, YOLOv7 has 51.4% AP at frame rate of 161 fps,\\nwhile PPYOLOE-L with the same AP has only 78 fps frame\\nrate. In terms of parameter usage, YOLOv7 is 41% less than\\nPPYOLOE-L. If we compare YOLOv7-X with 114 fps in-\\nference speed to YOLOv5-L (r6.1) with 99 fps inference\\nspeed, YOLOv7-X can improve AP by 3.9%. If YOLOv7-\\nX is compared with YOLOv5-X (r6.1) of similar scale, the\\ninference speed of YOLOv7-X is 31 fps faster. In addi-\\ntion, in terms of the amount of parameters and computation,\\nYOLOv7-X reduces 22% of parameters and 8% of compu-\\ntation compared to YOLOv5-X (r6.1), but improves AP by\\n2.2%.\\n7', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 6}),\n",
       " Document(page_content='If we compare YOLOv7 with YOLOR using the input\\nresolution 1280, the inference speed of YOLOv7-W6 is 8\\nfps faster than that of YOLOR-P6, and the detection rate is\\nalso increased by 1% AP. As for the comparison between\\nYOLOv7-E6 and YOLOv5-X6 (r6.1), the former has 0.9%\\nAP gain than the latter, 45% less parameters and 63% less\\ncomputation, and the inference speed is increased by 47%.\\nYOLOv7-D6 has close inference speed to YOLOR-E6, but\\nimproves AP by 0.8%. YOLOv7-E6E has close inference\\nspeed to YOLOR-D6, but improves AP by 0.3%.\\n5.4. Ablation study\\n5.4.1 Proposed compound scaling method\\nTable 3 shows the results obtained when using different\\nmodel scaling strategies for scaling up. Among them, our\\nproposed compound scaling method is to scale up the depth\\nof computational block by 1.5 times and the width of tran-\\nsition block by 1.25 times. If our method is compared with\\nthe method that only scaled up the width, our method can\\nimprove the AP by 0.5% with less parameters and amount\\nof computation. If our method is compared with the method\\nthat only scales up the depth, our method only needs to in-\\ncrease the number of parameters by 2.9% and the amount of\\ncomputation by 1.2%, which can improve the AP by 0.2%.\\nIt can be seen from the results of Table 3 that our proposed\\ncompound scaling strategy can utilize parameters and com-\\nputation more efﬁciently.\\nTable 3: Ablation study on proposed model scaling.\\nModel #Param. FLOPs Size APvalAPval\\n50APval\\n75\\nbase (v7-X light) 47.0M 125.5G 640 51.7% 70.1% 56.0%\\nwidth only (1.25 w)73.4M 195.5G 640 52.4% 70.9% 57.1%\\ndepth only (2.0 d) 69.3M 187.6G 640 52.7% 70.8% 57.3%\\ncompound (v7-X) 71.3M 189.9G 640 52.9% 71.1% 57.5%\\nimprovement - - - +1.2 +1.0 +1.5\\n5.4.2 Proposed planned re-parameterized model\\nIn order to verify the generality of our proposed planed\\nre-parameterized model, we use it on concatenation-based\\nmodel and residual-based model respectively for veriﬁca-\\ntion. The concatenation-based model and residual-based\\nmodel we chose for veriﬁcation are 3-stacked ELAN and\\nCSPDarknet, respectively.\\nIn the experiment of concatenation-based model, we re-\\nplace the 3×3convolutional layers in different positions in\\n3-stacked ELAN with RepConv, and the detailed conﬁgura-\\ntion is shown in Figure 6. From the results shown in Table 4\\nwe see that all higher AP values are present on our proposed\\nplanned re-parameterized model.\\nIn the experiment dealing with residual-based model,\\nsince the original dark block does not have a 3×3con-\\nFigure 6: Planned RepConv 3-stacked ELAN. Blue circles are the\\nposition we replace Conv by RepConv.\\nTable 4: Ablation study on planned RepConcatenation model.\\nModel APvalAPval\\n50APval\\n75APval\\nSAPval\\nMAPval\\nL\\nbase (3-S ELAN) 52.26% 70.41% 56.77% 35.81% 57.00% 67.59%\\nFigure 6 (a) 52.18% 70.34% 56.90% 35.71% 56.83% 67.51%\\nFigure 6 (b) 52.30% 70.30% 56.92% 35.76% 56.95% 67.74%\\nFigure 6 (c) 52.33% 70.56% 56.91% 35.90% 57.06% 67.50%\\nFigure 6 (d) 52.17% 70.32% 56.82% 35.33% 57.06% 68.09%\\nFigure 6 (e) 52.23% 70.20% 56.81% 35.34% 56.97% 66.88%\\nvolution block that conforms to our design strategy, we ad-\\nditionally design a reversed dark block for the experiment,\\nwhose architecture is shown in Figure 7. Since the CSP-\\nDarknet with dark block and reversed dark block has exactly\\nthe same amount of parameters and operations, it is fair to\\ncompare. The experiment results illustrated in Table 5 fully\\nconﬁrm that the proposed planned re-parameterized model\\nis equally effective on residual-based model. We ﬁnd that\\nthe design of RepCSPResNet [85] also ﬁt our design pat-\\ntern.\\nFigure 7: Reversed CSPDarknet. We reverse the position of 1×1\\nand3×3convolutional layer in dark block to ﬁt our planned re-\\nparameterized model design strategy.\\nTable 5: Ablation study on planned RepResidual model.\\nModel APvalAPval\\n50APval\\n75APval\\nSAPval\\nMAPval\\nL\\nbase (YOLOR-W6) 54.82% 72.39% 59.95% 39.68% 59.38% 68.30%\\nRepCSP 54.67% 72.50% 59.58% 40.22% 59.61% 67.87%\\nRCSP 54.36% 71.95% 59.54% 40.15% 59.02% 67.44%\\nRepRCSP 54.85% 72.51% 60.08% 40.53% 59.52% 68.06%\\nbase (YOLOR-CSP) 50.81% 69.47% 55.28% 33.74% 56.01% 65.38%\\nRepRCSP 50.91% 69.54% 55.55% 34.44% 55.74% 65.46%\\n8', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 7}),\n",
       " Document(page_content='Figure 8: Objectness map predicted by different methods at auxiliary head and lead head.\\n5.4.3 Proposed assistant loss for auxiliary head\\nIn the assistant loss for auxiliary head experiments, we com-\\npare the general independent label assignment for lead head\\nand auxiliary head methods, and we also compare the two\\nproposed lead guided label assignment methods. We show\\nall comparison results in Table 6. From the results listed in\\nTable 6, it is clear that any model that increases assistant\\nloss can signiﬁcantly improve the overall performance. In\\naddition, our proposed lead guided label assignment strat-\\negy receives better performance than the general indepen-\\ndent label assignment strategy in AP, AP 50, and AP 75. As\\nfor our proposed coarse for assistant and ﬁne for lead label\\nassignment strategy, it results in best results in all cases. In\\nFigure 8 we show the objectness map predicted by different\\nmethods at auxiliary head and lead head. From Figure 8 we\\nﬁnd that if auxiliary head learns lead guided soft label, it\\nwill indeed help lead head to extract the residual informa-\\ntion from the consistant targets.\\nTable 6: Ablation study on proposed auxiliary head.\\nModel Size APvalAPval\\n50APval\\n75\\nbase (v7-E6) 1280 55.6% 73.2% 60.7%\\nindependent 1280 55.8% 73.4% 60.9%\\nlead guided 1280 55.9% 73.5% 61.0%\\ncoarse-to-ﬁne lead guided 1280 55.9% 73.5% 61.1%\\nimprovement - +0.3 +0.3 +0.4\\nIn Table 7 we further analyze the effect of the proposed\\ncoarse-to-ﬁne lead guided label assignment method on the\\ndecoder of auxiliary head. That is, we compared the results\\nof with/without the introduction of upper bound constraint.\\nJudging from the numbers in the Table, the method of con-\\nstraining the upper bound of objectness by the distance from\\nthe center of the object can achieve better performance.\\nTable 7: Ablation study on constrained auxiliary head.\\nModel Size APvalAPval\\n50APval\\n75\\nbase (v7-E6) 1280 55.6% 73.2% 60.7%\\naux without constraint 1280 55.9% 73.5% 61.0%\\naux with constraint 1280 55.9% 73.5% 61.1%\\nimprovement - +0.3 +0.3 +0.4Since the proposed YOLOv7 uses multiple pyramids to\\njointly predict object detection results, we can directly con-\\nnect auxiliary head to the pyramid in the middle layer for\\ntraining. This type of training can make up for informa-\\ntion that may be lost in the next level pyramid prediction.\\nFor the above reasons, we designed partial auxiliary head\\nin the proposed E-ELAN architecture. Our approach is to\\nconnect auxiliary head after one of the sets of feature map\\nbefore merging cardinality, and this connection can make\\nthe weight of the newly generated set of feature map not\\ndirectly updated by assistant loss. Our design allows each\\npyramid of lead head to still get information from objects\\nwith different sizes. Table 8 shows the results obtained us-\\ning two different methods, i.e., coarse-to-ﬁne lead guided\\nand partial coarse-to-ﬁne lead guided methods. Obviously,\\nthe partial coarse-to-ﬁne lead guided method has a better\\nauxiliary effect.\\nTable 8: Ablation study on partial auxiliary head.\\nModel Size APvalAPval\\n50APval\\n75\\nbase (v7-E6E) 1280 56.3% 74.0% 61.5%\\naux 1280 56.5% 74.0% 61.6%\\npartial aux 1280 56.8% 74.4% 62.1%\\nimprovement - +0.5 +0.4 +0.6\\n6. Conclusions\\nIn this paper we propose a new architecture of real-\\ntime object detector and the corresponding model scaling\\nmethod. Furthermore, we ﬁnd that the evolving process\\nof object detection methods generates new research top-\\nics. During the research process, we found the replace-\\nment problem of re-parameterized module and the alloca-\\ntion problem of dynamic label assignment. To solve the\\nproblem, we propose the trainable bag-of-freebies method\\nto enhance the accuracy of object detection. Based on the\\nabove, we have developed the YOLOv7 series of object de-\\ntection systems, which receives the state-of-the-art results.\\n7. Acknowledgements\\nThe authors wish to thank National Center for High-\\nperformance Computing (NCHC) for providing computa-\\ntional and storage resources.\\n9', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 8}),\n",
       " Document(page_content='Table 9: More comparison (batch=1, no-TRT, without extra object detection training data)\\nModel #Param. FLOPs Size FPSV100APtest/APvalAPtest\\n50APtest\\n75\\nYOLOv7-tiny-SiLU 6.2M 13.8G 640 286 38.7% /38.7% 56.7% 41.7%\\nPPYOLOE-S [85] 7.9M 17.4G 640 208 43.1% /42.7% 60.5% 46.6%\\nYOLOv7 36.9M 104.7G 640 161 51.4% /51.2% 69.7% 55.9%\\nYOLOv5-N (r6.1) [23] 1.9M 4.5G 640 159 - / 28.0% - -\\nYOLOv5-S (r6.1) [23] 7.2M 16.5G 640 156 - / 37.4% - -\\nPPYOLOE-M [85] 23.4M 49.9G 640 123 48.9% / 48.6% 66.5% 53.0%\\nYOLOv5-N6 (r6.1) [23] 3.2M 18.4G 1280 123 - / 36.0% - -\\nYOLOv5-S6 (r6.1) [23] 12.6M 67.2G 1280 122 - / 44.8% - -\\nYOLOv5-M (r6.1) [23] 21.2M 49.0G 640 122 - / 45.4% - -\\nYOLOv7-X 71.3M 189.9G 640 114 53.1% /52.9% 71.2% 57.8%\\nYOLOR-CSP [81] 52.9M 120.4G 640 106 51.1% / 50.8% 69.6% 55.7%\\nYOLOX-S [21] 9.0M 26.8G 640 102 40.5% / 40.5% - -\\nYOLOv5-L (r6.1) [23] 46.5M 109.1G 640 99 - / 49.0% - -\\nYOLOv5-M6 (r6.1) [23] 35.7M 200.0G 1280 90 - / 51.3% - -\\nYOLOR-CSP-X [81] 96.9M 226.8G 640 87 53.0% / 52.7% 71.4% 57.9%\\nYOLOv7-W6 70.4M 360.0G 1280 84 54.9% /54.6% 72.6% 60.1%\\nYOLOv5-X (r6.1) [23] 86.7M 205.7G 640 83 - / 50.7% - -\\nYOLOX-M [21] 25.3M 73.8G 640 81 47.2% / 46.9% - -\\nPPYOLOE-L [85] 52.2M 110.1G 640 78 51.4% / 50.9% 68.9% 55.6%\\nYOLOR-P6 [81] 37.2M 325.6G 1280 76 53.9% / 53.5% 71.4% 58.9%\\nYOLOX-L [21] 54.2M 155.6G 640 69 50.1% / 49.7% - -\\nYOLOR-W6 [81] 79.8G 453.2G 1280 66 55.2% /54.8% 72.7% 60.5%\\nYOLOv5-L6 (r6.1) [23] 76.8M 445.6G 1280 63 - / 53.7% - -\\nYOLOX-X [21] 99.1M 281.9G 640 58 51.5% / 51.1% - -\\nYOLOv7-E6 97.2M 515.2G 1280 56 56.0% /55.9% 73.5% 61.2%\\nYOLOR-E6 [81] 115.8M 683.2G 1280 45 55.8% / 55.7% 73.4% 61.1%\\nPPYOLOE-X [85] 98.4M 206.6G 640 45 52.2% / 51.9% 69.9% 56.5%\\nYOLOv7-D6 154.7M 806.8G 1280 44 56.6% /56.3% 74.0% 61.8%\\nYOLOv5-X6 (r6.1) [23] 140.7M 839.2G 1280 38 - / 55.0% - -\\nYOLOv7-E6E 151.7M 843.2G 1280 36 56.8% /56.8% 74.4% 62.1%\\nYOLOR-D6 [81] 151.7M 935.6G 1280 34 56.5% / 56.1% 74.1% 61.9%\\nF-RCNN-R101-FPN+ [5] 60.0M 246.0G 1333 20 - / 44.0% - -\\nDeformable DETR [100] 40.0M 173.0G - 19 - / 46.2% - -\\nSwin-B (C-M-RCNN) [52] 145.0M 982.0G 1333 11.6 - / 51.9% - -\\nDETR DC5-R101 [5] 60.0M 253.0G 1333 10 - / 44.9% - -\\nEfﬁcientDet-D7x [74] 77.0M 410.0G 1536 6.5 55.1% / 54.4% 72.4% 58.4%\\nDual-Swin-T (C-M-RCNN) [47] 113.8M 836.0G 1333 6.5 - / 53.6% - -\\nViT-Adapter-B [7] 122.0M 997.0G - 4.4 - / 50.8% - -\\nDual-Swin-B (HTC) [47] 235.0M - 1600 2.5 58.7% /58.4% - -\\nDual-Swin-L (HTC) [47] 453.0M - 1600 1.5 59.4% /59.1% - -\\nModel #Param. FLOPs Size FPSA100APtest/APvalAPtest\\n50APtest\\n75\\nDN-Deformable-DETR [41] 48.0M 265.0G 1333 23.0 - / 48.6% - -\\nConvNeXt-B (C-M-RCNN) [53] - 964.0G 1280 11.5 - / 54.0% 73.1% 58.8%\\nSwin-B (C-M-RCNN) [52] - 982.0G 1280 10.7 - / 53.0% 71.8% 57.5%\\nDINO-5scale (R50) [89] 47.0M 860.0G 1333 10.0 - / 51.0% - -\\nConvNeXt-L (C-M-RCNN) [53] - 1354.0G 1280 10.0 - / 54.8% 73.8% 59.8%\\nSwin-L (C-M-RCNN) [52] - 1382.0G 1280 9.2 - / 53.9% 72.4% 58.8%\\nConvNeXt-XL (C-M-RCNN) [53] - 1898.0G 1280 8.6 - / 55.2% 74.2% 59.9%\\n8. More comparison\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS and\\nhas the highest accuracy 56.8% AP test-dev / 56.8% AP\\nmin-val among all known real-time object detectors with 30\\nFPS or higher on GPU V100. YOLOv7-E6 object detector\\n(56 FPS V100, 55.9% AP) outperforms both transformer-\\nbased detector SWIN-L Cascade-Mask R-CNN (9.2 FPS\\nA100, 53.9% AP) by 509% in speed and 2% in accuracy,and convolutional-based detector ConvNeXt-XL Cascade-\\nMask R-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed\\nand 0.7% AP in accuracy, as well as YOLOv7 outperforms:\\nYOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, De-\\nformable DETR, DINO-5scale-R50, ViT-Adapter-B and\\nmany other object detectors in speed and accuracy. More\\nover, we train YOLOv7 only on MS COCO dataset from\\nscratch without using any other datasets or pre-trained\\nweights.\\n10', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 9}),\n",
       " Document(page_content='Figure 9: Comparison with other object detectors.\\nFigure 10: Comparison with other real-time object detectors.\\nTable 10: Comparison of different setting.\\nModel Presicion IoU threshold APval\\nYOLOv7-X FP16 (default) 0.65 (default) 52.9%\\nYOLOv7-X FP32 0.65 53.0%\\nYOLOv7-X FP16 0.70 53.0%\\nYOLOv7-X FP32 0.70 53.1%\\nimprovement - - +0.2%\\n*Similar to meituan/YOLOv6 and PPYOLOE, our model could\\nget higher AP when set higher IoU threshold.\\nThe maximum accuracy of the YOLOv7-E6E (56.8%\\nAP) real-time model is +13.7% AP higher than the cur-\\nrent most accurate meituan/YOLOv6-s model (43.1% AP)\\non COCO dataset. Our YOLOv7-tiny (35.2% AP, 0.4\\nms) model is +25% faster and +0.2% AP higher than\\nmeituan/YOLOv6-n (35.0% AP, 0.5 ms) under identical\\nconditions on COCO dataset and V100 GPU with batch=32.\\nFigure 11: Comparison with other real-time object detectors.\\n11', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 10}),\n",
       " Document(page_content='References\\n[1] anonymous. Designing network design strategies. anony-\\nmous submission , 2022. 3\\n[2] Irwan Bello, William Fedus, Xianzhi Du, Ekin Dogus\\nCubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens,\\nand Barret Zoph. Revisiting ResNets: Improved training\\nand scaling strategies. Advances in Neural Information Pro-\\ncessing Systems (NeurIPS) , 34, 2021. 2\\n[3] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-\\nYuan Mark Liao. YOLOv4: Optimal speed and accuracy of\\nobject detection. arXiv preprint arXiv:2004.10934 , 2020.\\n2, 6, 7\\n[4] Yue Cao, Thomas Andrew Geddes, Jean Yee Hwa Yang,\\nand Pengyi Yang. Ensemble deep learning in bioinformat-\\nics.Nature Machine Intelligence , 2(9):500–508, 2020. 2\\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\\nEnd-to-end object detection with transformers. In Pro-\\nceedings of the European Conference on Computer Vision\\n(ECCV) , pages 213–229, 2020. 10\\n[6] Kean Chen, Weiyao Lin, Jianguo Li, John See, Ji Wang, and\\nJunni Zou. AP-loss for accurate one-stage object detection.\\nIEEE Transactions on Pattern Analysis and Machine Intel-\\nligence (TPAMI) , 43(11):3782–3798, 2020. 2\\n[7] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong\\nLu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for\\ndense predictions. arXiv preprint arXiv:2205.08534 , 2022.\\n10\\n[8] Jiwoong Choi, Dayoung Chun, Hyun Kim, and Hyuk-Jae\\nLee. Gaussian YOLOv3: An accurate and fast object detec-\\ntor using localization uncertainty for autonomous driving.\\nInProceedings of the IEEE/CVF International Conference\\non Computer Vision (ICCV) , pages 502–511, 2019. 5\\n[9] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen,\\nMengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head:\\nUnifying object detection heads with attentions. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition (CVPR) , pages 7373–7382, 2021.\\n2\\n[10] Xiaohan Ding, Honghao Chen, Xiangyu Zhang, Kaiqi\\nHuang, Jungong Han, and Guiguang Ding. Re-\\nparameterizing your optimizers rather than architectures.\\narXiv preprint arXiv:2205.15242 , 2022. 2\\n[11] Xiaohan Ding, Yuchen Guo, Guiguang Ding, and Jungong\\nHan. ACNet: Strengthening the kernel skeletons for pow-\\nerful CNN via asymmetric convolution blocks. In Proceed-\\nings of the IEEE/CVF International Conference on Com-\\nputer Vision (ICCV) , pages 1911–1920, 2019. 2\\n[12] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and\\nGuiguang Ding. Diverse branch block: Building a con-\\nvolution as an inception-like unit. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 10886–10895, 2021. 2\\n[13] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong\\nHan, Guiguang Ding, and Jian Sun. RepVGG: Making\\nVGG-style convnets great again. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 13733–13742, 2021. 2, 4\\n[14] Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong\\nHan, Guiguang Ding, and Jian Sun. Scaling up your ker-\\nnels to 31x31: Revisiting large kernel design in CNNs. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition (CVPR) , 2022. 2\\n[15] Piotr Doll ´ar, Mannat Singh, and Ross Girshick. Fast and\\naccurate model scaling. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 924–932, 2021. 2, 3\\n[16] Xianzhi Du, Barret Zoph, Wei-Chih Hung, and Tsung-Yi\\nLin. Simple training strategies and model scaling for object\\ndetection. arXiv preprint arXiv:2107.00057 , 2021. 2\\n[17] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott,\\nand Weilin Huang. TOOD: Task-aligned one-stage object\\ndetection. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision (ICCV) , pages 3490–3499,\\n2021. 2, 5\\n[18] Di Feng, Christian Haase-Sch ¨utz, Lars Rosenbaum, Heinz\\nHertlein, Claudius Glaeser, Fabian Timm, Werner Wies-\\nbeck, and Klaus Dietmayer. Deep multi-modal object de-\\ntection and semantic segmentation for autonomous driv-\\ning: Datasets, methods, and challenges. IEEE Transac-\\ntions on Intelligent Transportation Systems , 22(3):1341–\\n1360, 2020. 1\\n[19] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin,\\nDmitry P Vetrov, and Andrew G Wilson. Loss sur-\\nfaces, mode connectivity, and fast ensembling of DNNs.\\nAdvances in Neural Information Processing Systems\\n(NeurIPS) , 31, 2018. 2\\n[20] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and\\nJian Sun. OTA: Optimal transport assignment for object\\ndetection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n303–312, 2021. 2, 5\\n[21] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\\nSun. YOLOX: Exceeding YOLO series in 2021. arXiv\\npreprint arXiv:2107.08430 , 2021. 1, 2, 7, 10\\n[22] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. NAS-FPN:\\nLearning scalable feature pyramid architecture for object\\ndetection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n7036–7045, 2019. 2\\n[23] Jocher Glenn. YOLOv5 release v6.1. https://github.com/\\nultralytics/yolov5/releases/tag/v6.1, 2022. 2, 7, 10\\n[24] Shuxuan Guo, Jose M Alvarez, and Mathieu Salzmann. Ex-\\npandNets: Linear over-parameterization to train compact\\nconvolutional networks. Advances in Neural Information\\nProcessing Systems (NeurIPS) , 33:1298–1310, 2020. 2\\n[25] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing\\nXu, and Chang Xu. GhostNet: More features from cheap\\noperations. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n1580–1589, 2020. 1\\n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In Proceed-\\n12', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 11}),\n",
       " Document(page_content='ings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR) , pages 770–778, 2016. 1, 4, 5\\n[27] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\\nRuoming Pang, Vijay Vasudevan, et al. Searching for Mo-\\nbileNetV3. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n1314–1324, 2019. 1\\n[28] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\\ndreetto, and Hartwig Adam. MobileNets: Efﬁcient con-\\nvolutional neural networks for mobile vision applications.\\narXiv preprint arXiv:1704.04861 , 2017. 1\\n[29] Mu Hu, Junyi Feng, Jiashen Hua, Baisheng Lai, Jian-\\nqiang Huang, Xiaojin Gong, and Xiansheng Hua. On-\\nline convolutional re-parameterization. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , 2022. 2\\n[30] Miao Hu, Yali Li, Lu Fang, and Shengjin Wang. A2-FPN:\\nAttention aggregation based feature pyramid network for\\ninstance segmentation. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 15343–15352, 2021. 2\\n[31] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E\\nHopcroft, and Kilian Q Weinberger. Snapshot ensembles:\\nTrain 1, get m for free. International Conference on Learn-\\ning Representations (ICLR) , 2017. 2\\n[32] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\\nian Q Weinberger. Densely connected convolutional net-\\nworks. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n4700–4708, 2017. 2, 4, 5\\n[33] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov,\\nDmitry Vetrov, and Andrew Gordon Wilson. Averaging\\nweights leads to wider optima and better generalization. In\\nConference on Uncertainty in Artiﬁcial Intelligence (UAI) ,\\n2018. 2\\n[34] Paul F Jaeger, Simon AA Kohl, Sebastian Bickel-\\nhaupt, Fabian Isensee, Tristan Anselm Kuder, Heinz-Peter\\nSchlemmer, and Klaus H Maier-Hein. Retina U-Net: Em-\\nbarrassingly simple exploitation of segmentation supervi-\\nsion for medical object detection. In Machine Learning for\\nHealth Workshop , pages 171–183, 2020. 1\\n[35] Hakan Karaoguz and Patric Jensfelt. Object detection ap-\\nproach for robot grasp detection. In IEEE International\\nConference on Robotics and Automation (ICRA) , pages\\n4953–4959, 2019. 1\\n[36] Kang Kim and Hee Seok Lee. Probabilistic anchor as-\\nsignment with iou prediction for object detection. In Pro-\\nceedings of the European conference on computer vision\\n(ECCV) , pages 355–371, 2020. 5\\n[37] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr\\nDoll´ar. Panoptic feature pyramid networks. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR) , pages 6399–6408, 2019. 2\\n[38] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou\\nZhang, and Zhuowen Tu. Deeply-supervised nets. In Arti-\\nﬁcial Intelligence and Statistics , pages 562–570, 2015. 5[39] Youngwan Lee, Joong-won Hwang, Sangrok Lee, Yuseok\\nBae, and Jongyoul Park. An energy and GPU-computation\\nefﬁcient backbone network for real-time object detection.\\nInProceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition Workshops (CVPRW) ,\\npages 0–0, 2019. 2, 3\\n[40] Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, and\\nXiaogang Wang. GS3D: An efﬁcient 3d object detection\\nframework for autonomous driving. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 1019–1028, 2019. 1\\n[41] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M\\nNi, and Lei Zhang. DN-DETR: Accelerate detr training\\nby introducing query denoising. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 13619–13627, 2022. 10\\n[42] Shuai Li, Chenhang He, Ruihuang Li, and Lei Zhang. A\\ndual weighting label assignment scheme for object detec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 9387–\\n9396, 2022. 2, 5\\n[43] Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang,\\nand Jian Yang. Generalized focal loss v2: Learning reliable\\nlocalization quality estimation for dense object detection. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition (CVPR) , pages 11632–11641,\\n2021. 5\\n[44] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin\\nHu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal\\nloss: Learning qualiﬁed and distributed bounding boxes for\\ndense object detection. Advances in Neural Information\\nProcessing Systems (NeurIPS) , 33:21002–21012, 2020. 5\\n[45] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\\nExploring plain vision transformer backbones for object de-\\ntection. arXiv preprint arXiv:2203.16527 , 2022. 2\\n[46] Zhuoling Li, Minghui Dong, Shiping Wen, Xiang Hu, Pan\\nZhou, and Zhigang Zeng. CLU-CNNs: Object detection for\\nmedical images. Neurocomputing , 350:53–59, 2019. 1\\n[47] Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang,\\nZhi Tang, Wei Chu, Jingdong Chen, and Haibin Ling. CB-\\nNetV2: A composite backbone network architecture for ob-\\nject detection. arXiv preprint arXiv:2107.00420 , 2021. 5,\\n10\\n[48] Ji Lin, Wei-Ming Chen, Han Cai, Chuang Gan, and Song\\nHan. Memory-efﬁcient patch-based inference for tiny deep\\nlearning. Advances in Neural Information Processing Sys-\\ntems (NeurIPS) , 34:2346–2358, 2021. 1\\n[49] Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song\\nHan, et al. MCUNet: Tiny deep learning on IoT de-\\nvices. Advances in Neural Information Processing Systems\\n(NeurIPS) , 33:11711–11722, 2020. 1\\n[50] Yuxuan Liu, Lujia Wang, and Ming Liu. YOLOStereo3D:\\nA step back to 2D for efﬁcient stereo 3D detection. In\\nIEEE International Conference on Robotics and Automa-\\ntion (ICRA) , pages 13018–13024, 2021. 5\\n[51] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong,\\n13', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 12}),\n",
       " Document(page_content='et al. Swin transformer v2: Scaling up capacity and res-\\nolution. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , 2022. 2\\n[52] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\\nHierarchical vision transformer using shifted windows. In\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision (ICCV) , pages 10012–10022, 2021. 10\\n[53] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\\nenhofer, Trevor Darrell, and Saining Xie. A ConvNet for\\nthe 2020s. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n11976–11986, 2022. 10\\n[54] Rangi Lyu. NanoDet-Plus. https://github.com/RangiLyu/\\nnanodet/releases/tag/v1.0.0-alpha-1, 2021. 1, 2\\n[55] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian\\nSun. ShufﬂeNet V2: Practical guidelines for efﬁcient CNN\\narchitecture design. In Proceedings of the European Con-\\nference on Computer Vision (ECCV) , pages 116–131, 2018.\\n1, 3\\n[56] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. A ranking-based, balanced loss function unifying\\nclassiﬁcation and localisation in object detection. Advances\\nin Neural Information Processing Systems (NeurIPS) ,\\n33:15534–15545, 2020. 2\\n[57] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. Rank & sort loss for object detection and in-\\nstance segmentation. In Proceedings of the IEEE/CVF In-\\nternational Conference on Computer Vision (ICCV) , pages\\n3009–3018, 2021. 2\\n[58] Shuvo Kumar Paul, Muhammed Tawﬁq Chowdhury,\\nMircea Nicolescu, Monica Nicolescu, and David Feil-\\nSeifer. Object detection and pose estimation from rgb and\\ndepth data for real-time, adaptive robotic grasping. In Ad-\\nvances in Computer Vision and Computational Biology ,\\npages 121–142. 2021. 1\\n[59] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. De-\\ntectoRS: Detecting objects with recursive feature pyramid\\nand switchable atrous convolution. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 10213–10224, 2021. 2\\n[60] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\\nKaiming He, and Piotr Doll ´ar. Designing network design\\nspaces. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n10428–10436, 2020. 2\\n[61] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\\nFarhadi. You only look once: Uniﬁed, real-time object de-\\ntection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n779–788, 2016. 2, 5\\n[62] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,\\nstronger. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n7263–7271, 2017. 2\\n[63] Joseph Redmon and Ali Farhadi. YOLOv3: An incremental\\nimprovement. arXiv preprint arXiv:1804.02767 , 2018. 1, 2[64] Hamid Rezatoﬁghi, Nathan Tsoi, JunYoung Gwak, Amir\\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\\ntersection over union: A metric and a loss for bounding\\nbox regression. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR) ,\\npages 658–666, 2019. 2\\n[65] Byungseok Roh, JaeWoong Shin, Wuhyun Shin, and\\nSaehoon Kim. Sparse DETR: Efﬁcient end-to-end ob-\\nject detection with learnable sparsity. arXiv preprint\\narXiv:2111.14330 , 2021. 5\\n[66] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey\\nZhmoginov, and Liang-Chieh Chen. MobileNetV2: In-\\nverted residuals and linear bottlenecks. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 4510–4520, 2018. 1\\n[67] Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang,\\nYurong Chen, and Xiangyang Xue. Object detection\\nfrom scratch with deep supervision. IEEE Transactions\\non Pattern Analysis and Machine Intelligence (TPAMI) ,\\n42(2):398–412, 2019. 5\\n[68] Karen Simonyan and Andrew Zisserman. Very deep convo-\\nlutional networks for large-scale image recognition. arXiv\\npreprint arXiv:1409.1556 , 2014. 4\\n[69] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng\\nXu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan,\\nChanghu Wang, et al. Sparse R-CNN: End-to-end ob-\\nject detection with learnable proposals. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 14454–14463, 2021. 2\\n[70] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich. Going deeper with\\nconvolutions. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR) ,\\npages 1–9, 2015. 5\\n[71] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\\nShlens, and Zbigniew Wojna. Rethinking the inception\\narchitecture for computer vision. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 2818–2826, 2016. 2\\n[72] Mingxing Tan and Quoc Le. EfﬁcientNet: Rethinking\\nmodel scaling for convolutional neural networks. In Inter-\\nnational Conference on Machine Learning (ICML) , pages\\n6105–6114, 2019. 2, 3\\n[73] Mingxing Tan and Quoc Le. EfﬁcientNetv2: Smaller mod-\\nels and faster training. In International Conference on Ma-\\nchine Learning (ICML) , pages 10096–10106, 2021. 2\\n[74] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efﬁcient-\\nDet: Scalable and efﬁcient object detection. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition (CVPR) , pages 10781–10790, 2020. 2, 10\\n[75] Antti Tarvainen and Harri Valpola. Mean teachers are better\\nrole models: Weight-averaged consistency targets improve\\nsemi-supervised deep learning results. Advances in Neural\\nInformation Processing Systems (NeurIPS) , 30, 2017. 2, 6\\n[76] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nFully convolutional one-stage object detection. In Proceed-\\n14', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 13}),\n",
       " Document(page_content='ings of the IEEE/CVF International Conference on Com-\\nputer Vision (ICCV) , pages 9627–9636, 2019. 2\\n[77] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nA simple and strong anchor-free object detector. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\n(TPAMI) , 44(4):1922–1933, 2022. 2\\n[78] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff\\nZhu, Oncel Tuzel, and Anurag Ranjan. An im-\\nproved one millisecond mobile backbone. arXiv preprint\\narXiv:2206.04040 , 2022. 2\\n[79] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\\nYuan Mark Liao. Scaled-YOLOv4: Scaling cross stage\\npartial network. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR) ,\\npages 13029–13038, 2021. 2, 3, 6, 7\\n[80] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,\\nPing-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. CSP-\\nNet: A new backbone that can enhance learning capabil-\\nity of CNN. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition Workshops\\n(CVPRW) , pages 390–391, 2020. 1\\n[81] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao.\\nYou only learn one representation: Uniﬁed network for\\nmultiple tasks. arXiv preprint arXiv:2105.04206 , 2021. 1,\\n2, 6, 7, 10\\n[82] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian\\nSun, and Nanning Zheng. End-to-end object detection\\nwith fully convolutional network. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 15849–15858, 2021. 2, 5\\n[83] Bichen Wu, Chaojian Li, Hang Zhang, Xiaoliang Dai,\\nPeizhao Zhang, Matthew Yu, Jialiang Wang, Yingyan Lin,\\nand Peter Vajda. FBNetv5: Neural architecture search for\\nmultiple tasks in one run. arXiv preprint arXiv:2111.10007 ,\\n2021. 1\\n[84] Yunyang Xiong, Hanxiao Liu, Suyog Gupta, Berkin Akin,\\nGabriel Bender, Yongzhe Wang, Pieter-Jan Kindermans,\\nMingxing Tan, Vikas Singh, and Bo Chen. MobileDets:\\nSearching for object detection architectures for mobile ac-\\ncelerators. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n3825–3834, 2021. 1\\n[85] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao\\nChang, Cheng Cui, Kaipeng Deng, Guanzhong Wang,\\nQingqing Dang, Shengyu Wei, Yuning Du, et al. PP-\\nYOLOE: An evolved version of YOLO. arXiv preprint\\narXiv:2203.16250 , 2022. 2, 7, 8, 10\\n[86] Zetong Yang, Yin Zhou, Zhifeng Chen, and Jiquan Ngiam.\\n3D-MAN: 3D multi-frame attention network for object de-\\ntection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n1863–1872, 2021. 5\\n[87] Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor\\nDarrell. Deep layer aggregation. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 2403–2412, 2018. 1\\n[88] Guanghua Yu, Qinyao Chang, Wenyu Lv, Chang Xu, Cheng\\nCui, Wei Ji, Qingqing Dang, Kaipeng Deng, GuanzhongWang, Yuning Du, et al. PP-PicoDet: A better real-\\ntime object detector on mobile devices. arXiv preprint\\narXiv:2111.00902 , 2021. 1\\n[89] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun\\nZhu, Lionel M Ni, and Heung-Yeung Shum. DINO: DETR\\nwith improved denoising anchor boxes for end-to-end ob-\\nject detection. arXiv preprint arXiv:2203.03605 , 2022. 10\\n[90] Haoyang Zhang, Ying Wang, Feras Dayoub, and Niko Sun-\\nderhauf. VarifocalNet: An IoU-aware dense object detector.\\nInProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR) , pages 8514–8523,\\n2021. 5\\n[91] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and\\nStan Z Li. Bridging the gap between anchor-based and\\nanchor-free detection via adaptive training sample selec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 9759–\\n9768, 2020. 5\\n[92] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian\\nSun. ShufﬂeNet: An extremely efﬁcient convolutional neu-\\nral network for mobile devices. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 6848–6856, 2018. 1\\n[93] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Zehuan\\nYuan, Ping Luo, Wenyu Liu, and Xinggang Wang. BYTE-\\nTrack: Multi-object tracking by associating every detection\\nbox. arXiv preprint arXiv:2110.06864 , 2021. 1\\n[94] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,\\nand Wenyu Liu. FAIRMOT: On the fairness of detec-\\ntion and re-identiﬁcation in multiple object tracking. Inter-\\nnational Journal of Computer Vision , 129(11):3069–3087,\\n2021. 1\\n[95] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang\\nYe, and Dongwei Ren. Distance-IoU loss: Faster and bet-\\nter learning for bounding box regression. In Proceedings\\nof the AAAI Conference on Artiﬁcial Intelligence (AAAI) ,\\nvolume 34, pages 12993–13000, 2020. 2\\n[96] Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, Junbo\\nYin, Yuchao Dai, and Ruigang Yang. IoU loss for 2D/3D\\nobject detection. In International Conference on 3D Vision\\n(3DV) , pages 85–94, 2019. 2\\n[97] Xingyi Zhou, Dequan Wang, and Philipp Kr ¨ahenb ¨uhl. Ob-\\njects as points. arXiv preprint arXiv:1904.07850 , 2019. 1,\\n2\\n[98] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima\\nTajbakhsh, and Jianming Liang. UNet++: A nested U-\\nNet architecture for medical image segmentation. In\\nDeep Learning in Medical Image Analysis and Multimodal\\nLearning for Clinical Decision Support , 2018. 5\\n[99] Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong,\\nSongtao Liu, Zeming Li, and Jian Sun. AutoAssign: Differ-\\nentiable label assignment for dense object detection. arXiv\\npreprint arXiv:2007.03496 , 2020. 2, 5\\n[100] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\\nand Jifeng Dai. Deformable DETR: Deformable trans-\\nformers for end-to-end object detection. In Proceedings of\\nthe International Conference on Learning Representations\\n(ICLR) , 2021. 10\\n15', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 14}),\n",
       " Document(page_content='3 grad.illinois.edu/CareerDevelopment Rachel Green  \\n2 1 0  W .  G R E E N  S T . ,  C H A M P A I G N ,  I L  \\n( 2 1 7 )  5 5 5 - 1 2 3 4  •  R S T U D E N T @ I L L I N O I S . E D U  \\nEDUCATION  \\nPhD in English May 20xx \\nUniversity of Illinois at Urbana-Champaign \\nDissertation title:  “Down on the Farm: World War One and the Emergence of Literary  \\nModernism in the American South”  \\nCommittee : Margaret Black, Naomi Blue, John Jay, Robert Roberts (Chair) \\nMA in English  20xx \\nUniversity of Illinois at Urbana-Champaign \\nBA in English and Communications, summa cum laude  20xx \\nButler University, Indianapolis, IN  \\nTEACHING  & A DVISING   \\nComposition Instructor  20xx-present \\nResearch Writing Program, University of Illinois \\n\\uf0b7Facilitator for seven sections of English composition.\\n\\uf0b7Planned and taught a writing-intensive course based upon current events.\\n\\uf0b7Used instructional technology to enhance pedagogical technique.\\n\\uf0b7Taught in part with an innovative, interdisciplinary team-teaching program design.\\nLiterature Instructor 20xx-present \\nDepartment of English, University of Illinois \\n\\uf0b7Instructor of record for two sections of literature, including Major American Authors  and\\nIntroduction to Poetry per semester.\\n\\uf0b7Integrated multimedia and humanities approaches to teaching literature using film and instructional\\ntechnology.\\nCoordinating Group Leader 20xx-20xx \\nResearch Writing Program, University of Illinois \\n\\uf0b7Planned and led required training session for teaching assistants and new composition teachers.\\n\\uf0b7Helped to mentor new hires to the English Department staff to ensure their engagement and\\nprofessional development.\\n\\uf0b7Provided job shadowing and training opportunities to assist new hires in adjusting to the pace of\\nwork and the tone and style of the University.\\nDiscussion Leader  20xx \\nCarolina Summer Reading Program, University of Illinois  \\n\\uf0b7Led group discussion for first-year students on academic topics.\\nTeaching Assistant 20xx-20xx \\nDepartment of English, University of Illinois at Urbana-Champaign \\n\\uf0b7Taught a section on film criticism, including film history, theory and technical vocabulary.\\n\\uf0b7Planned lessons and assignments, led discussion sections, graded papers and exams.\\n\\uf0b7Organized and led group discussions on social and academic issues.CV SAMPLE ', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 0}),\n",
       " Document(page_content='4 grad.illinois.edu/CareerDevelopment RESEARCH  EXPERIENCE   \\nDoctoral Researcher 20xx-20xx \\nDepartment of English, University of Illinois at Urbana-Champaign \\n\\uf0b7Conducted primary source research at numerous archives, examining publication history through\\nmultiple sources.\\n\\uf0b7Examined the literature of William Faulkner, Thomas Wolfe, and Tennessee Williams, exploring\\ntheir publication records, construction of literary identity, and relationship with modernism.\\nResearch Assistant 20xx \\nDepartment of English, University of Illinois at Urbana-Champaign \\n\\uf0b7Assistant to Professor Robert Warren, conducting primary and secondary source research.\\n\\uf0b7Organized for the “New Directions in the Study of Southern Literature: An Interdisciplinary\\nConference.”\\nPUBLICATIONS   \\nAssociate Editor of North Carolina Slave Narratives. John Jacob Franz, general editor. Forthcoming \\nfrom University of Illinois Press, 20xx. \\nJohnson, JM, Lolie, T., and Green, R.  “Lost on the Farm: Popular Beliefs” Somebody Journal, Special \\nIssue, Reflections on the Americas. Vol. 6. Accepted and forthcoming. \\nGreen, R. “Fugitives/Agrarians” in A Companion to Twentieth -Century American Poetry. Rutgers \\nPress., 20xx. \\nDavis, D.A. and Green, R.  “Will N. Harben,” “Etheridge Knight,” and “James Wilcox” in Southern \\nWriters: A Biographical Dictionary. Louisiana State University Press, 20xx. \\nCONFERENCE  PRESENTATIONS   \\n“Artistic Colloquialism,” Illinois Graduate College Seminar, speaker and organizer. Urbana, IL, 20xx.  \\n“Transitional Bible Belt,” US Divergence Symposium, Duke University, NC, February 20xx.  \\n“The Ministry of Rev. Thomas H. Jones,” South Atlantic Modern Language Association. Atlanta, GA, \\nMay 20xx. \\n“Shackles and Stripes: The Cinematic Representation of the Southern Chain Gain.” American Literature \\nAssociation. Cambridge, Massachusetts, November 20xx. \\n“Body Place of Sprits in the South,” Queen Mary College, University of London, April 6 -8, 20xx. \\nHONORS  AND  AWARDS  \\nJacob K. Javitz Fellowship, U.S. Department of Education 20xx-present \\nGraduate College Dissertation Completion Award, University of Illinois 20xx \\nCampus Teaching Award based on student evaluations, University of Illinois 20xx-20xx \\nDoctoral Fellowship, Illinois Program for Research in the Humanities,  20xx-20xx \\nUniversity of Illinois \\nSummer Research Grant, Center for Summer Studies, City, ST  20xx \\nGraduate College Conference Travel Grant, University of Illinois 20xx & 20xx \\nMost Outstanding Butler Woman, Butler University, Indianapolis, IN 20xx \\nAcademic Scholarship, Butler University, Indianapolis, IN 20xx-20xx \\nRachel Green, page 2 of 3 ', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 1}),\n",
       " Document(page_content='5 grad.illinois.edu/CareerDevelopment PROFESSIONAL  SERVICE  \\nManaging Editor 20xx-present \\nSouthern Literary Journal  \\n\\uf0b7Process manuscripts submitted for publication\\n\\uf0b7Oversee production and publication procedures.\\n\\uf0b7Maintain editorial correspondence with prospective contributors.\\n\\uf0b7Conduct business transactions including publicity, subscriptions and advertising.\\nPoetry Staff 20xx-present \\nUniversity Quarterly \\n\\uf0b7Review and solicit poems for possible publication.\\nEditorial Assistant 20xx-20xx \\nSouthern Literary Journal \\n\\uf0b7Designed and maintained journal’s internet presence.\\n\\uf0b7Edited copy for publication on a monthly basis.\\nUNIVERSITY  SERVICE  \\nGraduate Mentor 20xx-20xx \\nThe Career Center, University of Illinois \\n\\uf0b7Counsel minority undergraduates on graduate programs, application procedures and funding.\\nCareer Advisory Committee 20xx-20xx \\nGraduate College, University of Illinois \\n\\uf0b7Served on university committee to evaluate and propose career services for graduate students.\\n\\uf0b7Collaborated with faculty and students to prepare final report for submission to the Graduate\\nCollege Dean.\\nUniversity Library Advisory Committee 20xx-20xx \\nUndergraduate Library, University of Illinois \\n\\uf0b7Advised University Librarian on needed services and improvements.\\nPROFESSIONAL  MEMBERSHIPS  \\n\\uf0b7Modern Language Association (MLA)\\n\\uf0b7American Literature Association (ALA)\\n\\uf0b7American Studies Association (ASA)\\n\\uf0b7South Atlantic Modern Language Association\\n(samla)\\uf0b7Society for the Study of Southern Literature\\n\\uf0b7Robert Penn Warren Circle\\n\\uf0b7Southern Research Circle\\n\\uf0b7Fellowship of Southern Writers\\nREFERENCES  \\nJohn Jay , Assoc. Professor of English \\nUniversity of Illinois at Urbana-Champaign \\n(217) 333-1112, jjay@illinois.eduJacob S. Snyder , Assoc. Professor of English \\nUniversity of Illinois at Urbana-Champaign \\n(217) 333-4700, jssnyd@illinois.edu\\nRobert Roberts , Professor of English \\nUniversity of Illinois at Urbana-Champaign \\n(217) 333-0203, rrobe3@illinois.eduSally Briscoe, Assoc. Professor of English \\nButler University, Indianapolis, IN \\n(317) 492-8763, briscoe@butler.edu\\nRachel Green, page 3 of 3 ', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 2})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the Extracted Data into Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\\ndetectors\\nChien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\\n1Institute of Information Science, Academia Sinica, Taiwan\\nkinyiu@iis.sinica.edu.tw, alexeyab84@gmail.com, and liao@iis.sinica.edu.tw\\nAbstract\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS\\nand has the highest accuracy 56.8% AP among all known', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 0}),\n",
       " Document(page_content='real-time object detectors with 30 FPS or higher on GPU\\nV100. YOLOv7-E6 object detector (56 FPS V100, 55.9%\\nAP) outperforms both transformer-based detector SWIN-\\nL Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by\\n509% in speed and 2% in accuracy, and convolutional-\\nbased detector ConvNeXt-XL Cascade-Mask R-CNN (8.6\\nFPS A100, 55.2% AP) by 551% in speed and 0.7% AP\\nin accuracy, as well as YOLOv7 outperforms: YOLOR,\\nYOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 0}),\n",
       " Document(page_content='DETR, DINO-5scale-R50, ViT-Adapter-B and many other\\nobject detectors in speed and accuracy. Moreover, we train\\nYOLOv7 only on MS COCO dataset from scratch without\\nusing any other datasets or pre-trained weights. Source\\ncode is released in https://github.com/WongKinYiu/yolov7.\\n1. Introduction\\nReal-time object detection is a very important topic in\\ncomputer vision, as it is often a necessary component in\\ncomputer vision systems. For example, multi-object track-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 0}),\n",
       " Document(page_content='ing [94, 93], autonomous driving [40, 18], robotics [35, 58],\\nmedical image analysis [34, 46], etc. The computing de-\\nvices that execute real-time object detection is usually some\\nmobile CPU or GPU, as well as various neural processing\\nunits (NPU) developed by major manufacturers. For exam-\\nple, the Apple neural engine (Apple), the neural compute\\nstick (Intel), Jetson AI edge devices (Nvidia), the edge TPU\\n(Google), the neural processing engine (Qualcomm), the AI', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 0}),\n",
       " Document(page_content='processing unit (MediaTek), and the AI SoCs (Kneron), are\\nall NPUs. Some of the above mentioned edge devices focus\\non speeding up different operations such as vanilla convolu-\\ntion, depth-wise convolution, or MLP operations. In this pa-\\nper, the real-time object detector we proposed mainly hopes\\nthat it can support both mobile GPU and GPU devices from\\nthe edge to the cloud.\\nIn recent years, the real-time object detector is still de-\\nveloped for different edge device. For example, the devel-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 0}),\n",
       " Document(page_content='Figure 1: Comparison with other real-time object detectors, our\\nproposed methods achieve state-of-the-arts performance.\\nopment of MCUNet [49, 48] and NanoDet [54] focused on\\nproducing low-power single-chip and improving the infer-\\nence speed on edge CPU. As for methods such as YOLOX\\n[21] and YOLOR [81], they focus on improving the infer-\\nence speed of various GPUs. More recently, the develop-\\nment of real-time object detector has focused on the de-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 0}),\n",
       " Document(page_content='sign of efﬁcient architecture. As for real-time object de-\\ntectors that can be used on CPU [54, 88, 84, 83], their de-\\nsign is mostly based on MobileNet [28, 66, 27], ShufﬂeNet\\n[92, 55], or GhostNet [25]. Another mainstream real-time\\nobject detectors are developed for GPU [81, 21, 97], they\\nmostly use ResNet [26], DarkNet [63], or DLA [87], and\\nthen use the CSPNet [80] strategy to optimize the architec-\\nture. The development direction of the proposed methods in', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 0}),\n",
       " Document(page_content='this paper are different from that of the current mainstream\\nreal-time object detectors. In addition to architecture op-\\ntimization, our proposed methods will focus on the opti-\\nmization of the training process. Our focus will be on some\\noptimized modules and optimization methods which may\\nstrengthen the training cost for improving the accuracy of\\nobject detection, but without increasing the inference cost.\\nWe call the proposed modules and optimization methods\\ntrainable bag-of-freebies.', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 0}),\n",
       " Document(page_content='1arXiv:2207.02696v1  [cs.CV]  6 Jul 2022', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 0}),\n",
       " Document(page_content='Recently, model re-parameterization [13, 12, 29] and dy-\\nnamic label assignment [20, 17, 42] have become important\\ntopics in network training and object detection. Mainly af-\\nter the above new concepts are proposed, the training of\\nobject detector evolves many new issues. In this paper, we\\nwill present some of the new issues we have discovered and\\ndevise effective methods to address them. For model re-\\nparameterization, we analyze the model re-parameterization', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 1}),\n",
       " Document(page_content='strategies applicable to layers in different networks with the\\nconcept of gradient propagation path, and propose planned\\nre-parameterized model. In addition, when we discover that\\nwith dynamic label assignment technology, the training of\\nmodel with multiple output layers will generate new issues.\\nThat is: “How to assign dynamic targets for the outputs of\\ndifferent branches?” For this problem, we propose a new\\nlabel assignment method called coarse-to-ﬁne lead guided\\nlabel assignment.', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 1}),\n",
       " Document(page_content='label assignment.\\nThe contributions of this paper are summarized as fol-\\nlows: (1) we design several trainable bag-of-freebies meth-\\nods, so that real-time object detection can greatly improve\\nthe detection accuracy without increasing the inference\\ncost; (2) for the evolution of object detection methods, we\\nfound two new issues, namely how re-parameterized mod-\\nule replaces original module, and how dynamic label as-\\nsignment strategy deals with assignment to different output', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 1}),\n",
       " Document(page_content='layers. In addition, we also propose methods to address the\\ndifﬁculties arising from these issues; (3) we propose “ex-\\ntend” and “compound scaling” methods for the real-time\\nobject detector that can effectively utilize parameters and\\ncomputation; and (4) the method we proposed can effec-\\ntively reduce about 40% parameters and 50% computation\\nof state-of-the-art real-time object detector, and has faster\\ninference speed and higher detection accuracy.\\n2. Related work', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 1}),\n",
       " Document(page_content='2. Related work\\n2.1. Real-time object detectors\\nCurrently state-of-the-art real-time object detectors are\\nmainly based on YOLO [61, 62, 63] and FCOS [76, 77],\\nwhich are [3, 79, 81, 21, 54, 85, 23]. Being able to become\\na state-of-the-art real-time object detector usually requires\\nthe following characteristics: (1) a faster and stronger net-\\nwork architecture; (2) a more effective feature integration\\nmethod [22, 97, 37, 74, 59, 30, 9, 45]; (3) a more accurate', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 1}),\n",
       " Document(page_content='detection method [76, 77, 69]; (4) a more robust loss func-\\ntion [96, 64, 6, 56, 95, 57]; (5) a more efﬁcient label assign-\\nment method [99, 20, 17, 82, 42]; and (6) a more efﬁcient\\ntraining method. In this paper, we do not intend to explore\\nself-supervised learning or knowledge distillation methods\\nthat require additional data or large model. Instead, we will\\ndesign new trainable bag-of-freebies method for the issues\\nderived from the state-of-the-art methods associated with', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 1}),\n",
       " Document(page_content='(4), (5), and (6) mentioned above.2.2. Model re-parameterization\\nModel re-parametrization techniques [71, 31, 75, 19, 33,\\n11, 4, 24, 13, 12, 10, 29, 14, 78] merge multiple compu-\\ntational modules into one at inference stage. The model\\nre-parameterization technique can be regarded as an en-\\nsemble technique, and we can divide it into two cate-\\ngories, i.e., module-level ensemble and model-level ensem-\\nble. There are two common practices for model-level re-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 1}),\n",
       " Document(page_content='parameterization to obtain the ﬁnal inference model. One\\nis to train multiple identical models with different train-\\ning data, and then average the weights of multiple trained\\nmodels. The other is to perform a weighted average of the\\nweights of models at different iteration number. Module-\\nlevel re-parameterization is a more popular research issue\\nrecently. This type of method splits a module into multi-\\nple identical or different module branches during training', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 1}),\n",
       " Document(page_content='and integrates multiple branched modules into a completely\\nequivalent module during inference. However, not all pro-\\nposed re-parameterized module can be perfectly applied to\\ndifferent architectures. With this in mind, we have devel-\\noped new re-parameterization module and designed related\\napplication strategies for various architectures.\\n2.3. Model scaling\\nModel scaling [72, 60, 74, 73, 15, 16, 2, 51] is a way\\nto scale up or down an already designed model and make', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 1}),\n",
       " Document(page_content='it ﬁt in different computing devices. The model scaling\\nmethod usually uses different scaling factors, such as reso-\\nlution (size of input image), depth (number of layer), width\\n(number of channel), and stage (number of feature pyra-\\nmid), so as to achieve a good trade-off for the amount of\\nnetwork parameters, computation, inference speed, and ac-\\ncuracy. Network architecture search (NAS) is one of the\\ncommonly used model scaling methods. NAS can automat-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 1}),\n",
       " Document(page_content='ically search for suitable scaling factors from search space\\nwithout deﬁning too complicated rules. The disadvantage\\nof NAS is that it requires very expensive computation to\\ncomplete the search for model scaling factors. In [15], the\\nresearcher analyzes the relationship between scaling factors\\nand the amount of parameters and operations, trying to di-\\nrectly estimate some rules, and thereby obtain the scaling\\nfactors required by model scaling. Checking the literature,', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 1}),\n",
       " Document(page_content='we found that almost all model scaling methods analyze in-\\ndividual scaling factor independently, and even the methods\\nin the compound scaling category also optimized scaling\\nfactor independently. The reason for this is because most\\npopular NAS architectures deal with scaling factors that are\\nnot very correlated. We observed that all concatenation-\\nbased models, such as DenseNet [32] or V oVNet [39], will\\nchange the input width of some layers when the depth of', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 1}),\n",
       " Document(page_content='such models is scaled. Since the proposed architecture is\\nconcatenation-based, we have to design a new compound\\nscaling method for this model.\\n2', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 1}),\n",
       " Document(page_content='Figure 2: Extended efﬁcient layer aggregation networks. The proposed extended ELAN (E-ELAN) does not change the gradient transmis-\\nsion path of the original architecture at all, but use group convolution to increase the cardinality of the added features, and combine the\\nfeatures of different groups in a shufﬂe and merge cardinality manner. This way of operation can enhance the features learned by different\\nfeature maps and improve the use of parameters and calculations.\\n3. Architecture', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 2}),\n",
       " Document(page_content='3. Architecture\\n3.1. Extended efﬁcient layer aggregation networks\\nIn most of the literature on designing the efﬁcient ar-\\nchitectures, the main considerations are no more than the\\nnumber of parameters, the amount of computation, and the\\ncomputational density. Starting from the characteristics of\\nmemory access cost, Ma et al. [55] also analyzed the in-\\nﬂuence of the input/output channel ratio, the number of\\nbranches of the architecture, and the element-wise opera-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 2}),\n",
       " Document(page_content='tion on the network inference speed. Doll ´aret al. [15] addi-\\ntionally considered activation when performing model scal-\\ning, that is, to put more consideration on the number of el-\\nements in the output tensors of convolutional layers. The\\ndesign of CSPV oVNet [79] in Figure 2 (b) is a variation of\\nV oVNet [39]. In addition to considering the aforementioned\\nbasic designing concerns, the architecture of CSPV oVNet\\n[79] also analyzes the gradient path, in order to enable the', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 2}),\n",
       " Document(page_content='weights of different layers to learn more diverse features.\\nThe gradient analysis approach described above makes in-\\nferences faster and more accurate. ELAN [1] in Figure 2 (c)\\nconsiders the following design strategy – “How to design an\\nefﬁcient network?.” They came out with a conclusion: By\\ncontrolling the shortest longest gradient path, a deeper net-\\nwork can learn and converge effectively. In this paper, we\\npropose Extended-ELAN (E-ELAN) based on ELAN and', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 2}),\n",
       " Document(page_content='its main architecture is shown in Figure 2 (d).\\nRegardless of the gradient path length and the stacking\\nnumber of computational blocks in large-scale ELAN, it has\\nreached a stable state. If more computational blocks are\\nstacked unlimitedly, this stable state may be destroyed, and\\nthe parameter utilization rate will decrease. The proposedE-ELAN uses expand, shufﬂe, merge cardinality to achieve\\nthe ability to continuously enhance the learning ability of', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 2}),\n",
       " Document(page_content='the network without destroying the original gradient path.\\nIn terms of architecture, E-ELAN only changes the archi-\\ntecture in computational block, while the architecture of\\ntransition layer is completely unchanged. Our strategy is\\nto use group convolution to expand the channel and car-\\ndinality of computational blocks. We will apply the same\\ngroup parameter and channel multiplier to all the compu-\\ntational blocks of a computational layer. Then, the feature', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 2}),\n",
       " Document(page_content='map calculated by each computational block will be shuf-\\nﬂed into ggroups according to the set group parameter g,\\nand then concatenate them together. At this time, the num-\\nber of channels in each group of feature map will be the\\nsame as the number of channels in the original architec-\\nture. Finally, we add ggroups of feature maps to perform\\nmerge cardinality. In addition to maintaining the original\\nELAN design architecture, E-ELAN can also guide differ-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 2}),\n",
       " Document(page_content='ent groups of computational blocks to learn more diverse\\nfeatures.\\n3.2. Model scaling for concatenation-based models\\nThe main purpose of model scaling is to adjust some at-\\ntributes of the model and generate models of different scales\\nto meet the needs of different inference speeds. For ex-\\nample the scaling model of EfﬁcientNet [72] considers the\\nwidth, depth, and resolution. As for the scaled-YOLOv4\\n[79], its scaling model is to adjust the number of stages. In', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 2}),\n",
       " Document(page_content='[15], Doll ´aret al. analyzed the inﬂuence of vanilla convolu-\\ntion and group convolution on the amount of parameter and\\ncomputation when performing width and depth scaling, and\\nused this to design the corresponding model scaling method.\\n3', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 2}),\n",
       " Document(page_content='Figure 3: Model scaling for concatenation-based models. From (a) to (b), we observe that when depth scaling is performed on\\nconcatenation-based models, the output width of a computational block also increases. This phenomenon will cause the input width\\nof the subsequent transmission layer to increase. Therefore, we propose (c), that is, when performing model scaling on concatenation-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 3}),\n",
       " Document(page_content='based models, only the depth in a computational block needs to be scaled, and the remaining of transmission layer is performed with\\ncorresponding width scaling.\\nThe above methods are mainly used in architectures such as\\nPlainNet or ResNet. When these architectures are in execut-\\ning scaling up or scaling down, the in-degree and out-degree\\nof each layer will not change, so we can independently an-\\nalyze the impact of each scaling factor on the amount of', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 3}),\n",
       " Document(page_content='parameters and computation. However, if these methods\\nare applied to the concatenation-based architecture, we will\\nﬁnd that when scaling up or scaling down is performed on\\ndepth, the in-degree of a translation layer which is immedi-\\nately after a concatenation-based computational block will\\ndecrease or increase, as shown in Figure 3 (a) and (b).\\nIt can be inferred from the above phenomenon that\\nwe cannot analyze different scaling factors separately for', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 3}),\n",
       " Document(page_content='a concatenation-based model but must be considered to-\\ngether. Take scaling-up depth as an example, such an ac-\\ntion will cause a ratio change between the input channel and\\noutput channel of a transition layer, which may lead to a de-\\ncrease in the hardware usage of the model. Therefore, we\\nmust propose the corresponding compound model scaling\\nmethod for a concatenation-based model. When we scale\\nthe depth factor of a computational block, we must also cal-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 3}),\n",
       " Document(page_content='culate the change of the output channel of that block. Then,\\nwe will perform width factor scaling with the same amount\\nof change on the transition layers, and the result is shown\\nin Figure 3 (c). Our proposed compound scaling method\\ncan maintain the properties that the model had at the initial\\ndesign and maintains the optimal structure.\\n4. Trainable bag-of-freebies\\n4.1. Planned re-parameterized convolution\\nAlthough RepConv [13] has achieved excellent perfor-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 3}),\n",
       " Document(page_content='mance on the VGG [68], when we directly apply it to\\nResNet [26] and DenseNet [32] and other architectures,\\nits accuracy will be signiﬁcantly reduced. We use gradi-\\nent ﬂow propagation paths to analyze how re-parameterized\\nconvolution should be combined with different network.\\nWe also designed planned re-parameterized convolution ac-\\ncordingly.\\nFigure 4: Planned re-parameterized model. In the proposed\\nplanned re-parameterized model, we found that a layer with resid-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 3}),\n",
       " Document(page_content='ual or concatenation connections, its RepConv should not have\\nidentity connection. Under these circumstances, it can be replaced\\nby RepConvN that contains no identity connections.\\nRepConv actually combines 3×3convolution, 1×1\\nconvolution, and identity connection in one convolutional\\nlayer. After analyzing the combination and correspond-\\ning performance of RepConv and different architectures,\\nwe ﬁnd that the identity connection in RepConv destroys', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 3}),\n",
       " Document(page_content='the residual in ResNet and the concatenation in DenseNet,\\nwhich provides more diversity of gradients for different fea-\\nture maps. For the above reasons, we use RepConv with-\\nout identity connection (RepConvN) to design the architec-\\nture of planned re-parameterized convolution. In our think-\\ning, when a convolutional layer with residual or concate-\\nnation is replaced by re-parameterized convolution, there\\nshould be no identity connection. Figure 4 shows an exam-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 3}),\n",
       " Document(page_content='ple of our designed “planned re-parameterized convolution”\\nused in PlainNet and ResNet. As for the complete planned\\nre-parameterized convolution experiment in residual-based\\nmodel and concatenation-based model, it will be presented\\nin the ablation study session.\\n4', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 3}),\n",
       " Document(page_content='Figure 5: Coarse for auxiliary and ﬁne for lead head label assigner. Compare with normal model (a), the schema in (b) has auxiliary head.\\nDifferent from the usual independent label assigner (c), we propose (d) lead head guided label assigner and (e) coarse-to-ﬁne lead head\\nguided label assigner. The proposed label assigner is optimized by lead head prediction and the ground truth to get the labels of training', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 4}),\n",
       " Document(page_content='lead head and auxiliary head at the same time. The detailed coarse-to-ﬁne implementation method and constraint design details will be\\nelaborated in Apendix.\\n4.2. Coarse for auxiliary and ﬁne for lead loss\\nDeep supervision [38] is a technique that is often used\\nin training deep networks. Its main concept is to add\\nextra auxiliary head in the middle layers of the network,\\nand the shallow network weights with assistant loss as the\\nguide. Even for architectures such as ResNet [26] and', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 4}),\n",
       " Document(page_content='DenseNet [32] which usually converge well, deep supervi-\\nsion [70, 98, 67, 47, 82, 65, 86, 50] can still signiﬁcantly\\nimprove the performance of the model on many tasks. Fig-\\nure 5 (a) and (b) show, respectively, the object detector ar-\\nchitecture “without” and “with” deep supervision. In this\\npaper, we call the head responsible for the ﬁnal output as\\nthe lead head, and the head used to assist training is called\\nauxiliary head.\\nNext we want to discuss the issue of label assignment. In', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 4}),\n",
       " Document(page_content='the past, in the training of deep network, label assignment\\nusually refers directly to the ground truth and generate hard\\nlabel according to the given rules. However, in recent years,\\nif we take object detection as an example, researchers often\\nuse the quality and distribution of prediction output by the\\nnetwork, and then consider together with the ground truth to\\nuse some calculation and optimization methods to generate\\na reliable soft label [61, 8, 36, 99, 91, 44, 43, 90, 20, 17, 42].', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 4}),\n",
       " Document(page_content='For example, YOLO [61] use IoU of prediction of bounding\\nbox regression and ground truth as the soft label of object-\\nness. In this paper, we call the mechanism that considers\\nthe network prediction results together with the ground truth\\nand then assigns soft labels as “label assigner.”\\nDeep supervision needs to be trained on the target ob-\\njectives regardless of the circumstances of auxiliary head or\\nlead head. During the development of soft label assigner re-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 4}),\n",
       " Document(page_content='lated techniques, we accidentally discovered a new deriva-\\ntive issue, i.e., “How to assign soft label to auxiliary head\\nand lead head ?” To the best of our knowledge, the relevant\\nliterature has not explored this issue so far. The results of\\nthe most popular method at present is as shown in Figure 5\\n(c), which is to separate auxiliary head and lead head, and\\nthen use their own prediction results and the ground truthto execute label assignment. The method proposed in this', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 4}),\n",
       " Document(page_content='paper is a new label assignment method that guides both\\nauxiliary head and lead head by the lead head prediction.\\nIn other words, we use lead head prediction as guidance to\\ngenerate coarse-to-ﬁne hierarchical labels, which are used\\nfor auxiliary head and lead head learning, respectively. The\\ntwo proposed deep supervision label assignment strategies\\nare shown in Figure 5 (d) and (e), respectively.\\nLead head guided label assigner is mainly calculated', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 4}),\n",
       " Document(page_content='based on the prediction result of the lead head and the\\nground truth, and generate soft label through the optimiza-\\ntion process. This set of soft labels will be used as the tar-\\nget training model for both auxiliary head and lead head.\\nThe reason to do this is because lead head has a relatively\\nstrong learning capability, so the soft label generated from it\\nshould be more representative of the distribution and corre-\\nlation between the source data and the target. Furthermore,', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 4}),\n",
       " Document(page_content='we can view such learning as a kind of generalized residual\\nlearning. By letting the shallower auxiliary head directly\\nlearn the information that lead head has learned, lead head\\nwill be more able to focus on learning residual information\\nthat has not yet been learned.\\nCoarse-to-ﬁne lead head guided label assigner also\\nused the predicted result of the lead head and the ground\\ntruth to generate soft label. However, in the process we gen-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 4}),\n",
       " Document(page_content='erate two different sets of soft label, i.e., coarse label and\\nﬁne label, where ﬁne label is the same as the soft label gen-\\nerated by lead head guided label assigner, and coarse label\\nis generated by allowing more grids to be treated as posi-\\ntive target by relaxing the constraints of the positive sample\\nassignment process. The reason for this is that the learning\\nability of an auxiliary head is not as strong as that of a lead\\nhead, and in order to avoid losing the information that needs', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 4}),\n",
       " Document(page_content='to be learned, we will focus on optimizing the recall of aux-\\niliary head in the object detection task. As for the output\\nof lead head, we can ﬁlter the high precision results from\\nthe high recall results as the ﬁnal output. However, we must\\nnote that if the additional weight of coarse label is close to\\n5', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 4}),\n",
       " Document(page_content='Table 1: Comparison of baseline object detectors.\\nModel #Param. FLOPs Size APvalAPval\\n50APval\\n75APval\\nSAPval\\nMAPval\\nL\\nYOLOv4 [3] 64.4M 142.8G 640 49.7% 68.2% 54.3% 32.9% 54.8% 63.7%\\nYOLOR-u5 (r6.1) [81] 46.5M 109.1G 640 50.2% 68.7% 54.6% 33.2% 55.5% 63.7%\\nYOLOv4-CSP [79] 52.9M 120.4G 640 50.3% 68.6% 54.9% 34.2% 55.6% 65.1%\\nYOLOR-CSP [81] 52.9M 120.4G 640 50.8% 69.5% 55.3% 33.7% 56.0% 65.4%\\nYOLOv7 36.9M 104.7G 640 51.2% 69.7% 55.5% 35.2% 56.0% 66.7%', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 5}),\n",
       " Document(page_content='improvement -43% -15% - +0.4 +0.2 +0.2 +1.5 = +1.3\\nYOLOR-CSP-X [81] 96.9M 226.8G 640 52.7% 71.3% 57.4% 36.3% 57.5% 68.3%\\nYOLOv7-X 71.3M 189.9G 640 52.9% 71.1% 57.5% 36.9% 57.7% 68.6%\\nimprovement -36% -19% - +0.2 -0.2 +0.1 +0.6 +0.2 +0.3\\nYOLOv4-tiny [79] 6.1 6.9 416 24.9% 42.1% 25.7% 8.7% 28.4% 39.2%\\nYOLOv7-tiny 6.2 5.8 416 35.2% 52.8% 37.3% 15.7% 38.0% 53.4%\\nimprovement +2% -19% - +10.3 +10.7 +11.6 +7.0 +9.6 +14.2\\nYOLOv4-tiny-3l [79] 8.7 5.2 320 30.8% 47.3% 32.2% 10.9% 31.9% 51.5%', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 5}),\n",
       " Document(page_content='YOLOv7-tiny 6.2 3.5 320 30.8% 47.3% 32.2% 10.0% 31.9% 52.2%\\nimprovement -39% -49% - = = = -0.9 = +0.7\\nYOLOR-E6 [81] 115.8M 683.2G 1280 55.7% 73.2% 60.7% 40.1% 60.4% 69.2%\\nYOLOv7-E6 97.2M 515.2G 1280 55.9% 73.5% 61.1% 40.6% 60.3% 70.0%\\nimprovement -19% -33% - +0.2 +0.3 +0.4 +0.5 -0.1 +0.8\\nYOLOR-D6 [81] 151.7M 935.6G 1280 56.1% 73.9% 61.2% 42.4% 60.5% 69.9%\\nYOLOv7-D6 154.7M 806.8G 1280 56.3% 73.8% 61.4% 41.3% 60.6% 70.1%\\nYOLOv7-E6E 151.7M 843.2G 1280 56.8% 74.4% 62.1% 40.8% 62.1% 70.6%', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 5}),\n",
       " Document(page_content='improvement = -11% - +0.7 +0.5 +0.9 -1.6 +1.6 +0.7\\nthat of ﬁne label, it may produce bad prior at ﬁnal predic-\\ntion. Therefore, in order to make those extra coarse positive\\ngrids have less impact, we put restrictions in the decoder,\\nso that the extra coarse positive grids cannot produce soft\\nlabel perfectly. The mechanism mentioned above allows\\nthe importance of ﬁne label and coarse label to be dynam-\\nically adjusted during the learning process, and makes the', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 5}),\n",
       " Document(page_content='optimizable upper bound of ﬁne label always higher than\\ncoarse label.\\n4.3. Other trainable bag-of-freebies\\nIn this section we will list some trainable bag-of-\\nfreebies. These freebies are some of the tricks we used\\nin training, but the original concepts were not proposed\\nby us. The training details of these freebies will be elab-\\norated in the Appendix, including (1) Batch normalization\\nin conv-bn-activation topology: This part mainly connects', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 5}),\n",
       " Document(page_content='batch normalization layer directly to convolutional layer.\\nThe purpose of this is to integrate the mean and variance\\nof batch normalization into the bias and weight of convolu-\\ntional layer at the inference stage. (2) Implicit knowledge\\nin YOLOR [81] combined with convolution feature map in\\naddition and multiplication manner: Implicit knowledge in\\nYOLOR can be simpliﬁed to a vector by pre-computing at\\nthe inference stage. This vector can be combined with the', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 5}),\n",
       " Document(page_content='bias and weight of the previous or subsequent convolutional\\nlayer. (3) EMA model: EMA is a technique used in mean\\nteacher [75], and in our system we use EMA model purely\\nas the ﬁnal inference model.5. Experiments\\n5.1. Experimental setup\\nWe use Microsoft COCO dataset to conduct experiments\\nand validate our object detection method. All our experi-\\nments did not use pre-trained models. That is, all models\\nwere trained from scratch. During the development pro-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 5}),\n",
       " Document(page_content='cess, we used train 2017 set for training, and then used val\\n2017 set for veriﬁcation and choosing hyperparameters. Fi-\\nnally, we show the performance of object detection on the\\ntest 2017 set and compare it with the state-of-the-art object\\ndetection algorithms. Detailed training parameter settings\\nare described in Appendix.\\nWe designed basic model for edge GPU, normal GPU,\\nand cloud GPU, and they are respectively called YOLOv7-\\ntiny, YOLOv7, and YOLOv7-W6. At the same time, we', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 5}),\n",
       " Document(page_content='also use basic model for model scaling for different ser-\\nvice requirements and get different types of models. For\\nYOLOv7, we do stack scaling on neck, and use the pro-\\nposed compound scaling method to perform scaling-up of\\nthe depth and width of the entire model, and use this to ob-\\ntain YOLOv7-X. As for YOLOv7-W6, we use the newly\\nproposed compound scaling method to obtain YOLOv7-E6\\nand YOLOv7-D6. In addition, we use the proposed E-\\nELAN for YOLOv7-E6, and thereby complete YOLOv7-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 5}),\n",
       " Document(page_content='E6E. Since YOLOv7-tiny is an edge GPU-oriented archi-\\ntecture, it will use leaky ReLU as activation function. As\\nfor other models we use SiLU as activation function. We\\nwill describe the scaling factor of each model in detail in\\nAppendix.\\n6', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 5}),\n",
       " Document(page_content='Table 2: Comparison of state-of-the-art real-time object detectors.\\nModel #Param. FLOPs Size FPS APtest/APvalAPtest\\n50APtest\\n75APtest\\nSAPtest\\nMAPtest\\nL\\nYOLOX-S [21] 9.0M 26.8G 640 102 40.5% / 40.5% - - - - -\\nYOLOX-M [21] 25.3M 73.8G 640 81 47.2% / 46.9% - - - - -\\nYOLOX-L [21] 54.2M 155.6G 640 69 50.1% / 49.7% - - - - -\\nYOLOX-X [21] 99.1M 281.9G 640 58 51.5% / 51.1% - - - - -\\nPPYOLOE-S [85] 7.9M 17.4G 640 208 43.1% / 42.7% 60.5% 46.6% 23.2% 46.4% 56.9%', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 6}),\n",
       " Document(page_content='PPYOLOE-M [85] 23.4M 49.9G 640 123 48.9% / 48.6% 66.5% 53.0% 28.6% 52.9% 63.8%\\nPPYOLOE-L [85] 52.2M 110.1G 640 78 51.4% / 50.9% 68.9% 55.6% 31.4% 55.3% 66.1%\\nPPYOLOE-X [85] 98.4M 206.6G 640 45 52.2% / 51.9% 69.9% 56.5% 33.3% 56.3% 66.4%\\nYOLOv5-N (r6.1) [23] 1.9M 4.5G 640 159 - / 28.0% - - - - -\\nYOLOv5-S (r6.1) [23] 7.2M 16.5G 640 156 - / 37.4% - - - - -\\nYOLOv5-M (r6.1) [23] 21.2M 49.0G 640 122 - / 45.4% - - - - -\\nYOLOv5-L (r6.1) [23] 46.5M 109.1G 640 99 - / 49.0% - - - - -', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 6}),\n",
       " Document(page_content='YOLOv5-X (r6.1) [23] 86.7M 205.7G 640 83 - / 50.7% - - - - -\\nYOLOR-CSP [81] 52.9M 120.4G 640 106 51.1% / 50.8% 69.6% 55.7% 31.7% 55.3% 64.7%\\nYOLOR-CSP-X [81] 96.9M 226.8G 640 87 53.0% / 52.7% 71.4% 57.9% 33.7% 57.1% 66.8%\\nYOLOv7-tiny-SiLU 6.2M 13.8G 640 286 38.7% / 38.7% 56.7% 41.7% 18.8% 42.4% 51.9%\\nYOLOv7 36.9M 104.7G 640 161 51.4% / 51.2% 69.7% 55.9% 31.8% 55.5% 65.0%\\nYOLOv7-X 71.3M 189.9G 640 114 53.1% / 52.9% 71.2% 57.8% 33.8% 57.1% 67.4%', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 6}),\n",
       " Document(page_content='YOLOv5-N6 (r6.1) [23] 3.2M 18.4G 1280 123 - / 36.0% - - - - -\\nYOLOv5-S6 (r6.1) [23] 12.6M 67.2G 1280 122 - / 44.8% - - - - -\\nYOLOv5-M6 (r6.1) [23] 35.7M 200.0G 1280 90 - / 51.3% - - - - -\\nYOLOv5-L6 (r6.1) [23] 76.8M 445.6G 1280 63 - / 53.7% - - - - -\\nYOLOv5-X6 (r6.1) [23] 140.7M 839.2G 1280 38 - / 55.0% - - - - -\\nYOLOR-P6 [81] 37.2M 325.6G 1280 76 53.9% / 53.5% 71.4% 58.9% 36.1% 57.7% 65.6%\\nYOLOR-W6 [81] 79.8G 453.2G 1280 66 55.2% / 54.8% 72.7% 60.5% 37.7% 59.1% 67.1%', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 6}),\n",
       " Document(page_content='YOLOR-E6 [81] 115.8M 683.2G 1280 45 55.8% / 55.7% 73.4% 61.1% 38.4% 59.7% 67.7%\\nYOLOR-D6 [81] 151.7M 935.6G 1280 34 56.5% / 56.1% 74.1% 61.9% 38.9% 60.4% 68.7%\\nYOLOv7-W6 70.4M 360.0G 1280 84 54.9% / 54.6% 72.6% 60.1% 37.3% 58.7% 67.1%\\nYOLOv7-E6 97.2M 515.2G 1280 56 56.0% / 55.9% 73.5% 61.2% 38.0% 59.9% 68.4%\\nYOLOv7-D6 154.7M 806.8G 1280 44 56.6% / 56.3% 74.0% 61.8% 38.8% 60.1% 69.5%\\nYOLOv7-E6E 151.7M 843.2G 1280 36 56.8% / 56.8% 74.4% 62.1% 39.3% 60.5% 69.0%', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 6}),\n",
       " Document(page_content='1Our FLOPs is calaculated by rectangle input resolution like 640 ×640 or 1280 ×1280.\\n2Our inference time is estimated by using letterbox resize input image to make its long side equals to 640 or 1280.\\n5.2. Baselines\\nWe choose previous version of YOLO [3, 79] and state-\\nof-the-art object detector YOLOR [81] as our baselines. Ta-\\nble 1 shows the comparison of our proposed YOLOv7 mod-\\nels and those baseline that are trained with the same settings.', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 6}),\n",
       " Document(page_content='From the results we see that if compared with YOLOv4,\\nYOLOv7 has 75% less parameters, 36% less computation,\\nand brings 1.5% higher AP. If compared with state-of-the-\\nart YOLOR-CSP, YOLOv7 has 43% fewer parameters, 15%\\nless computation, and 0.4% higher AP. In the performance\\nof tiny model, compared with YOLOv4-tiny-31, YOLOv7-\\ntiny reduces the number of parameters by 39% and the\\namount of computation by 49%, but maintains the same AP.\\nOn the cloud GPU model, our model can still have a higher', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 6}),\n",
       " Document(page_content='AP while reducing the number of parameters by 19% and\\nthe amount of computation by 33%.5.3. Comparison with state-of-the-arts\\nWe compare the proposed method with state-of-the-art\\nobject detectors for general GPUs and Mobile GPUs, and\\nthe results are shown in Table 2. From the results in\\nTable 2 we know that the proposed method has the best\\nspeed-accuracy trade-off comprehensively. If we compare\\nYOLOv7-tiny-SiLU with YOLOv5-N (r6.1), our method', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 6}),\n",
       " Document(page_content='is 127 fps faster and 10.7% more accurate on AP. In ad-\\ndition, YOLOv7 has 51.4% AP at frame rate of 161 fps,\\nwhile PPYOLOE-L with the same AP has only 78 fps frame\\nrate. In terms of parameter usage, YOLOv7 is 41% less than\\nPPYOLOE-L. If we compare YOLOv7-X with 114 fps in-\\nference speed to YOLOv5-L (r6.1) with 99 fps inference\\nspeed, YOLOv7-X can improve AP by 3.9%. If YOLOv7-\\nX is compared with YOLOv5-X (r6.1) of similar scale, the\\ninference speed of YOLOv7-X is 31 fps faster. In addi-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 6}),\n",
       " Document(page_content='tion, in terms of the amount of parameters and computation,\\nYOLOv7-X reduces 22% of parameters and 8% of compu-\\ntation compared to YOLOv5-X (r6.1), but improves AP by\\n2.2%.\\n7', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 6}),\n",
       " Document(page_content='If we compare YOLOv7 with YOLOR using the input\\nresolution 1280, the inference speed of YOLOv7-W6 is 8\\nfps faster than that of YOLOR-P6, and the detection rate is\\nalso increased by 1% AP. As for the comparison between\\nYOLOv7-E6 and YOLOv5-X6 (r6.1), the former has 0.9%\\nAP gain than the latter, 45% less parameters and 63% less\\ncomputation, and the inference speed is increased by 47%.\\nYOLOv7-D6 has close inference speed to YOLOR-E6, but\\nimproves AP by 0.8%. YOLOv7-E6E has close inference', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 7}),\n",
       " Document(page_content='speed to YOLOR-D6, but improves AP by 0.3%.\\n5.4. Ablation study\\n5.4.1 Proposed compound scaling method\\nTable 3 shows the results obtained when using different\\nmodel scaling strategies for scaling up. Among them, our\\nproposed compound scaling method is to scale up the depth\\nof computational block by 1.5 times and the width of tran-\\nsition block by 1.25 times. If our method is compared with\\nthe method that only scaled up the width, our method can', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 7}),\n",
       " Document(page_content='improve the AP by 0.5% with less parameters and amount\\nof computation. If our method is compared with the method\\nthat only scales up the depth, our method only needs to in-\\ncrease the number of parameters by 2.9% and the amount of\\ncomputation by 1.2%, which can improve the AP by 0.2%.\\nIt can be seen from the results of Table 3 that our proposed\\ncompound scaling strategy can utilize parameters and com-\\nputation more efﬁciently.\\nTable 3: Ablation study on proposed model scaling.', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 7}),\n",
       " Document(page_content='Model #Param. FLOPs Size APvalAPval\\n50APval\\n75\\nbase (v7-X light) 47.0M 125.5G 640 51.7% 70.1% 56.0%\\nwidth only (1.25 w)73.4M 195.5G 640 52.4% 70.9% 57.1%\\ndepth only (2.0 d) 69.3M 187.6G 640 52.7% 70.8% 57.3%\\ncompound (v7-X) 71.3M 189.9G 640 52.9% 71.1% 57.5%\\nimprovement - - - +1.2 +1.0 +1.5\\n5.4.2 Proposed planned re-parameterized model\\nIn order to verify the generality of our proposed planed\\nre-parameterized model, we use it on concatenation-based', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 7}),\n",
       " Document(page_content='model and residual-based model respectively for veriﬁca-\\ntion. The concatenation-based model and residual-based\\nmodel we chose for veriﬁcation are 3-stacked ELAN and\\nCSPDarknet, respectively.\\nIn the experiment of concatenation-based model, we re-\\nplace the 3×3convolutional layers in different positions in\\n3-stacked ELAN with RepConv, and the detailed conﬁgura-\\ntion is shown in Figure 6. From the results shown in Table 4\\nwe see that all higher AP values are present on our proposed', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 7}),\n",
       " Document(page_content='planned re-parameterized model.\\nIn the experiment dealing with residual-based model,\\nsince the original dark block does not have a 3×3con-\\nFigure 6: Planned RepConv 3-stacked ELAN. Blue circles are the\\nposition we replace Conv by RepConv.\\nTable 4: Ablation study on planned RepConcatenation model.\\nModel APvalAPval\\n50APval\\n75APval\\nSAPval\\nMAPval\\nL\\nbase (3-S ELAN) 52.26% 70.41% 56.77% 35.81% 57.00% 67.59%\\nFigure 6 (a) 52.18% 70.34% 56.90% 35.71% 56.83% 67.51%', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 7}),\n",
       " Document(page_content='Figure 6 (b) 52.30% 70.30% 56.92% 35.76% 56.95% 67.74%\\nFigure 6 (c) 52.33% 70.56% 56.91% 35.90% 57.06% 67.50%\\nFigure 6 (d) 52.17% 70.32% 56.82% 35.33% 57.06% 68.09%\\nFigure 6 (e) 52.23% 70.20% 56.81% 35.34% 56.97% 66.88%\\nvolution block that conforms to our design strategy, we ad-\\nditionally design a reversed dark block for the experiment,\\nwhose architecture is shown in Figure 7. Since the CSP-\\nDarknet with dark block and reversed dark block has exactly', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 7}),\n",
       " Document(page_content='the same amount of parameters and operations, it is fair to\\ncompare. The experiment results illustrated in Table 5 fully\\nconﬁrm that the proposed planned re-parameterized model\\nis equally effective on residual-based model. We ﬁnd that\\nthe design of RepCSPResNet [85] also ﬁt our design pat-\\ntern.\\nFigure 7: Reversed CSPDarknet. We reverse the position of 1×1\\nand3×3convolutional layer in dark block to ﬁt our planned re-\\nparameterized model design strategy.', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 7}),\n",
       " Document(page_content='Table 5: Ablation study on planned RepResidual model.\\nModel APvalAPval\\n50APval\\n75APval\\nSAPval\\nMAPval\\nL\\nbase (YOLOR-W6) 54.82% 72.39% 59.95% 39.68% 59.38% 68.30%\\nRepCSP 54.67% 72.50% 59.58% 40.22% 59.61% 67.87%\\nRCSP 54.36% 71.95% 59.54% 40.15% 59.02% 67.44%\\nRepRCSP 54.85% 72.51% 60.08% 40.53% 59.52% 68.06%\\nbase (YOLOR-CSP) 50.81% 69.47% 55.28% 33.74% 56.01% 65.38%\\nRepRCSP 50.91% 69.54% 55.55% 34.44% 55.74% 65.46%\\n8', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 7}),\n",
       " Document(page_content='Figure 8: Objectness map predicted by different methods at auxiliary head and lead head.\\n5.4.3 Proposed assistant loss for auxiliary head\\nIn the assistant loss for auxiliary head experiments, we com-\\npare the general independent label assignment for lead head\\nand auxiliary head methods, and we also compare the two\\nproposed lead guided label assignment methods. We show\\nall comparison results in Table 6. From the results listed in\\nTable 6, it is clear that any model that increases assistant', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 8}),\n",
       " Document(page_content='loss can signiﬁcantly improve the overall performance. In\\naddition, our proposed lead guided label assignment strat-\\negy receives better performance than the general indepen-\\ndent label assignment strategy in AP, AP 50, and AP 75. As\\nfor our proposed coarse for assistant and ﬁne for lead label\\nassignment strategy, it results in best results in all cases. In\\nFigure 8 we show the objectness map predicted by different\\nmethods at auxiliary head and lead head. From Figure 8 we', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 8}),\n",
       " Document(page_content='ﬁnd that if auxiliary head learns lead guided soft label, it\\nwill indeed help lead head to extract the residual informa-\\ntion from the consistant targets.\\nTable 6: Ablation study on proposed auxiliary head.\\nModel Size APvalAPval\\n50APval\\n75\\nbase (v7-E6) 1280 55.6% 73.2% 60.7%\\nindependent 1280 55.8% 73.4% 60.9%\\nlead guided 1280 55.9% 73.5% 61.0%\\ncoarse-to-ﬁne lead guided 1280 55.9% 73.5% 61.1%\\nimprovement - +0.3 +0.3 +0.4\\nIn Table 7 we further analyze the effect of the proposed', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 8}),\n",
       " Document(page_content='coarse-to-ﬁne lead guided label assignment method on the\\ndecoder of auxiliary head. That is, we compared the results\\nof with/without the introduction of upper bound constraint.\\nJudging from the numbers in the Table, the method of con-\\nstraining the upper bound of objectness by the distance from\\nthe center of the object can achieve better performance.\\nTable 7: Ablation study on constrained auxiliary head.\\nModel Size APvalAPval\\n50APval\\n75\\nbase (v7-E6) 1280 55.6% 73.2% 60.7%', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 8}),\n",
       " Document(page_content='aux without constraint 1280 55.9% 73.5% 61.0%\\naux with constraint 1280 55.9% 73.5% 61.1%\\nimprovement - +0.3 +0.3 +0.4Since the proposed YOLOv7 uses multiple pyramids to\\njointly predict object detection results, we can directly con-\\nnect auxiliary head to the pyramid in the middle layer for\\ntraining. This type of training can make up for informa-\\ntion that may be lost in the next level pyramid prediction.\\nFor the above reasons, we designed partial auxiliary head', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 8}),\n",
       " Document(page_content='in the proposed E-ELAN architecture. Our approach is to\\nconnect auxiliary head after one of the sets of feature map\\nbefore merging cardinality, and this connection can make\\nthe weight of the newly generated set of feature map not\\ndirectly updated by assistant loss. Our design allows each\\npyramid of lead head to still get information from objects\\nwith different sizes. Table 8 shows the results obtained us-\\ning two different methods, i.e., coarse-to-ﬁne lead guided', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 8}),\n",
       " Document(page_content='and partial coarse-to-ﬁne lead guided methods. Obviously,\\nthe partial coarse-to-ﬁne lead guided method has a better\\nauxiliary effect.\\nTable 8: Ablation study on partial auxiliary head.\\nModel Size APvalAPval\\n50APval\\n75\\nbase (v7-E6E) 1280 56.3% 74.0% 61.5%\\naux 1280 56.5% 74.0% 61.6%\\npartial aux 1280 56.8% 74.4% 62.1%\\nimprovement - +0.5 +0.4 +0.6\\n6. Conclusions\\nIn this paper we propose a new architecture of real-\\ntime object detector and the corresponding model scaling', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 8}),\n",
       " Document(page_content='method. Furthermore, we ﬁnd that the evolving process\\nof object detection methods generates new research top-\\nics. During the research process, we found the replace-\\nment problem of re-parameterized module and the alloca-\\ntion problem of dynamic label assignment. To solve the\\nproblem, we propose the trainable bag-of-freebies method\\nto enhance the accuracy of object detection. Based on the\\nabove, we have developed the YOLOv7 series of object de-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 8}),\n",
       " Document(page_content='tection systems, which receives the state-of-the-art results.\\n7. Acknowledgements\\nThe authors wish to thank National Center for High-\\nperformance Computing (NCHC) for providing computa-\\ntional and storage resources.\\n9', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 8}),\n",
       " Document(page_content='Table 9: More comparison (batch=1, no-TRT, without extra object detection training data)\\nModel #Param. FLOPs Size FPSV100APtest/APvalAPtest\\n50APtest\\n75\\nYOLOv7-tiny-SiLU 6.2M 13.8G 640 286 38.7% /38.7% 56.7% 41.7%\\nPPYOLOE-S [85] 7.9M 17.4G 640 208 43.1% /42.7% 60.5% 46.6%\\nYOLOv7 36.9M 104.7G 640 161 51.4% /51.2% 69.7% 55.9%\\nYOLOv5-N (r6.1) [23] 1.9M 4.5G 640 159 - / 28.0% - -\\nYOLOv5-S (r6.1) [23] 7.2M 16.5G 640 156 - / 37.4% - -\\nPPYOLOE-M [85] 23.4M 49.9G 640 123 48.9% / 48.6% 66.5% 53.0%', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 9}),\n",
       " Document(page_content='YOLOv5-N6 (r6.1) [23] 3.2M 18.4G 1280 123 - / 36.0% - -\\nYOLOv5-S6 (r6.1) [23] 12.6M 67.2G 1280 122 - / 44.8% - -\\nYOLOv5-M (r6.1) [23] 21.2M 49.0G 640 122 - / 45.4% - -\\nYOLOv7-X 71.3M 189.9G 640 114 53.1% /52.9% 71.2% 57.8%\\nYOLOR-CSP [81] 52.9M 120.4G 640 106 51.1% / 50.8% 69.6% 55.7%\\nYOLOX-S [21] 9.0M 26.8G 640 102 40.5% / 40.5% - -\\nYOLOv5-L (r6.1) [23] 46.5M 109.1G 640 99 - / 49.0% - -\\nYOLOv5-M6 (r6.1) [23] 35.7M 200.0G 1280 90 - / 51.3% - -', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 9}),\n",
       " Document(page_content='YOLOR-CSP-X [81] 96.9M 226.8G 640 87 53.0% / 52.7% 71.4% 57.9%\\nYOLOv7-W6 70.4M 360.0G 1280 84 54.9% /54.6% 72.6% 60.1%\\nYOLOv5-X (r6.1) [23] 86.7M 205.7G 640 83 - / 50.7% - -\\nYOLOX-M [21] 25.3M 73.8G 640 81 47.2% / 46.9% - -\\nPPYOLOE-L [85] 52.2M 110.1G 640 78 51.4% / 50.9% 68.9% 55.6%\\nYOLOR-P6 [81] 37.2M 325.6G 1280 76 53.9% / 53.5% 71.4% 58.9%\\nYOLOX-L [21] 54.2M 155.6G 640 69 50.1% / 49.7% - -\\nYOLOR-W6 [81] 79.8G 453.2G 1280 66 55.2% /54.8% 72.7% 60.5%', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 9}),\n",
       " Document(page_content='YOLOv5-L6 (r6.1) [23] 76.8M 445.6G 1280 63 - / 53.7% - -\\nYOLOX-X [21] 99.1M 281.9G 640 58 51.5% / 51.1% - -\\nYOLOv7-E6 97.2M 515.2G 1280 56 56.0% /55.9% 73.5% 61.2%\\nYOLOR-E6 [81] 115.8M 683.2G 1280 45 55.8% / 55.7% 73.4% 61.1%\\nPPYOLOE-X [85] 98.4M 206.6G 640 45 52.2% / 51.9% 69.9% 56.5%\\nYOLOv7-D6 154.7M 806.8G 1280 44 56.6% /56.3% 74.0% 61.8%\\nYOLOv5-X6 (r6.1) [23] 140.7M 839.2G 1280 38 - / 55.0% - -\\nYOLOv7-E6E 151.7M 843.2G 1280 36 56.8% /56.8% 74.4% 62.1%', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 9}),\n",
       " Document(page_content='YOLOR-D6 [81] 151.7M 935.6G 1280 34 56.5% / 56.1% 74.1% 61.9%\\nF-RCNN-R101-FPN+ [5] 60.0M 246.0G 1333 20 - / 44.0% - -\\nDeformable DETR [100] 40.0M 173.0G - 19 - / 46.2% - -\\nSwin-B (C-M-RCNN) [52] 145.0M 982.0G 1333 11.6 - / 51.9% - -\\nDETR DC5-R101 [5] 60.0M 253.0G 1333 10 - / 44.9% - -\\nEfﬁcientDet-D7x [74] 77.0M 410.0G 1536 6.5 55.1% / 54.4% 72.4% 58.4%\\nDual-Swin-T (C-M-RCNN) [47] 113.8M 836.0G 1333 6.5 - / 53.6% - -\\nViT-Adapter-B [7] 122.0M 997.0G - 4.4 - / 50.8% - -', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 9}),\n",
       " Document(page_content='Dual-Swin-B (HTC) [47] 235.0M - 1600 2.5 58.7% /58.4% - -\\nDual-Swin-L (HTC) [47] 453.0M - 1600 1.5 59.4% /59.1% - -\\nModel #Param. FLOPs Size FPSA100APtest/APvalAPtest\\n50APtest\\n75\\nDN-Deformable-DETR [41] 48.0M 265.0G 1333 23.0 - / 48.6% - -\\nConvNeXt-B (C-M-RCNN) [53] - 964.0G 1280 11.5 - / 54.0% 73.1% 58.8%\\nSwin-B (C-M-RCNN) [52] - 982.0G 1280 10.7 - / 53.0% 71.8% 57.5%\\nDINO-5scale (R50) [89] 47.0M 860.0G 1333 10.0 - / 51.0% - -', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 9}),\n",
       " Document(page_content='ConvNeXt-L (C-M-RCNN) [53] - 1354.0G 1280 10.0 - / 54.8% 73.8% 59.8%\\nSwin-L (C-M-RCNN) [52] - 1382.0G 1280 9.2 - / 53.9% 72.4% 58.8%\\nConvNeXt-XL (C-M-RCNN) [53] - 1898.0G 1280 8.6 - / 55.2% 74.2% 59.9%\\n8. More comparison\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS and\\nhas the highest accuracy 56.8% AP test-dev / 56.8% AP\\nmin-val among all known real-time object detectors with 30\\nFPS or higher on GPU V100. YOLOv7-E6 object detector', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 9}),\n",
       " Document(page_content='(56 FPS V100, 55.9% AP) outperforms both transformer-\\nbased detector SWIN-L Cascade-Mask R-CNN (9.2 FPS\\nA100, 53.9% AP) by 509% in speed and 2% in accuracy,and convolutional-based detector ConvNeXt-XL Cascade-\\nMask R-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed\\nand 0.7% AP in accuracy, as well as YOLOv7 outperforms:\\nYOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, De-\\nformable DETR, DINO-5scale-R50, ViT-Adapter-B and\\nmany other object detectors in speed and accuracy. More', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 9}),\n",
       " Document(page_content='over, we train YOLOv7 only on MS COCO dataset from\\nscratch without using any other datasets or pre-trained\\nweights.\\n10', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 9}),\n",
       " Document(page_content='Figure 9: Comparison with other object detectors.\\nFigure 10: Comparison with other real-time object detectors.\\nTable 10: Comparison of different setting.\\nModel Presicion IoU threshold APval\\nYOLOv7-X FP16 (default) 0.65 (default) 52.9%\\nYOLOv7-X FP32 0.65 53.0%\\nYOLOv7-X FP16 0.70 53.0%\\nYOLOv7-X FP32 0.70 53.1%\\nimprovement - - +0.2%\\n*Similar to meituan/YOLOv6 and PPYOLOE, our model could\\nget higher AP when set higher IoU threshold.\\nThe maximum accuracy of the YOLOv7-E6E (56.8%', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 10}),\n",
       " Document(page_content='AP) real-time model is +13.7% AP higher than the cur-\\nrent most accurate meituan/YOLOv6-s model (43.1% AP)\\non COCO dataset. Our YOLOv7-tiny (35.2% AP, 0.4\\nms) model is +25% faster and +0.2% AP higher than\\nmeituan/YOLOv6-n (35.0% AP, 0.5 ms) under identical\\nconditions on COCO dataset and V100 GPU with batch=32.\\nFigure 11: Comparison with other real-time object detectors.\\n11', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 10}),\n",
       " Document(page_content='References\\n[1] anonymous. Designing network design strategies. anony-\\nmous submission , 2022. 3\\n[2] Irwan Bello, William Fedus, Xianzhi Du, Ekin Dogus\\nCubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens,\\nand Barret Zoph. Revisiting ResNets: Improved training\\nand scaling strategies. Advances in Neural Information Pro-\\ncessing Systems (NeurIPS) , 34, 2021. 2\\n[3] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-\\nYuan Mark Liao. YOLOv4: Optimal speed and accuracy of', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 11}),\n",
       " Document(page_content='object detection. arXiv preprint arXiv:2004.10934 , 2020.\\n2, 6, 7\\n[4] Yue Cao, Thomas Andrew Geddes, Jean Yee Hwa Yang,\\nand Pengyi Yang. Ensemble deep learning in bioinformat-\\nics.Nature Machine Intelligence , 2(9):500–508, 2020. 2\\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\\nEnd-to-end object detection with transformers. In Pro-\\nceedings of the European Conference on Computer Vision\\n(ECCV) , pages 213–229, 2020. 10', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 11}),\n",
       " Document(page_content='[6] Kean Chen, Weiyao Lin, Jianguo Li, John See, Ji Wang, and\\nJunni Zou. AP-loss for accurate one-stage object detection.\\nIEEE Transactions on Pattern Analysis and Machine Intel-\\nligence (TPAMI) , 43(11):3782–3798, 2020. 2\\n[7] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong\\nLu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for\\ndense predictions. arXiv preprint arXiv:2205.08534 , 2022.\\n10\\n[8] Jiwoong Choi, Dayoung Chun, Hyun Kim, and Hyuk-Jae', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 11}),\n",
       " Document(page_content='Lee. Gaussian YOLOv3: An accurate and fast object detec-\\ntor using localization uncertainty for autonomous driving.\\nInProceedings of the IEEE/CVF International Conference\\non Computer Vision (ICCV) , pages 502–511, 2019. 5\\n[9] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen,\\nMengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head:\\nUnifying object detection heads with attentions. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition (CVPR) , pages 7373–7382, 2021.\\n2', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 11}),\n",
       " Document(page_content='2\\n[10] Xiaohan Ding, Honghao Chen, Xiangyu Zhang, Kaiqi\\nHuang, Jungong Han, and Guiguang Ding. Re-\\nparameterizing your optimizers rather than architectures.\\narXiv preprint arXiv:2205.15242 , 2022. 2\\n[11] Xiaohan Ding, Yuchen Guo, Guiguang Ding, and Jungong\\nHan. ACNet: Strengthening the kernel skeletons for pow-\\nerful CNN via asymmetric convolution blocks. In Proceed-\\nings of the IEEE/CVF International Conference on Com-\\nputer Vision (ICCV) , pages 1911–1920, 2019. 2', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 11}),\n",
       " Document(page_content='[12] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and\\nGuiguang Ding. Diverse branch block: Building a con-\\nvolution as an inception-like unit. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 10886–10895, 2021. 2\\n[13] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong\\nHan, Guiguang Ding, and Jian Sun. RepVGG: Making\\nVGG-style convnets great again. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 11}),\n",
       " Document(page_content='Recognition (CVPR) , pages 13733–13742, 2021. 2, 4\\n[14] Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong\\nHan, Guiguang Ding, and Jian Sun. Scaling up your ker-\\nnels to 31x31: Revisiting large kernel design in CNNs. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition (CVPR) , 2022. 2\\n[15] Piotr Doll ´ar, Mannat Singh, and Ross Girshick. Fast and\\naccurate model scaling. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 11}),\n",
       " Document(page_content='(CVPR) , pages 924–932, 2021. 2, 3\\n[16] Xianzhi Du, Barret Zoph, Wei-Chih Hung, and Tsung-Yi\\nLin. Simple training strategies and model scaling for object\\ndetection. arXiv preprint arXiv:2107.00057 , 2021. 2\\n[17] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott,\\nand Weilin Huang. TOOD: Task-aligned one-stage object\\ndetection. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision (ICCV) , pages 3490–3499,\\n2021. 2, 5', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 11}),\n",
       " Document(page_content='2021. 2, 5\\n[18] Di Feng, Christian Haase-Sch ¨utz, Lars Rosenbaum, Heinz\\nHertlein, Claudius Glaeser, Fabian Timm, Werner Wies-\\nbeck, and Klaus Dietmayer. Deep multi-modal object de-\\ntection and semantic segmentation for autonomous driv-\\ning: Datasets, methods, and challenges. IEEE Transac-\\ntions on Intelligent Transportation Systems , 22(3):1341–\\n1360, 2020. 1\\n[19] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin,\\nDmitry P Vetrov, and Andrew G Wilson. Loss sur-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 11}),\n",
       " Document(page_content='faces, mode connectivity, and fast ensembling of DNNs.\\nAdvances in Neural Information Processing Systems\\n(NeurIPS) , 31, 2018. 2\\n[20] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and\\nJian Sun. OTA: Optimal transport assignment for object\\ndetection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n303–312, 2021. 2, 5\\n[21] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\\nSun. YOLOX: Exceeding YOLO series in 2021. arXiv', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 11}),\n",
       " Document(page_content='preprint arXiv:2107.08430 , 2021. 1, 2, 7, 10\\n[22] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. NAS-FPN:\\nLearning scalable feature pyramid architecture for object\\ndetection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n7036–7045, 2019. 2\\n[23] Jocher Glenn. YOLOv5 release v6.1. https://github.com/\\nultralytics/yolov5/releases/tag/v6.1, 2022. 2, 7, 10\\n[24] Shuxuan Guo, Jose M Alvarez, and Mathieu Salzmann. Ex-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 11}),\n",
       " Document(page_content='pandNets: Linear over-parameterization to train compact\\nconvolutional networks. Advances in Neural Information\\nProcessing Systems (NeurIPS) , 33:1298–1310, 2020. 2\\n[25] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing\\nXu, and Chang Xu. GhostNet: More features from cheap\\noperations. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n1580–1589, 2020. 1\\n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 11}),\n",
       " Document(page_content='Deep residual learning for image recognition. In Proceed-\\n12', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 11}),\n",
       " Document(page_content='ings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR) , pages 770–778, 2016. 1, 4, 5\\n[27] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\\nRuoming Pang, Vijay Vasudevan, et al. Searching for Mo-\\nbileNetV3. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n1314–1324, 2019. 1\\n[28] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 12}),\n",
       " Document(page_content='Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-\\ndreetto, and Hartwig Adam. MobileNets: Efﬁcient con-\\nvolutional neural networks for mobile vision applications.\\narXiv preprint arXiv:1704.04861 , 2017. 1\\n[29] Mu Hu, Junyi Feng, Jiashen Hua, Baisheng Lai, Jian-\\nqiang Huang, Xiaojin Gong, and Xiansheng Hua. On-\\nline convolutional re-parameterization. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , 2022. 2', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 12}),\n",
       " Document(page_content='[30] Miao Hu, Yali Li, Lu Fang, and Shengjin Wang. A2-FPN:\\nAttention aggregation based feature pyramid network for\\ninstance segmentation. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 15343–15352, 2021. 2\\n[31] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E\\nHopcroft, and Kilian Q Weinberger. Snapshot ensembles:\\nTrain 1, get m for free. International Conference on Learn-\\ning Representations (ICLR) , 2017. 2', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 12}),\n",
       " Document(page_content='[32] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\\nian Q Weinberger. Densely connected convolutional net-\\nworks. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n4700–4708, 2017. 2, 4, 5\\n[33] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov,\\nDmitry Vetrov, and Andrew Gordon Wilson. Averaging\\nweights leads to wider optima and better generalization. In\\nConference on Uncertainty in Artiﬁcial Intelligence (UAI) ,\\n2018. 2', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 12}),\n",
       " Document(page_content='2018. 2\\n[34] Paul F Jaeger, Simon AA Kohl, Sebastian Bickel-\\nhaupt, Fabian Isensee, Tristan Anselm Kuder, Heinz-Peter\\nSchlemmer, and Klaus H Maier-Hein. Retina U-Net: Em-\\nbarrassingly simple exploitation of segmentation supervi-\\nsion for medical object detection. In Machine Learning for\\nHealth Workshop , pages 171–183, 2020. 1\\n[35] Hakan Karaoguz and Patric Jensfelt. Object detection ap-\\nproach for robot grasp detection. In IEEE International\\nConference on Robotics and Automation (ICRA) , pages', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 12}),\n",
       " Document(page_content='4953–4959, 2019. 1\\n[36] Kang Kim and Hee Seok Lee. Probabilistic anchor as-\\nsignment with iou prediction for object detection. In Pro-\\nceedings of the European conference on computer vision\\n(ECCV) , pages 355–371, 2020. 5\\n[37] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr\\nDoll´ar. Panoptic feature pyramid networks. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR) , pages 6399–6408, 2019. 2', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 12}),\n",
       " Document(page_content='[38] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou\\nZhang, and Zhuowen Tu. Deeply-supervised nets. In Arti-\\nﬁcial Intelligence and Statistics , pages 562–570, 2015. 5[39] Youngwan Lee, Joong-won Hwang, Sangrok Lee, Yuseok\\nBae, and Jongyoul Park. An energy and GPU-computation\\nefﬁcient backbone network for real-time object detection.\\nInProceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition Workshops (CVPRW) ,\\npages 0–0, 2019. 2, 3', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 12}),\n",
       " Document(page_content='[40] Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, and\\nXiaogang Wang. GS3D: An efﬁcient 3d object detection\\nframework for autonomous driving. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 1019–1028, 2019. 1\\n[41] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M\\nNi, and Lei Zhang. DN-DETR: Accelerate detr training\\nby introducing query denoising. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 12}),\n",
       " Document(page_content='Recognition (CVPR) , pages 13619–13627, 2022. 10\\n[42] Shuai Li, Chenhang He, Ruihuang Li, and Lei Zhang. A\\ndual weighting label assignment scheme for object detec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 9387–\\n9396, 2022. 2, 5\\n[43] Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang,\\nand Jian Yang. Generalized focal loss v2: Learning reliable\\nlocalization quality estimation for dense object detection. In', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 12}),\n",
       " Document(page_content='Proceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition (CVPR) , pages 11632–11641,\\n2021. 5\\n[44] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin\\nHu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal\\nloss: Learning qualiﬁed and distributed bounding boxes for\\ndense object detection. Advances in Neural Information\\nProcessing Systems (NeurIPS) , 33:21002–21012, 2020. 5\\n[45] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 12}),\n",
       " Document(page_content='Exploring plain vision transformer backbones for object de-\\ntection. arXiv preprint arXiv:2203.16527 , 2022. 2\\n[46] Zhuoling Li, Minghui Dong, Shiping Wen, Xiang Hu, Pan\\nZhou, and Zhigang Zeng. CLU-CNNs: Object detection for\\nmedical images. Neurocomputing , 350:53–59, 2019. 1\\n[47] Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang,\\nZhi Tang, Wei Chu, Jingdong Chen, and Haibin Ling. CB-\\nNetV2: A composite backbone network architecture for ob-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 12}),\n",
       " Document(page_content='ject detection. arXiv preprint arXiv:2107.00420 , 2021. 5,\\n10\\n[48] Ji Lin, Wei-Ming Chen, Han Cai, Chuang Gan, and Song\\nHan. Memory-efﬁcient patch-based inference for tiny deep\\nlearning. Advances in Neural Information Processing Sys-\\ntems (NeurIPS) , 34:2346–2358, 2021. 1\\n[49] Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song\\nHan, et al. MCUNet: Tiny deep learning on IoT de-\\nvices. Advances in Neural Information Processing Systems\\n(NeurIPS) , 33:11711–11722, 2020. 1', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 12}),\n",
       " Document(page_content='[50] Yuxuan Liu, Lujia Wang, and Ming Liu. YOLOStereo3D:\\nA step back to 2D for efﬁcient stereo 3D detection. In\\nIEEE International Conference on Robotics and Automa-\\ntion (ICRA) , pages 13018–13024, 2021. 5\\n[51] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong,\\n13', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 12}),\n",
       " Document(page_content='et al. Swin transformer v2: Scaling up capacity and res-\\nolution. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , 2022. 2\\n[52] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\\nHierarchical vision transformer using shifted windows. In\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision (ICCV) , pages 10012–10022, 2021. 10', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 13}),\n",
       " Document(page_content='[53] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\\nenhofer, Trevor Darrell, and Saining Xie. A ConvNet for\\nthe 2020s. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n11976–11986, 2022. 10\\n[54] Rangi Lyu. NanoDet-Plus. https://github.com/RangiLyu/\\nnanodet/releases/tag/v1.0.0-alpha-1, 2021. 1, 2\\n[55] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian\\nSun. ShufﬂeNet V2: Practical guidelines for efﬁcient CNN', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 13}),\n",
       " Document(page_content='architecture design. In Proceedings of the European Con-\\nference on Computer Vision (ECCV) , pages 116–131, 2018.\\n1, 3\\n[56] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. A ranking-based, balanced loss function unifying\\nclassiﬁcation and localisation in object detection. Advances\\nin Neural Information Processing Systems (NeurIPS) ,\\n33:15534–15545, 2020. 2\\n[57] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. Rank & sort loss for object detection and in-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 13}),\n",
       " Document(page_content='stance segmentation. In Proceedings of the IEEE/CVF In-\\nternational Conference on Computer Vision (ICCV) , pages\\n3009–3018, 2021. 2\\n[58] Shuvo Kumar Paul, Muhammed Tawﬁq Chowdhury,\\nMircea Nicolescu, Monica Nicolescu, and David Feil-\\nSeifer. Object detection and pose estimation from rgb and\\ndepth data for real-time, adaptive robotic grasping. In Ad-\\nvances in Computer Vision and Computational Biology ,\\npages 121–142. 2021. 1\\n[59] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. De-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 13}),\n",
       " Document(page_content='tectoRS: Detecting objects with recursive feature pyramid\\nand switchable atrous convolution. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 10213–10224, 2021. 2\\n[60] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\\nKaiming He, and Piotr Doll ´ar. Designing network design\\nspaces. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n10428–10436, 2020. 2', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 13}),\n",
       " Document(page_content='[61] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\\nFarhadi. You only look once: Uniﬁed, real-time object de-\\ntection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n779–788, 2016. 2, 5\\n[62] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,\\nstronger. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n7263–7271, 2017. 2\\n[63] Joseph Redmon and Ali Farhadi. YOLOv3: An incremental', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 13}),\n",
       " Document(page_content='improvement. arXiv preprint arXiv:1804.02767 , 2018. 1, 2[64] Hamid Rezatoﬁghi, Nathan Tsoi, JunYoung Gwak, Amir\\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\\ntersection over union: A metric and a loss for bounding\\nbox regression. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR) ,\\npages 658–666, 2019. 2\\n[65] Byungseok Roh, JaeWoong Shin, Wuhyun Shin, and\\nSaehoon Kim. Sparse DETR: Efﬁcient end-to-end ob-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 13}),\n",
       " Document(page_content='ject detection with learnable sparsity. arXiv preprint\\narXiv:2111.14330 , 2021. 5\\n[66] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey\\nZhmoginov, and Liang-Chieh Chen. MobileNetV2: In-\\nverted residuals and linear bottlenecks. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 4510–4520, 2018. 1\\n[67] Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang,\\nYurong Chen, and Xiangyang Xue. Object detection', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 13}),\n",
       " Document(page_content='from scratch with deep supervision. IEEE Transactions\\non Pattern Analysis and Machine Intelligence (TPAMI) ,\\n42(2):398–412, 2019. 5\\n[68] Karen Simonyan and Andrew Zisserman. Very deep convo-\\nlutional networks for large-scale image recognition. arXiv\\npreprint arXiv:1409.1556 , 2014. 4\\n[69] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng\\nXu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan,\\nChanghu Wang, et al. Sparse R-CNN: End-to-end ob-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 13}),\n",
       " Document(page_content='ject detection with learnable proposals. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 14454–14463, 2021. 2\\n[70] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich. Going deeper with\\nconvolutions. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR) ,\\npages 1–9, 2015. 5', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 13}),\n",
       " Document(page_content='pages 1–9, 2015. 5\\n[71] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\\nShlens, and Zbigniew Wojna. Rethinking the inception\\narchitecture for computer vision. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 2818–2826, 2016. 2\\n[72] Mingxing Tan and Quoc Le. EfﬁcientNet: Rethinking\\nmodel scaling for convolutional neural networks. In Inter-\\nnational Conference on Machine Learning (ICML) , pages\\n6105–6114, 2019. 2, 3', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 13}),\n",
       " Document(page_content='[73] Mingxing Tan and Quoc Le. EfﬁcientNetv2: Smaller mod-\\nels and faster training. In International Conference on Ma-\\nchine Learning (ICML) , pages 10096–10106, 2021. 2\\n[74] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efﬁcient-\\nDet: Scalable and efﬁcient object detection. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition (CVPR) , pages 10781–10790, 2020. 2, 10\\n[75] Antti Tarvainen and Harri Valpola. Mean teachers are better', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 13}),\n",
       " Document(page_content='role models: Weight-averaged consistency targets improve\\nsemi-supervised deep learning results. Advances in Neural\\nInformation Processing Systems (NeurIPS) , 30, 2017. 2, 6\\n[76] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nFully convolutional one-stage object detection. In Proceed-\\n14', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 13}),\n",
       " Document(page_content='ings of the IEEE/CVF International Conference on Com-\\nputer Vision (ICCV) , pages 9627–9636, 2019. 2\\n[77] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nA simple and strong anchor-free object detector. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\n(TPAMI) , 44(4):1922–1933, 2022. 2\\n[78] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff\\nZhu, Oncel Tuzel, and Anurag Ranjan. An im-\\nproved one millisecond mobile backbone. arXiv preprint\\narXiv:2206.04040 , 2022. 2', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 14}),\n",
       " Document(page_content='[79] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\\nYuan Mark Liao. Scaled-YOLOv4: Scaling cross stage\\npartial network. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR) ,\\npages 13029–13038, 2021. 2, 3, 6, 7\\n[80] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,\\nPing-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. CSP-\\nNet: A new backbone that can enhance learning capabil-\\nity of CNN. In Proceedings of the IEEE/CVF Conference', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 14}),\n",
       " Document(page_content='on Computer Vision and Pattern Recognition Workshops\\n(CVPRW) , pages 390–391, 2020. 1\\n[81] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao.\\nYou only learn one representation: Uniﬁed network for\\nmultiple tasks. arXiv preprint arXiv:2105.04206 , 2021. 1,\\n2, 6, 7, 10\\n[82] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian\\nSun, and Nanning Zheng. End-to-end object detection\\nwith fully convolutional network. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 14}),\n",
       " Document(page_content='Recognition (CVPR) , pages 15849–15858, 2021. 2, 5\\n[83] Bichen Wu, Chaojian Li, Hang Zhang, Xiaoliang Dai,\\nPeizhao Zhang, Matthew Yu, Jialiang Wang, Yingyan Lin,\\nand Peter Vajda. FBNetv5: Neural architecture search for\\nmultiple tasks in one run. arXiv preprint arXiv:2111.10007 ,\\n2021. 1\\n[84] Yunyang Xiong, Hanxiao Liu, Suyog Gupta, Berkin Akin,\\nGabriel Bender, Yongzhe Wang, Pieter-Jan Kindermans,\\nMingxing Tan, Vikas Singh, and Bo Chen. MobileDets:', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 14}),\n",
       " Document(page_content='Searching for object detection architectures for mobile ac-\\ncelerators. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n3825–3834, 2021. 1\\n[85] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao\\nChang, Cheng Cui, Kaipeng Deng, Guanzhong Wang,\\nQingqing Dang, Shengyu Wei, Yuning Du, et al. PP-\\nYOLOE: An evolved version of YOLO. arXiv preprint\\narXiv:2203.16250 , 2022. 2, 7, 8, 10\\n[86] Zetong Yang, Yin Zhou, Zhifeng Chen, and Jiquan Ngiam.', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 14}),\n",
       " Document(page_content='3D-MAN: 3D multi-frame attention network for object de-\\ntection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n1863–1872, 2021. 5\\n[87] Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor\\nDarrell. Deep layer aggregation. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 2403–2412, 2018. 1\\n[88] Guanghua Yu, Qinyao Chang, Wenyu Lv, Chang Xu, Cheng', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 14}),\n",
       " Document(page_content='Cui, Wei Ji, Qingqing Dang, Kaipeng Deng, GuanzhongWang, Yuning Du, et al. PP-PicoDet: A better real-\\ntime object detector on mobile devices. arXiv preprint\\narXiv:2111.00902 , 2021. 1\\n[89] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun\\nZhu, Lionel M Ni, and Heung-Yeung Shum. DINO: DETR\\nwith improved denoising anchor boxes for end-to-end ob-\\nject detection. arXiv preprint arXiv:2203.03605 , 2022. 10\\n[90] Haoyang Zhang, Ying Wang, Feras Dayoub, and Niko Sun-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 14}),\n",
       " Document(page_content='derhauf. VarifocalNet: An IoU-aware dense object detector.\\nInProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR) , pages 8514–8523,\\n2021. 5\\n[91] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and\\nStan Z Li. Bridging the gap between anchor-based and\\nanchor-free detection via adaptive training sample selec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 9759–\\n9768, 2020. 5', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 14}),\n",
       " Document(page_content='9768, 2020. 5\\n[92] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian\\nSun. ShufﬂeNet: An extremely efﬁcient convolutional neu-\\nral network for mobile devices. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 6848–6856, 2018. 1\\n[93] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Zehuan\\nYuan, Ping Luo, Wenyu Liu, and Xinggang Wang. BYTE-\\nTrack: Multi-object tracking by associating every detection\\nbox. arXiv preprint arXiv:2110.06864 , 2021. 1', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 14}),\n",
       " Document(page_content='[94] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,\\nand Wenyu Liu. FAIRMOT: On the fairness of detec-\\ntion and re-identiﬁcation in multiple object tracking. Inter-\\nnational Journal of Computer Vision , 129(11):3069–3087,\\n2021. 1\\n[95] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang\\nYe, and Dongwei Ren. Distance-IoU loss: Faster and bet-\\nter learning for bounding box regression. In Proceedings\\nof the AAAI Conference on Artiﬁcial Intelligence (AAAI) ,', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 14}),\n",
       " Document(page_content='volume 34, pages 12993–13000, 2020. 2\\n[96] Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, Junbo\\nYin, Yuchao Dai, and Ruigang Yang. IoU loss for 2D/3D\\nobject detection. In International Conference on 3D Vision\\n(3DV) , pages 85–94, 2019. 2\\n[97] Xingyi Zhou, Dequan Wang, and Philipp Kr ¨ahenb ¨uhl. Ob-\\njects as points. arXiv preprint arXiv:1904.07850 , 2019. 1,\\n2\\n[98] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima\\nTajbakhsh, and Jianming Liang. UNet++: A nested U-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 14}),\n",
       " Document(page_content='Net architecture for medical image segmentation. In\\nDeep Learning in Medical Image Analysis and Multimodal\\nLearning for Clinical Decision Support , 2018. 5\\n[99] Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong,\\nSongtao Liu, Zeming Li, and Jian Sun. AutoAssign: Differ-\\nentiable label assignment for dense object detection. arXiv\\npreprint arXiv:2007.03496 , 2020. 2, 5\\n[100] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\\nand Jifeng Dai. Deformable DETR: Deformable trans-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 14}),\n",
       " Document(page_content='formers for end-to-end object detection. In Proceedings of\\nthe International Conference on Learning Representations\\n(ICLR) , 2021. 10\\n15', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 14}),\n",
       " Document(page_content='3 grad.illinois.edu/CareerDevelopment Rachel Green  \\n2 1 0  W .  G R E E N  S T . ,  C H A M P A I G N ,  I L  \\n( 2 1 7 )  5 5 5 - 1 2 3 4  •  R S T U D E N T @ I L L I N O I S . E D U  \\nEDUCATION  \\nPhD in English May 20xx \\nUniversity of Illinois at Urbana-Champaign \\nDissertation title:  “Down on the Farm: World War One and the Emergence of Literary  \\nModernism in the American South”  \\nCommittee : Margaret Black, Naomi Blue, John Jay, Robert Roberts (Chair) \\nMA in English  20xx', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 0}),\n",
       " Document(page_content='University of Illinois at Urbana-Champaign \\nBA in English and Communications, summa cum laude  20xx \\nButler University, Indianapolis, IN  \\nTEACHING  & A DVISING   \\nComposition Instructor  20xx-present \\nResearch Writing Program, University of Illinois \\n\\uf0b7Facilitator for seven sections of English composition.\\n\\uf0b7Planned and taught a writing-intensive course based upon current events.\\n\\uf0b7Used instructional technology to enhance pedagogical technique.', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf0b7Taught in part with an innovative, interdisciplinary team-teaching program design.\\nLiterature Instructor 20xx-present \\nDepartment of English, University of Illinois \\n\\uf0b7Instructor of record for two sections of literature, including Major American Authors  and\\nIntroduction to Poetry per semester.\\n\\uf0b7Integrated multimedia and humanities approaches to teaching literature using film and instructional\\ntechnology.\\nCoordinating Group Leader 20xx-20xx \\nResearch Writing Program, University of Illinois', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf0b7Planned and led required training session for teaching assistants and new composition teachers.\\n\\uf0b7Helped to mentor new hires to the English Department staff to ensure their engagement and\\nprofessional development.\\n\\uf0b7Provided job shadowing and training opportunities to assist new hires in adjusting to the pace of\\nwork and the tone and style of the University.\\nDiscussion Leader  20xx \\nCarolina Summer Reading Program, University of Illinois', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf0b7Led group discussion for first-year students on academic topics.\\nTeaching Assistant 20xx-20xx \\nDepartment of English, University of Illinois at Urbana-Champaign \\n\\uf0b7Taught a section on film criticism, including film history, theory and technical vocabulary.\\n\\uf0b7Planned lessons and assignments, led discussion sections, graded papers and exams.\\n\\uf0b7Organized and led group discussions on social and academic issues.CV SAMPLE', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 0}),\n",
       " Document(page_content='4 grad.illinois.edu/CareerDevelopment RESEARCH  EXPERIENCE   \\nDoctoral Researcher 20xx-20xx \\nDepartment of English, University of Illinois at Urbana-Champaign \\n\\uf0b7Conducted primary source research at numerous archives, examining publication history through\\nmultiple sources.\\n\\uf0b7Examined the literature of William Faulkner, Thomas Wolfe, and Tennessee Williams, exploring\\ntheir publication records, construction of literary identity, and relationship with modernism.\\nResearch Assistant 20xx', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 1}),\n",
       " Document(page_content='Department of English, University of Illinois at Urbana-Champaign \\n\\uf0b7Assistant to Professor Robert Warren, conducting primary and secondary source research.\\n\\uf0b7Organized for the “New Directions in the Study of Southern Literature: An Interdisciplinary\\nConference.”\\nPUBLICATIONS   \\nAssociate Editor of North Carolina Slave Narratives. John Jacob Franz, general editor. Forthcoming \\nfrom University of Illinois Press, 20xx.', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 1}),\n",
       " Document(page_content='Johnson, JM, Lolie, T., and Green, R.  “Lost on the Farm: Popular Beliefs” Somebody Journal, Special \\nIssue, Reflections on the Americas. Vol. 6. Accepted and forthcoming. \\nGreen, R. “Fugitives/Agrarians” in A Companion to Twentieth -Century American Poetry. Rutgers \\nPress., 20xx. \\nDavis, D.A. and Green, R.  “Will N. Harben,” “Etheridge Knight,” and “James Wilcox” in Southern \\nWriters: A Biographical Dictionary. Louisiana State University Press, 20xx. \\nCONFERENCE  PRESENTATIONS', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 1}),\n",
       " Document(page_content='“Artistic Colloquialism,” Illinois Graduate College Seminar, speaker and organizer. Urbana, IL, 20xx.  \\n“Transitional Bible Belt,” US Divergence Symposium, Duke University, NC, February 20xx.  \\n“The Ministry of Rev. Thomas H. Jones,” South Atlantic Modern Language Association. Atlanta, GA, \\nMay 20xx. \\n“Shackles and Stripes: The Cinematic Representation of the Southern Chain Gain.” American Literature \\nAssociation. Cambridge, Massachusetts, November 20xx.', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 1}),\n",
       " Document(page_content='“Body Place of Sprits in the South,” Queen Mary College, University of London, April 6 -8, 20xx. \\nHONORS  AND  AWARDS  \\nJacob K. Javitz Fellowship, U.S. Department of Education 20xx-present \\nGraduate College Dissertation Completion Award, University of Illinois 20xx \\nCampus Teaching Award based on student evaluations, University of Illinois 20xx-20xx \\nDoctoral Fellowship, Illinois Program for Research in the Humanities,  20xx-20xx \\nUniversity of Illinois', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 1}),\n",
       " Document(page_content='Summer Research Grant, Center for Summer Studies, City, ST  20xx \\nGraduate College Conference Travel Grant, University of Illinois 20xx & 20xx \\nMost Outstanding Butler Woman, Butler University, Indianapolis, IN 20xx \\nAcademic Scholarship, Butler University, Indianapolis, IN 20xx-20xx \\nRachel Green, page 2 of 3', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 1}),\n",
       " Document(page_content='5 grad.illinois.edu/CareerDevelopment PROFESSIONAL  SERVICE  \\nManaging Editor 20xx-present \\nSouthern Literary Journal  \\n\\uf0b7Process manuscripts submitted for publication\\n\\uf0b7Oversee production and publication procedures.\\n\\uf0b7Maintain editorial correspondence with prospective contributors.\\n\\uf0b7Conduct business transactions including publicity, subscriptions and advertising.\\nPoetry Staff 20xx-present \\nUniversity Quarterly \\n\\uf0b7Review and solicit poems for possible publication.\\nEditorial Assistant 20xx-20xx', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 2}),\n",
       " Document(page_content='Southern Literary Journal \\n\\uf0b7Designed and maintained journal’s internet presence.\\n\\uf0b7Edited copy for publication on a monthly basis.\\nUNIVERSITY  SERVICE  \\nGraduate Mentor 20xx-20xx \\nThe Career Center, University of Illinois \\n\\uf0b7Counsel minority undergraduates on graduate programs, application procedures and funding.\\nCareer Advisory Committee 20xx-20xx \\nGraduate College, University of Illinois \\n\\uf0b7Served on university committee to evaluate and propose career services for graduate students.', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 2}),\n",
       " Document(page_content='\\uf0b7Collaborated with faculty and students to prepare final report for submission to the Graduate\\nCollege Dean.\\nUniversity Library Advisory Committee 20xx-20xx \\nUndergraduate Library, University of Illinois \\n\\uf0b7Advised University Librarian on needed services and improvements.\\nPROFESSIONAL  MEMBERSHIPS  \\n\\uf0b7Modern Language Association (MLA)\\n\\uf0b7American Literature Association (ALA)\\n\\uf0b7American Studies Association (ASA)\\n\\uf0b7South Atlantic Modern Language Association', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 2}),\n",
       " Document(page_content='(samla)\\uf0b7Society for the Study of Southern Literature\\n\\uf0b7Robert Penn Warren Circle\\n\\uf0b7Southern Research Circle\\n\\uf0b7Fellowship of Southern Writers\\nREFERENCES  \\nJohn Jay , Assoc. Professor of English \\nUniversity of Illinois at Urbana-Champaign \\n(217) 333-1112, jjay@illinois.eduJacob S. Snyder , Assoc. Professor of English \\nUniversity of Illinois at Urbana-Champaign \\n(217) 333-4700, jssnyd@illinois.edu\\nRobert Roberts , Professor of English \\nUniversity of Illinois at Urbana-Champaign', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 2}),\n",
       " Document(page_content='(217) 333-0203, rrobe3@illinois.eduSally Briscoe, Assoc. Professor of English \\nButler University, Indianapolis, IN \\n(317) 492-8763, briscoe@butler.edu\\nRachel Green, page 3 of 3', metadata={'source': 'pdfs/rachelgreecv.pdf', 'page': 2})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='real-time object detectors with 30 FPS or higher on GPU\\nV100. YOLOv7-E6 object detector (56 FPS V100, 55.9%\\nAP) outperforms both transformer-based detector SWIN-\\nL Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by\\n509% in speed and 2% in accuracy, and convolutional-\\nbased detector ConvNeXt-XL Cascade-Mask R-CNN (8.6\\nFPS A100, 55.2% AP) by 551% in speed and 0.7% AP\\nin accuracy, as well as YOLOv7 outperforms: YOLOR,\\nYOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='DETR, DINO-5scale-R50, ViT-Adapter-B and many other\\nobject detectors in speed and accuracy. Moreover, we train\\nYOLOv7 only on MS COCO dataset from scratch without\\nusing any other datasets or pre-trained weights. Source\\ncode is released in https://github.com/WongKinYiu/yolov7.\\n1. Introduction\\nReal-time object detection is a very important topic in\\ncomputer vision, as it is often a necessary component in\\ncomputer vision systems. For example, multi-object track-', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_chunks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='ing [94, 93], autonomous driving [40, 18], robotics [35, 58],\\nmedical image analysis [34, 46], etc. The computing de-\\nvices that execute real-time object detection is usually some\\nmobile CPU or GPU, as well as various neural processing\\nunits (NPU) developed by major manufacturers. For exam-\\nple, the Apple neural engine (Apple), the neural compute\\nstick (Intel), Jetson AI edge devices (Nvidia), the edge TPU\\n(Google), the neural processing engine (Qualcomm), the AI', metadata={'source': 'pdfs/yolov7paper.pdf', 'page': 0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_chunks[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downlaod the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = embeddings.embed_query(\"How are you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', '')\n",
    "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_API_ENV  # next to api key in console\n",
    ")\n",
    "index_name = \"test\" # put in the name of your pinecone index here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Embeddings for each of the Text Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Pinecone.from_texts([t.page_content for t in text_chunks], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you already have an index, you can load it like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.pinecone.Pinecone at 0x78f9e0e67460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
    "docsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"YOLOv7 outperforms which models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docsearch.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='YOLOv7-tiny 6.2 3.5 320 30.8% 47.3% 32.2% 10.0% 31.9% 52.2%\\nimprovement -39% -49% - = = = -0.9 = +0.7\\nYOLOR-E6 [81] 115.8M 683.2G 1280 55.7% 73.2% 60.7% 40.1% 60.4% 69.2%\\nYOLOv7-E6 97.2M 515.2G 1280 55.9% 73.5% 61.1% 40.6% 60.3% 70.0%\\nimprovement -19% -33% - +0.2 +0.3 +0.4 +0.5 -0.1 +0.8\\nYOLOR-D6 [81] 151.7M 935.6G 1280 56.1% 73.9% 61.2% 42.4% 60.5% 69.9%\\nYOLOv7-D6 154.7M 806.8G 1280 56.3% 73.8% 61.4% 41.3% 60.6% 70.1%\\nYOLOv7-E6E 151.7M 843.2G 1280 56.8% 74.4% 62.1% 40.8% 62.1% 70.6%'),\n",
       " Document(page_content='YOLOv5-L6 (r6.1) [23] 76.8M 445.6G 1280 63 - / 53.7% - -\\nYOLOX-X [21] 99.1M 281.9G 640 58 51.5% / 51.1% - -\\nYOLOv7-E6 97.2M 515.2G 1280 56 56.0% /55.9% 73.5% 61.2%\\nYOLOR-E6 [81] 115.8M 683.2G 1280 45 55.8% / 55.7% 73.4% 61.1%\\nPPYOLOE-X [85] 98.4M 206.6G 640 45 52.2% / 51.9% 69.9% 56.5%\\nYOLOv7-D6 154.7M 806.8G 1280 44 56.6% /56.3% 74.0% 61.8%\\nYOLOv5-X6 (r6.1) [23] 140.7M 839.2G 1280 38 - / 55.0% - -\\nYOLOv7-E6E 151.7M 843.2G 1280 36 56.8% /56.8% 74.4% 62.1%'),\n",
       " Document(page_content='YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\\ndetectors\\nChien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\\n1Institute of Information Science, Academia Sinica, Taiwan\\nkinyiu@iis.sinica.edu.tw, alexeyab84@gmail.com, and liao@iis.sinica.edu.tw\\nAbstract\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS\\nand has the highest accuracy 56.8% AP among all known')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a LLM Model Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"YOLOv7 outperforms which models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' YOLOv7 outperforms YOLOv5-L6 (r6.1), YOLOX-X, YOLOR-E6, PPYOLOE-X, YOLOv7-D6, YOLOv5-X6 (r6.1), YOLOv7-E6E, YOLOv5-X (r6.1), YOLOR-CSP, YOLOR-CSP-X, YOLOv7-tiny-SiLU, YOLOv7, and YOLOv7-X.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Rachel Green Experience\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Rachel Green has a PhD in English from the University of Illinois at Urbana-Champaign. Her dissertation title was \"Down on the Farm: World War One and the Emergence of Literary Modernism in the American South\". She also holds an MA in English from Butler University, and has received a Summer Research Grant from the Center for Summer Studies, a Graduate College Conference Travel Grant from the University of Illinois, the Most Outstanding Butler Woman award from Butler University, and an Academic Scholarship from Butler University. She has published multiple works, and has presented at conferences.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Prompt: what is yolo v7\n",
      "Answer:  YOLOv7 is a real-time object detector which surpasses all known object detectors in both speed and accuracy. It has the highest accuracy of 56.8% AP among all known detectors and can run from 5 FPS to 160 FPS.\n",
      "Input Prompt: tell me about Rechel Green\n",
      "Answer:  Rachel Green is a PhD in English from the University of Illinois at Urbana-Champaign. Her dissertation title was “Down on the Farm: World War One and the Emergence of Literary Modernism in the American South.” She also has a MA in English and was awarded a Summer Research Grant, a Graduate College Conference Travel Grant, Most Outstanding Butler Woman, and an Academic Scholarship. She has published extensively and has given multiple conference presentations.\n",
      "Input Prompt: exit\n",
      "Exiting\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "  user_input = input(f\"Input Prompt: \")\n",
    "  if user_input == 'exit':\n",
    "    print('Exiting')\n",
    "    sys.exit()\n",
    "  if user_input == '':\n",
    "    continue\n",
    "  result = qa({'query': user_input})\n",
    "  print(f\"Answer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/479.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/479.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m479.8/479.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.9/103.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "lida 0.0.10 requires kaleido, which is not installed.\n",
      "lida 0.0.10 requires python-multipart, which is not installed.\n",
      "llmx 0.0.15a0 requires cohere, which is not installed.\n",
      "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install chromadb openai langchain tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: chromadb\n",
      "Version: 0.4.15\n",
      "Summary: Chroma.\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Jeff Huber <jeff@trychroma.com>, Anton Troynikov <anton@trychroma.com>\n",
      "License: \n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: bcrypt, chroma-hnswlib, fastapi, grpcio, importlib-resources, kubernetes, numpy, onnxruntime, opentelemetry-api, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-sdk, overrides, posthog, pulsar-client, pydantic, pypika, requests, tenacity, tokenizers, tqdm, typer, typing-extensions, uvicorn\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q new_articles.zip -d new_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\"/content/new_articles/\", glob = \"./*.txt\", loader_cls= TextLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Google I/O 2023 is next week; here’s what we’re expecting A whole bunch of new hardware, coupled with a lot of AI and the best look yet at Android 14\\n\\nGoogle’s annual developer conference, Google I/O, returns to Mountain View’s Shoreline Amphitheater next week, and for the first time in four years, we’ll be returning along with it. The kickoff keynote is always jammed-packed full of information, debuting all of the different software projects the company has been working on for the past year.\\n\\nUpdate: Google just went ahead and announced the Pixel Fold over on Twitter. The company gave a good look at the upcoming foldable smartphone from just about every angle. That means all three of the expected pieces of hardware – including the Pixel 7a and Pixel Tablet – have officially been announced.\\n\\nThe event, which kicks off May 10 at 10 AM PT will be a big showcase for everything that’s on the way for Android 14. The company has, arguably, missed a step when it comes to the current generative AI land rush — hell, who could have predicted after all of these years that Bing would finally have a moment?\\n\\nCEO Sundar Pichai will no doubt be making the case that the company continues to lead the way in the world of artificial intelligence. There’s always been a fair bit of the stuff at the event largely focused on practical real-world applications like mobile imaging and dealing with customer service. This year, however, I’d say it’s safe to say the company is going to go bonkers with the stuff.\\n\\nHardware, meanwhile, is always a bit of a crapshoot at developer conferences. But after an off-year for the industry at large, a deluge of rumors are aligning, pointing to what’s likely to be an unusually consumer electronics-focused keynote. Given the fact that the last bit is my focus at TechCrunch, I’m going to start the list there.\\n\\nThe Pixel 7a is about as sure as bets get. Google has settled into a comfortable release cadence: releasing a flagship in the fall, followed by a budget device in the spring. The former is designed to be an ideal showcase for its latest mobile operating system and first-party silicon, while the latter makes some compromises for price, while maintaining as many of its predecessors as possible.\\n\\nHow to show excitement without shouting? Asking for a friend Coming to @Flipkart on 11th May. pic.twitter.com/il6GUx3MmR — Google India (@GoogleIndia) May 2, 2023\\n\\nIt’s a good system that works, and Google’s newly focused mobile hardware team has created some surprisingly good devices at extremely reasonable prices. Never one to be outdone by the deluge of rumors, the company went ahead and announced via Twitter its next device is due out on May 11 — the day after Google I/O and, perhaps not coincidentally, my birthday. It was Google India that specifically made the announcement — perhaps not surprising, as the company is likely to aggressively target the world’s number one smartphone market with the product. The image points to a very similar design as the 7 — not really a surprise as these things go. Though it does stop short of actually mentioning the name, as it’s done in the past.\\n\\nBasically expect the 7 with cheaper materials. Rumors point to a 6.1-inch device featuring a 90Hz refresh rate, coupled with a 64-megapixel rear camera. The 7’s Tensor G2 returns for a command performance, likely bringing with it many of the software features it enabled the first time around.\\n\\nWe know for sure that a Pixel Tablet is coming…at some point. Google confirmed the device’s existence at last year’s event, providing a broad 2023 release date, along with a render alongside the rest of the current Pixel lineup. Effectively there are two points this year Google is likely to officially announce the thing: next week or September/October. I would be shocked if the company’s long-awaited (?) reentry into the category doesn’t, at the very least, get a bit of stage time. As a category, the Android tablet has been very hit or miss over the years — presumably/hopefully the company’s got a unique spin here. I would be surprised if Google jumped back into the space without some sort of novel angle.\\n\\nThe leaks point to a design that would effectively turn the system into one giant Nest dock. It’s not entirely original, as Amazon tried something similar with its Fire tablets, but it would certainly buck the iPad model, which is so pervasive in the industry. Other rumors include the aforementioned Tensor G2, coupled with 8GB of RAM.\\n\\nHere’s your wildcard, folks: the Pixel Fold. Google has seemingly been laying the groundwork for its own foldable for years. Here’s what I wrote a couple of weeks ago:\\n\\nSome important background here. First, Google announced foldable screen support for Android back in 2018. Obviously, Samsung was both the big partner and recipient in those days, and Google wanted to make Android development as frictionless as possible for other OEMs in exploring the form factor. The following year, Google foldable patents surfaced. Now, we’re all adults here, who implicitly understand that patents don’t mean a company is working on a product. That said, it’s another key data point in this story. In the intervening years, foldables have begun gathering steam, even outside of the Samsung orbit. I was genuinely amazed by how many different models there were populating the halls of MWC back in March. The leaked renders point to a form factor that is more Samsung Galaxy Z Fold than Samsung Galaxy Z Flip. It also looks like it shares some common design DNA with Oppo’s recently foldable, which is frankly the right direction. EV Leaks says the foldable is half an inch thick when folded and 0.2 inches unfolded, weight in at 283 grams.\\n\\nAs evidenced by our trip to MWC back in February, foldables are no longer fringe devices. It’s true that they’re still cost-prohibitive for most, but it’s getting to the point soon where nearly ever Android manufacturer will have their take on the category. So why shouldn’t Google?\\n\\nOther less likely hardware rumors include a Google/Nest AirTag competitor (the company announced yesterday that it’s working with Apple to create a standard for the category), new Pixel Buds and a Pixel Watch 2. I’d say all are unlikely — that last one in particular. We didn’t get much in terms of Nest products last year, but so far not much is forthcoming in terms of rumors for home products.\\n\\nAndroid is always a tentpole of Google I/O for obvious reasons. We’ve already caught some major glimpses of the mobile operating system, by way of beta releases. As Frederic noted in March, “So far, most of the features Google has talked about have also been developer-centric, with only a few user-facing features exposed to far. That also holds true for this second preview, which mostly focuses on added new security and privacy features.”\\n\\nThe operating system, which is apparently named Upside Down Cake internally, is likely set for a summer release in late-July or August. At the top of the list of potential features are a boost to battery life (can always use one of those), additional accessibility features and privacy/security features, which include blocking users from installing ancient apps over malware concerns.\\n\\nAI is going to be everywhere. Expect generative AI (Bard) in particular to make appearances in virtually every existing piece of Google consumer software, following the lead of Gmail and Docs. Search and the Chrome browser are prime targets here.\\n\\nA preview of a new Wear OS seems likely. I don’t anticipate a ton of news on the AR/VR side of things, but I would also be surprised if it doesn’t at least get a nod, given what Apple reportedly has in the works for June.\\n\\nThe keynote kicks off at 10 AM PT on May 10. As ever, TechCrunch will be bringing you the news as it breaks.', metadata={'source': '/content/new_articles/05-05-google-i-o-2023-is-next-week-heres-what-were-expecting.txt'}),\n",
       " Document(page_content='Signaling that investments in the supply chain sector remain robust, Pando, a startup developing fulfillment management technologies, today announced that it raised $30 million in a Series B round, bringing its total raised to $45 million.\\n\\nIron Pillar and Uncorrelated Ventures led the round, with participation from existing investors Nexus Venture Partners, Chiratae Ventures and Next47. CEO and founder Nitin Jayakrishnan says that the new capital will be put toward expanding Pando’s global sales, marketing and delivery capabilities.\\n\\n“We will not expand into new industries or adjacent product areas,” he told TechCrunch in an email interview. “Great talent is the foundation of the business — we will continue to augment our teams at all levels of the organization. Pando is also open to exploring strategic partnerships and acquisitions with this round of funding.”\\n\\nPando was co-launched by Jayakrishnan and Abhijeet Manohar, who previously worked together at iDelivery, an India-based freight tech marketplace — and their first startup. The two saw firsthand manufacturers, distributors and retailers were struggling with legacy tech and point solutions to understand, optimize and manage their global logistics operations — or at least, that’s the story Jayakrishnan tells.\\n\\n“Supply chain leaders were trying to build their own tech and throwing people at the problem,” he said. “This caught our attention — we spent months talking to and building for enterprise users at warehouses, factories, freight yards and ports and eventually, in 2018, decided to start Pando to solve for global logistics through a software-as-a-service platform offering.”\\n\\nThere’s truth to what Jayakrishnan’s expressing about pent-up demand. According to a recent McKinsey survey, supply chain companies had — and have — a strong desire for tools that deliver greater supply chain visibility. Sixty-seven percent of respondents to the survey say that they’ve implemented dashboards for this purpose, while over half say that they’re investing in supply chain visibility services more broadly.\\n\\nPando aims to meet the need by consolidating supply chain data that resides in multiple silos within and outside of the enterprise, including data on customers, suppliers, logistics service providers, facilities and product SKUs. The platform provides various tools and apps for accomplishing different tasks across freight procurement, trade and transport management, freight audit and payment and document management, as well as dispatch planning and analytics.\\n\\nCustomers can customize the tools and apps or build their own using Pando’s APIs. This, along with the platform’s emphasis on no-code capabilities, differentiates Pando from incumbents like SAP, Oracle, Blue Yonder and E2Open, Jayakrishnan asserts.\\n\\n“Pando comes pre-integrated with leading enterprise resource planning (ERPs) systems and has ready APIs and a professional services team to integrate with any new ERPs and enterprise systems,” he added. “Pando’s no-code capabilities enable business users to customize the apps while maintaining platform integrity — reducing the need for IT resources for each customization.”\\n\\nPando also taps algorithms and forms of machine learning to make predictions around supply chain events. For example, the platform attempts to match customer orders with suppliers, customers through the “right” channel (in terms of aspects like cost and carbon footprint) and fulfillment strategy (e.g. mode of freight, carrier, etc.). Beyond this, Pando can detect anomalies among deliveries, orders and freight invoices and anticipate supply chain risk given demand and supply trends.\\n\\nPando isn’t the only vendor doing this. Altana, which bagged $100 million in venture capital last October, uses an AI system to connect to and learn from logistics and business-to-business data — creating a shared view of supply chain networks. Everstream, another Pando rival, offers its own dashboards for data analysis, integrated with existing ERP, transportation management and supplier relationship management systems.\\n\\nBut Pando has a compelling sales pitch, judging by its momentum. The company counts Fortune 500 manufacturers and retailers — including P&G, J&J, Valvoline, Castrol, Cummins, Siemens, Danaher and Accuride — among its customer base. Since the startup’s Series A in 2020, revenue has grown 8x while the number of customers has increased 5x, Jayakrishnan said.\\n\\nAsked whether he expects expansion to continue well into the future, given the signs of potential trouble on the horizon, Jayakrishnan seemed fairly optimistic. He pointed to a Deloitte survey that found that more than 70% of manufacturing companies have been impacted by supply chain disruptions in the past year, with 90% of those companies experiencing increased costs and declining productivity.\\n\\nThe result of those major disruptions? The digital logistics market is estimated to climb to $46.5 billion by 2025, per Markets and Markets — up from $17.4 billion in 2019. Crunchbase reports that investors poured more than $7 billion in seed through growth-stage rounds globally for supply chain-focused startups from January to October 2022, nearly eclipsing 2021’s record-setting levels.\\n\\n“Pando has a strong balance sheet and profit and loss statement, with an eye on profitable growth,” Jayakrishnan said. “We’re are scaling operations in North America, Europe and India with marquee customer wins and a network of strong partners … Pando is well-positioned to ride this growth wave, and drive supply chain agility for the 2030 economy.”', metadata={'source': '/content/new_articles/05-03-ai-powered-supply-chain-startup-pando-lands-30m-investment.txt'}),\n",
       " Document(page_content='As brands incorporate generative AI into their creative workflows to generate new content associated with the company, they need to tread carefully to be sure that the new material adheres to the company’s style and brand guidelines.\\n\\nNova is an early-stage startup building a suite of generative AI tools designed to protect brand integrity, and today, the company is announcing two new products to help brands police AI-generated content: BrandGuard and BrandGPT.\\n\\nWith BrandGuard, you ingest your company’s brand guidelines and style guide, and with a series of models Nova has created, it can check the content against those rules to make sure it’s in compliance, while BrandGPT lets you ask questions about the brand’s content rules in ChatGPT style.\\n\\nRob May, founder and CEO at the company, who previously founded Backupify, a cloud backup startup that was acquired by Datto back in 2014, recognized that companies wanted to start taking advantage of generative AI technology to create content faster, but they still worried about maintaining brand integrity, so he came up with the idea of building a guard rail system to protect the brand from generative AI mishaps.\\n\\n“We heard from multiple CMOs who were worried about ‘how do I know this AI-generated content is on brand?’ So we built this architecture that we’re launching called BrandGuard, which is a really interesting series of models, along with BrandGPT, which acts as an interface on top of the models,” May told TechCrunch.\\n\\nBrandGuard is like the back end for this brand protection system. Nova built five models that look for things that might seem out of whack. They run checks for brand safety, quality checking, whether it’s on brand, whether it adheres to style and whether it’s on campaign. Then it assigns each piece with a content score, and each company can decide what the threshold is for calling in a human to check the content before publishing.\\n\\n“When you have generative AI creating stuff, you can now score it on a continuum. And then you can set thresholds, and if something’s below, say 85% on brand, you can have the system flag it so that a human can take a look at it,” he said. Companies can decide whatever threshold they’re comfortable with.\\n\\nBrandGPT is designed for working with third parties like an agency or a contractor, who can ask questions about the company’s brand guidelines to make sure they are complying with them, May said. “We’re launching BrandGPT, which is meant to be the interface to all this brand-related security stuff that we’re doing, and as people interact with brands, they can access the style guides and better understand the brand, whether they’re a part of the company or not.\\n\\nThese two products are available in public beta starting today. The company launched last year and has raised $2.4 million from Bee Partners, Fyrfly Ventures and Argon Ventures.', metadata={'source': '/content/new_articles/05-03-nova-is-building-guardrails-for-generative-ai-content-to-protect-brand-integrity.txt'}),\n",
       " Document(page_content='Welcome back to This Week in Apps, the weekly TechCrunch series that recaps the latest in mobile OS news, mobile applications and the overall app economy.\\n\\nThe app economy in 2023 hit a few snags, as consumer spending last year dropped for the first time by 2% to $167 billion, according to data.ai’s “State of Mobile” report. However, downloads are continuing to grow, up 11% year-over-year in 2022 to reach 255 billion. Consumers are also spending more time in mobile apps than ever before. On Android devices alone, hours spent in 2022 grew 9%, reaching 4.1 trillion.\\n\\nThis Week in Apps offers a way to keep up with this fast-moving industry in one place with the latest from the world of apps, including news, updates, startup fundings, mergers and acquisitions, and much more.\\n\\nDo you want This Week in Apps in your inbox every Saturday? Sign up here: techcrunch.com/newsletters\\n\\nTop Stories\\n\\nDorsey criticizes Twitter, Musk on the alternative social networks he’s backing\\n\\nAs demand for Bluesky, the Jack Dorsey-backed decentralized Twitter rival grows, the former Twitter CEO took to the app to share his thoughts on Twitter’s future, Elon Musk and the decision to take the company private. As TechCrunch’s Darrell Etherington reported, Dorsey responded to questions posed to him from other users and reporters on Bluesky, including one where he was asked if Musk has proven to be the best possible steward for the social network.\\n\\nDorsey said he had not:\\n\\nNo. Nor do I think he acted right after realizing his timing was bad. Nor do I think the board should have forced the sale. It all went south. But it happened and all we can do now is build something to avoid that ever happening again. So I’m happy Jay and team and nostr devs exist and building it.\\n\\nHowever, the Twitter co-founder stressed that Twitter would have never survived as a public company and defended himself from an accusation that he was deflecting blame for Twitter’s current situation.\\n\\nThough Bluesky is having a moment, particularly as a haven for marginalized groups, sex workers and trans users, it’s not the only Twitter alternative Dorsey is now backing. In fact, he’s been more active in recent days on the social network nostr (which he also financially backed), where he’s also been critical of some of Musk’s recent decisions. For example, as The NYT reported, Dorsey posted last month “This is weak,” in response to Musk’s move to stop Twitter users from linking to Substack after it launched a Twitter-like service for its own community of writers and readers.\\n\\nDorsey also touted his belief in these platforms during Block’s recent earnings call, suggesting on his nostr profile this may be the first time the network’s name had been mentioned during a public earnings event.\\n\\n“Open protocols represent another fork in the road moment for people and companies,” Dorsey told investors. “Bitcoin, nostr, Bluesky, web5 and others are all working to level the playing field for competition and give individuals and organizations entirely new capabilities,” he added.\\n\\nOver the past few weeks, Bluesky has been gaining traction, but the network has been difficult to access due to its invite-only nature. That’s turned Bluesky invites into hot commodities, where they’re even selling for hundreds of dollars on eBay, as most users have to wait to receive only one invite every two weeks.\\n\\nBluesky leadership will also sometimes gift a user with a larger number of invites in order to have them invite members of a specific community. Developers who can demonstrate they’re building a Bluesky app may also request additional invites, we understand.\\n\\nThe network has received outsized press coverage relative to its size — just 50,000+ users — possibly because of the heavy infusion of tech journalists on there and Dorsey’s name attached. But the reality is that Bluesky’s future remains uncertain. The company, for now, is able to build and grow thanks to the $13 million in initial funds it received from Twitter, where it was incubated under Dorsey’s leadership. It has since spun out into its own, independent company (a public benefit LLC). It’s unclear how Bluesky intends to maintain its operations in the long term, not to mention its freewheeling culture and accepting community. Networks can often be pleasant and welcoming when small, like Bluesky — or early Twitter, for that matter — but face challenges once they scale to millions of users.\\n\\nNewFronts round-up\\n\\nThis week was IAB’s NewFronts, where digital media companies and social networks pitched their platforms to advertisers looking to reach online audiences. The event saw major brands introducing a range of new offerings, including both ad products and formats, as well as touting their latest features, in some cases, as Snap did with its My AI integration.\\n\\nHere’s what you may have missed from the app makers’ NewFronts this week:\\n\\nSnap said it’s beginning to test a feature that lets partners leverage its new My AI chatbot to place sponsored links in front of users. Snap also announced new ad slots, including the option to reserve the first video ad seen in Snapchat’s Friend Stories and the ability to advertise within its TikTok-like Spotlight feature.\\n\\nSnap also announced including the option to reserve the and the ability to YouTube introduced new ad opportunities for Shorts, including the expansion of Shorts into Video reach campaigns that leverage Google AI to serve the best combination of ads and improve reach on YouTube. Plus, YouTube Select is now coming to Shorts, allowing advertisers to place their ads alongside the most popular YouTube Shorts’ content, similar to TikTok Pulse. Another option, First Position on Shorts, will let advertisers be the first ad Shorts users see in their viewing session.\\n\\nincluding the that leverage Google AI to serve the best combination of ads and improve reach on YouTube. Plus, allowing advertisers to place their ads alongside the most popular YouTube Shorts’ content, similar to TikTok Pulse. Another option, will let advertisers be the first ad Shorts users see in their viewing session. TikTok announced partnerships with big-name publishers, including NBCU, Condé Nast, DotDash Meredith, BuzzFeed and others, in an effort to pull in more premium ad dollars. The new premium ad product, Pulse Premiere, would allow marketers, for the first time, to position their brand ads directly after TikTok’s publisher and media partners’ content in over a dozen categories, including lifestyle, sports, entertainment, education and more. Publisher partners would receive a rev share as a result.\\n\\nThe would allow marketers, for the first time, to position their brand ads directly after TikTok’s publisher and media partners’ content in over a dozen categories, including lifestyle, sports, entertainment, education and more. Publisher partners would receive a rev share as a result. Meta announced AR would become available to Reels Ads and Facebook Stories. They had previously been available only to the Facebook Feed, Instagram Feed and Instagram Stories. It also announced features to make Reels Ads more interactive, including a t est of a larger “call to action” button with additional advertiser information on Facebook and Instagram Reels ads. Other updates included multi-destination product ads, the ability to pause a video ad to preview a link’s destination and support for Reels Ads campaigns with select third-party measurement firms .\\n\\nThey had previously been available only to the Facebook Feed, Instagram Feed and Instagram Stories. It also announced features to make Reels Ads more interactive, including a t with additional advertiser information on Facebook and Instagram Reels ads. Other updates included the ability to and support for . NBCU will let Peacock users shop products that appear in its content through “Must ShopTV,” which puts a QR code on the screen when a shoppable product appears.\\n\\nApple & Google team up on Bluetooth tracker safety\\n\\nAfter numerous cases of Bluetooth trackers like Apple’s AirTag being used for stalking or other criminal apps, Apple and Google this week released a joint announcement saying they will work together to lead an industry-wide initiative to draft a specification that would alert users in the case of unwanted tracking from Bluetooth devices. The companies said they’re seeking input from other industry participants and advocacy groups in the matter, and noted that other tracker makers like Samsung, Tile, Chipolo, eufy Security and Pebblebee have also expressed interest in the draft.\\n\\nThe companies submitted a proposed specification as an Internet-Draft via a standards development organization, the Internet Engineering Task Force (IETF). Other interested parties are now being invited to review and comment over the next three months. After this time, Apple and Google will offer feedback and will release a production implementation of the specification by year’s end that will be supported in future versions of iOS and Android, they said.\\n\\nThe spec would build on the AirTag protections Apple had already released but also, critically, would ensure that users would be able to combat unwanted tracking by offering tools across both iOS and Android platforms.\\n\\nGoogle’s participation could signal more than a desire to protect its users — it’s been rumored the company may also be developing an AirTag rival.\\n\\nPlatforms\\n\\nApple\\n\\nGoogle — I/O Preview\\n\\nGoogle I/O kicks off next week and we already know at least one of the announcements — because Google leaked it. The company plans to introduce its first foldable smartphone with the Pixel Fold. The device shares Pixel’s familiar camera bar and features an interface that showcases Material UI design. We expect to learn more at the event.\\n\\nIn addition, Google I/O 2023 should bring a Pixel 7a , a budget device that could also help address Pixel demand in emerging markets, plus possibly a Pixel tablet, an AirTag rival, a Wear OS update, and a lot of new developer tools and features. We also expect to hear quite a bit about Google’s AI plans, with generative AI (like Bard) appearing across Google’s line of products.\\n\\n, a budget device that could also help address Pixel demand in emerging markets, plus possibly a Pixel tablet, an AirTag rival, a Wear OS update, and a lot of new developer tools and features. We also expect to hear quite a bit about Google’s AI plans, with generative AI (like Bard) appearing across Google’s line of products. To get ready for I/O, even if you’re attending virtually, Google offered a new planning guide and a playlist of developer content to help attendees prepare.\\n\\nto help attendees prepare. Checks, Google’s AI-powered data protection project, exited to Google from its in-house incubator Area 120. The tool uses AI to check mobile apps for compliance with various privacy rules and regulations.\\n\\nApp Updates\\n\\nSocial\\n\\nSocial networking app IRL’s CEO Abraham Shafi stepped down following allegations he used bots to inflate the number of users IRL reported publicly and to its investors , The Information reported. A former employee had alleged he was fired after expressing concern over the use of bots. The SEC is now investigating if the company violated securities laws. IRL raised around $200 million from SoftBank Vision Fund, Founders Fund and others.\\n\\n, The Information reported. A former employee had alleged he was fired after expressing concern over the use of bots. The SEC is now investigating if the company violated securities laws. IRL raised around $200 million from SoftBank Vision Fund, Founders Fund and others. After laying off 50% of staff, declining audio social network Clubhouse says it’s building “Clubhouse 2.0,” but hasn’t shared exactly what that plan may involve. Last year, the company began shifting its focus away from public audio to private rooms but it’s not clear there’s much demand for audio social networking in the post-pandemic market.\\n\\nbut hasn’t shared exactly what that plan may involve. Last year, the company began shifting its focus away from public audio to private rooms but it’s not clear there’s much demand for audio social networking in the post-pandemic market. Once-hot viral app Poparazzi shuts down and returns remaining funds to investors. The app had let friends tag others to build out their social profiles of real moments, not polished images, but had been on the decline, with only a few thousand MAUs down from a height of 4 million MAUs previously.\\n\\nA Twitter bug saw users able to regain their blue Verification checks just by editing their bio. Shortly afterward, the Twitter desktop website began randomly logging out users. Later in the week, the mobile website was also down.\\n\\nShortly afterward, the Twitter desktop website began randomly logging out users. Later in the week, the mobile website was also down. As Bluesky gains attention, rival decentralized social platform Mastodon announced a new, simpler onboarding experience that provides new users with an account on mastodon.social by default , instead of requiring them to pick a server. This doesn’t eliminate server choice, it simply means that joining another server requires a few extra clicks.\\n\\n, instead of requiring them to pick a server. This doesn’t eliminate server choice, it simply means that joining another server requires a few extra clicks. Neighborhood social network Nextdoor added new features powered by generative AI, including an Assistant feature aimed at helping users write posts that are more likely to drive positive community engagement. The Assistant will offer writing suggestions that users can review and optionally adopt. The company says it will also use AI to better match content to users when providing recommendations.\\n\\nincluding an Assistant feature aimed at helping users write posts that are more likely to drive positive community engagement. The Assistant will offer writing suggestions that users can review and optionally adopt. The company says it will also use AI to better match content to users when providing recommendations. BeReal is testing another new feature in the U.K., “RealPeople,” that shows users a timeline of the “world’s most interesting people” — that is, athletes, artists, activists and other public figures. The company also recently began testing the option to post more often as usage has declined.\\n\\nand other public figures. The company also recently began testing the option to post more often as usage has declined. Meta introduced new discovery and personalization options for Facebook Reels. Users can now choose “Show More” or “Show Less” options to control what sort of Reels they want to see. Facebook will also explain why it’s showing you a Reel, like if a friend viewed it, and is adding Reels to the main navigation at the top of Facebook Watch.\\n\\nWordPress drops Twitter integration, says sharing to Instagram and Mastodon is coming instead. The Automattic-owned publishing platform said the Twitter connection on Jetpack and WordPress.com will cease to work, meaning users’ blog posts will no longer be auto-shared to Twitter as before. The company said Elon Musk’s decision to “dramatically change the terms and pricing” for Twitter’s API was to blame for this decision. The API now starts at $42,000/month for 50 million tweets. The move will likely hurt Twitter more than WordPress, as the latter powers over 40% of the global internet, including WordPress.com blogs.\\n\\nThe Automattic-owned publishing platform said the Twitter connection on Jetpack and WordPress.com will cease to work, meaning users’ blog posts will no longer be auto-shared to Twitter as before. The company said Elon Musk’s decision to “dramatically change the terms and pricing” for Twitter’s API was to blame for this decision. The API now starts at $42,000/month for 50 million tweets. The move will likely hurt Twitter more than WordPress, as the latter powers over 40% of the global internet, including WordPress.com blogs. Mozilla announced it’s opening up its own Mastodon server — or “instance,” in Mastodon lingo — into private beta testing. The company had said last year it planned to create and begin testing a publicly accessible instance at mozilla.social. It explains its approach to Mastodon will involve high levels of moderation.\\n\\nThe company had said last year it planned to create and begin testing a publicly accessible instance at mozilla.social. It explains its approach to Mastodon will involve high levels of moderation. Twitter announced it would make its API free for public service announcements after New York’s Metro Transit Service (MTS) abandoned the service and the National Weather Services (NWS) said it would no longer auto-post warnings.\\n\\nafter New York’s Metro Transit Service (MTS) abandoned the service and the National Weather Services (NWS) said it would no longer auto-post warnings. TikTok’s U.S. head of trust and safety Eric Han is leaving the company on May 12 as lawmakers weigh a TikTok ban. Han had played a key role in TikTok’s strategy to avoid a U.S. ban.\\n\\nas lawmakers weigh a TikTok ban. Han had played a key role in TikTok’s strategy to avoid a U.S. ban. Discord is making all users change their usernames, the company announced this week. Originally, Discord users had been identified by a name and random number separated by a hash sign, but now it wants to adopt a simpler format so people can more easily share their usernames with others. The new plan will include a unique alphanumeric username with the @ symbol in front of it, plus a freely assignable display name that can be changed at any time.\\n\\nAI\\n\\nSlack introduced SlackGPT, its own generative AI built on Slack’s platform which developers can use to create AI-driven experiences.\\n\\nwhich developers can use to create AI-driven experiences. Microsoft launched its Bing chatbot to all users globally, meaning there’s no more waitlist to get started. It’s also adding more image- and graphic-centric answers in Bing Chat, including by creating graphs and charts and generating images from text prompts. It will also allow users to export their Bing Chat histories. And it will embrace multimodality, meaning it can understand queries with images and text combined. Bing now sees more than 100 million daily active users and says visitors have engaged in over half a billion chats.\\n\\nIt’s also adding more image- and graphic-centric answers in Bing Chat, including by creating graphs and charts and generating images from text prompts. It will also allow users to export their Bing Chat histories. And it will embrace multimodality, meaning it can understand queries with images and text combined. Bing now sees more than 100 million daily active users and says visitors have engaged in over half a billion chats. Plexamp, the music player originally incubated by the Labs division of media company Plex, is tapping into ChatGPT with its latest update. The new feature called “Sonic Sage,” powered by OpenAI’s ChatGPT, will build unique music playlists by scanning users’ libraries and leveraging their TIDAL subscription.\\n\\nMedia & Entertainment\\n\\nFintech\\n\\nYC-backed Kenyan fintech Fingo launched its neobanking app, developed in collaboration with Pan-African financial institution Ecobank Kenya. The company raised $4 million in seed funding after its YC S21 participation. Fingo offers users a bank account, paired with free peer-to-peer transactions and access to savings, financial education and smart spending analytics.\\n\\nThe company raised $4 million in seed funding after its YC S21 participation. Fingo offers users a bank account, paired with free peer-to-peer transactions and access to savings, financial education and smart spending analytics. The FDIC is looking into Tellus, an Andreessen Horowitz-backed fintech company that claims it can offer people higher yields on their savings balances by using that money to fund certain U.S. single-family-home loans. U.S. Senator Sherrod Brown, chairman of the Senate Banking, Housing, and Urban Affairs Committee, wrote a letter to FDIC Chairman Martin Gruenberg expressing concerns about Tellus, and asking the FDIC to review Tellus’s business practices which may put customers at risk.\\n\\nMessaging\\n\\nWhatsApp now lets users create single-vote polls and forward media with captions, Meta announced this week. Single-vote polls let users run a poll where people are only allowed to vote once, including multiple choice, as has been the default.\\n\\nMeta announced this week. Single-vote polls let users run a poll where people are only allowed to vote once, including multiple choice, as has been the default. Reddit’s latest update provides link previews for messaging apps. Now, when you share a Reddit link via a messaging app, it will include a visual preview of the content, the subreddit name, the total upvotes tally and the number of comments. The update also includes the ability to share directly to IG Stories and other tools for publishers.\\n\\nTravel & Transportation\\n\\nFollowing its acquisition by Via, Citymapper said it’s lowering the paywall for its premium features while also introducing a new subscription plan ($1.49/mo) purely for removing ads.\\n\\nwhile also introducing a new subscription plan ($1.49/mo) purely for removing ads. Uber reported a Q1 earnings beat with its revenue up 29% YoY to $8.82 billion, gross bookings up 19% YoY to $31.4 billion and adjusted EBITDA up 353% YoY to $761 million. It also reported a $157 million net loss.\\n\\ngross bookings up 19% YoY to $31.4 billion and adjusted EBITDA up 353% YoY to $761 million. It also reported a $157 million net loss. Uber Eats is also planning to offer support for Live Activities and Dynamic Island on iPhone and integrated with Alexa for order updates.\\n\\nfor order updates. Lyft shared worrisome Q2 guidance sending its stock down after Q1 earnings where it had reported a 14% YoY increase in revenue to $1 billion and a net loss drop of 5% to $187.6 million. Ridership was up 9.8% YoY to 19.5 million.\\n\\nGaming\\n\\nSnowman, the mobile game studio behind Alto’s Adventure and Alto’s Odyssey, launched its newest title, Laya’s Horizon, exclusively with Netflix. The wingsuit game sees players mastering the art of flying, diving off mountains, weaving across forests and gliding over rivers to unlock new abilities as they explore a vast and peaceful world.\\n\\nCross-platform game engine Unity announced layoffs of 8% of its workforce, or around 600 jobs, after laying off 500+ in January and last June.\\n\\nafter laying off 500+ in January and last June. Amazon announced that customers in the United States, Canada, Germany and the United Kingdom can now play Fortnite on their Fire TVs via its Amazon Luna cloud gaming service.\\n\\nCommerce & Food Delivery\\n\\nAmazon Inspire, the e-commerce giant’s in-app TikTok-like shopping feed has rolled out to all customers in the United States. The company had been experimenting since last year with the new feed, which features content creators by influencers.\\n\\nThe company had been experimenting since last year with the new feed, which features content creators by influencers. DoorDash revenue was up 40% YoY in Q1, reaching $2.04 billion, beating estimates of $1.93 billion. Its net loss also declined 3% to $162 million and orders were up 27% to 512 million.\\n\\nEtc.\\n\\nAmazon rolled out a Matter update for Alexa that includes support for Thread, setup on iOS, and a new version of its Works with Alexa program.\\n\\nand a new version of its Works with Alexa program. Match Group posted a Q1 earnings miss with revenue down by 1% YoY to $787 million and paying users down 3% to 15.9 million. The company, however, said it’s “very possible” the recent Apple-Epic court decision could result in App Store fee relief.\\n\\nMedtech startup Healthy.io, which provides urine analysis through a mobile app, is laying off a third of its staff, or around 70 people. The company had just raised $50 million in Series D funding.\\n\\nThe company had just raised $50 million in Series D funding. Airbnb announced Rooms, a feature that focuses on the ability to book single rooms averaging $67 per night as users complain about excessive fees, onerous checkout procedures and rising Airbnb prices.\\n\\naveraging $67 per night as users complain about excessive fees, onerous checkout procedures and rising Airbnb prices. Google’s smart home app, Google Home, added support for smart garage door openers.\\n\\nSecurity\\n\\nGoogle announced that passkeys are now rolling out to Google Account users globally. Passkey let users sign in to websites and apps using the same biometrics or screen-lock PIN they use to unlock their devices.\\n\\nPasskey let users sign in to websites and apps using the same biometrics or screen-lock PIN they use to unlock their devices. Google announced that in 2022, it prevented 1.43 million policy-violating apps from being published on Google Play “in part due to new and improved security features and policy enhancements.”\\n\\nGovernment, Policy and Lawsuits\\n\\nThe EU’s Digital Markets Act (DMA) became applicable on May 2, but enforcement is not expected until spring 2024. The act focused on gatekeepers like Apple, Google, Meta and Microsoft. It limits how they can use third-party data, bans self-preferencing, introduces interoperability requirements, bans tracking users for targeted ads without consent and more. It also says app stores can’t require the use of their own payment services and permits app sideloading.\\n\\nBipartisan U.S. lawmakers reintroduced the Kids Online Safety Act with updates aimed at fixing earlier issues. The bill says platforms have to take reasonable steps to stop the spread of posts that promote eating disorders, suicide, substance abuse and more and undergo independent analysis about their safety for minors. It now also includes protections for support services, like the National Suicide Hotline, substance abuse groups and LGBTQ+ youth centers. However, critics, including the ACLU, say the changes are not enough and they remain opposed to the increased surveillance of kids this bill would require and other matters.\\n\\nThe bill says platforms have to take reasonable steps to stop the spread of posts that promote eating disorders, suicide, substance abuse and more and undergo independent analysis about their safety for minors. It now also includes protections for support services, like the National Suicide Hotline, substance abuse groups and LGBTQ+ youth centers. However, critics, including the ACLU, say the changes are not enough and they remain opposed to the increased surveillance of kids this bill would require and other matters. France’s competition watchdog announced interim measures against Meta, saying it suspects Meta of abusing its dominant position in the French market for ads on social media and across the broader (non-search-related) online ads market.\\n\\nsaying it suspects Meta of abusing its dominant position in the French market for ads on social media and across the broader (non-search-related) online ads market. The U.S. Federal Trade Commission (FTC) says Meta has “repeatedly violated” privacy rules and proposed to tighten its 2020 privacy order against the company, which would completely bar it from monetizing data from anyone under 18 in any way, among other new restrictions. The FTC also accused Meta of COPPA, a children’s privacy law, by misrepresenting its Messenger Kids parental controls, which allowed group chats and group calls with unapproved contacts.\\n\\nFunding and M&A\\n\\nAmazon acquired a small audio-focused artificial intelligence firm called Snackable.AI in 2022, The Post reported. Deal terms weren’t disclosed but Mari Joller, the founder and CEO of Snackable, is now the artificial intelligence and machine learning product leader at Amazon.\\n\\nDownloads\\n\\nRTRO\\n\\nNew social networking startup RTRO launched its app this week with the goal of connecting brands, creators and their fans and followers in a more positive environment focused on human connections and communities, not algorithm-driven content. To accomplish this, RTRO divides its social experience into two parts — on one side, you can keep up with friends or family in RTRO’s “circles.” On the other side, users can switch over to see content from creators and brands in their own space, dubbed RTRO TV.\\n\\nDistroKid\\n\\nMusic distribution service DistroKid this week launched its first mobile app, initially only for iPhone. The new app lets artists upload new releases, receive instant payment alerts, access stats from Apple and Spotify, edit metadata and more from their phones. The company said the mobile app had been the number one request from DistroKid members.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='OpenAI may be synonymous with machine learning now and Google is doing its best to pick itself up off the floor, but both may soon face a new threat: rapidly multiplying open source projects that push the state of the art and leave the deep-pocketed but unwieldy corporations in their dust. This Zerg-like threat may not be an existential one, but it will certainly keep the dominant players on the defensive.\\n\\nThe notion is not new by a long shot — in the fast-moving AI community, it’s expected to see this kind of disruption on a weekly basis — but the situation was put in perspective by a widely shared document purported to originate within Google. “We have no moat, and neither does OpenAI,” the memo reads.\\n\\nI won’t encumber the reader with a lengthy summary of this perfectly readable and interesting piece, but the gist is that while GPT-4 and other proprietary models have obtained the lion’s share of attention and indeed income, the head start they’ve gained with funding and infrastructure is looking slimmer by the day.\\n\\nWhile the pace of OpenAI’s releases may seem blistering by the standards of ordinary major software releases, GPT-3, ChatGPT and GPT-4 were certainly hot on each other’s heels if you compare them to versions of iOS or Photoshop. But they are still occurring on the scale of months and years.\\n\\nWhat the memo points out is that in March, a leaked foundation language model from Meta, called LLaMA, was leaked in fairly rough form. Within weeks, people tinkering around on laptops and penny-a-minute servers had added core features like instruction tuning, multiple modalities and reinforcement learning from human feedback. OpenAI and Google were probably poking around the code, too, but they didn’t — couldn’t — replicate the level of collaboration and experimentation occurring in subreddits and Discords.\\n\\nCould it really be that the titanic computation problem that seemed to pose an insurmountable obstacle — a moat — to challengers is already a relic of a different era of AI development?\\n\\nSam Altman already noted that we should expect diminishing returns when throwing parameters at the problem. Bigger isn’t always better, sure — but few would have guessed that smaller was instead.\\n\\nGPT-4 is a Walmart, and nobody actually likes Walmart\\n\\nThe business paradigm being pursued by OpenAI and others right now is a direct descendant of the SaaS model. You have some software or service of high value and you offer carefully gated access to it through an API or some such. It’s a straightforward and proven approach that makes perfect sense when you’ve invested hundreds of millions into developing a single monolithic yet versatile product like a large language model.\\n\\nIf GPT-4 generalizes well to answering questions about precedents in contract law, great — never mind that a huge number of its “intellect” is dedicated to being able to parrot the style of every author who ever published a work in the English language. GPT-4 is like a Walmart. No one actually wants to go there, so the company makes damn sure there’s no other option.\\n\\nBut customers are starting to wonder, why am I walking through 50 aisles of junk to buy a few apples? Why am I hiring the services of the largest and most general-purpose AI model ever created if all I want to do is exert some intelligence in matching the language of this contract against a couple hundred other ones? At the risk of torturing the metaphor (to say nothing of the reader), if GPT-4 is the Walmart you go to for apples, what happens when a fruit stand opens in the parking lot?\\n\\nIt didn’t take long in the AI world for a large language model to be run, in highly truncated form of course, on (fittingly) a Raspberry Pi. For a business like OpenAI, its jockey Microsoft, Google or anyone else in the AI-as-a-service world, it effectively beggars the entire premise of their business: that these systems are so hard to build and run that they have to do it for you. In fact it starts to look like these companies picked and engineered a version of AI that fit their existing business model, not vice versa!\\n\\nOnce upon a time you had to offload the computation involved in word processing to a mainframe — your terminal was just a display. Of course that was a different era, and we’ve long since been able to fit the whole application on a personal computer. That process has occurred many times since as our devices have repeatedly and exponentially increased their capacity for computation. These days when something has to be done on a supercomputer, everyone understands that it’s just a matter of time and optimization.\\n\\nFor Google and OpenAI, the time came a lot quicker than expected. And they weren’t the ones to do the optimizing — and may never be at this rate.\\n\\nNow, that doesn’t mean that they’re plain out of luck. Google didn’t get where it is by being the best — not for a long time, anyway. Being a Walmart has its benefits. Companies don’t want to have to find the bespoke solution that performs the task they want 30% faster if they can get a decent price from their existing vendor and not rock the boat too much. Never underestimate the value of inertia in business!\\n\\nSure, people are iterating on LLaMA so fast that they’re running out of camelids to name them after. Incidentally, I’d like to thank the developers for an excuse to just scroll through hundreds of pictures of cute, tawny vicuñas instead of working. But few enterprise IT departments are going to cobble together an implementation of Stability’s open source derivative-in-progress of a quasi-legal leaked Meta model over OpenAI’s simple, effective API. They’ve got a business to run!\\n\\nBut at the same time, I stopped using Photoshop years ago for image editing and creation because the open source options like Gimp and Paint.net have gotten so incredibly good. At this point, the argument goes the other direction. Pay how much for Photoshop? No way, we’ve got a business to run!\\n\\nWhat Google’s anonymous authors are clearly worried about is that the distance from the first situation to the second is going to be much shorter than anyone thought, and there doesn’t appear to be a damn thing anybody can do about it.\\n\\nExcept, the memo argues: embrace it. Open up, publish, collaborate, share, compromise. As they conclude:', metadata={'source': '/content/new_articles/05-05-google-and-openai-are-walmarts-besieged-by-fruit-stands.txt'}),\n",
       " Document(page_content='The best way to avoid a down round is to found an AI startup\\n\\nAs we see unicorns slash staff and the prevalence of down rounds spike, it may seem that the startup ecosystem is chock-full of bad news and little else. That’s not precisely the case.\\n\\nWhile AI, and in particular the generative AI subcategory, are as hot as the sun, not all venture attention is going to the handful of names that you already know. Sure, OpenAI is able to land nine and 10-figure rounds from a murderer’s row of tech investors and mega-cap corporations. And rising companies like Hugging Face and Anthropic cannot stay out of the news, proving that smaller AI-focused startups are doing more than well.\\n\\nIn fact, new data from Carta, which provides cap table management and other services, indicates that AI-focused startups are outperforming their larger peer group at both the seed and Series A stage.\\n\\nThe dataset, which notes that AI-centered startups are raising more and at higher valuations than other startups, indicates that perhaps the best way to avoid a down round today is to build in the artificial intelligence space.\\n\\nWhat the data says\\n\\nPer Carta data relating to the first quarter of the year, seed funding to non-AI startups in the U.S. market that use its services dipped from $1.64 billion to $1.08 billion, or a decline of around 34%. That result is directionally aligned with other data that we’ve seen regarding Q1 2023 venture capital totals; the data points down.', metadata={'source': '/content/new_articles/05-06-ai-startups-q1-investments.txt'}),\n",
       " Document(page_content='The legal spats between artists and the companies training AI on their artwork show no sign of abating.\\n\\nWithin the span of a few months, several lawsuits have emerged over generative AI tech from companies including OpenAI and Stability AI, brought by plaintiffs who allege that copyrighted data — mostly art — was used without their permission to train the generative models. Generative AI models “learn” to create art, code and more by “training” on sample images and text, usually scraped indiscriminately from the web.\\n\\nIn an effort to grant artists more control over how — and where — their art’s used, Jordan Meyer and Mathew Dryhurst co-founded the startup Spawning AI. Spawning created HaveIBeenTrained, a website that allows creators to opt out of the training dataset for one art-generating AI model, Stable Diffusion v3, due to be released in the coming months.\\n\\nAs of March, artists had used HaveIBeenTrained to remove 80 million pieces of artwork from the Stable Diffusion training set. By late April, that figure had eclipsed 1 billion.\\n\\nAs the demand for Spawning’s service grew, the company — which was entirely bootstrapped up until that point — sought an outside investment. And it got it. Spawning today announced that it raised $3 million in a seed round led by True Ventures with participation from the Seed Club Ventures, Abhay Parasnis, Charles Songhurst, Balaji Srinivisan, Jacob.eth and Noise DAO.\\n\\nSpeaking to TechCrunch via email, Meyer said that the funding will allow Spawning to continue developing “IP standards for the AI era” and establish more robust opt-out and opt-in standards.\\n\\n“We are enthusiastic about the potential of AI tooling. We developed domain expertise in the field from being passionate about new opportunities AI provides to creators, but feel that consent is a fundamental layer to make these developments something everyone can feel good about,” Meyer said.\\n\\nSpawning’s metrics speak for themselves. Clearly, there’s a demand from artists for more say in how their art’s used (or scraped, as the case may be). But beyond partnerships with art platforms like Shutterstock and ArtStation, Spawning hasn’t managed to rally the industry around a common opt-out or provenance standard.\\n\\nAdobe, which recently announced generative AI tools, is pursuing its own opt-out mechanisms and tooling. So is DeviantArt, which in November launched a protection that relies on HTML tags to prohibit the software robots that crawl pages for images from downloading those images for training sets. OpenAI, the generative AI giant in the room, still doesn’t offer an opt-out tool — nor has it announced plans to anytime soon.\\n\\nSpawning has also come under criticism for the opaqueness — and vagueness — of its opt-out process. As Ars Technica noted in a recent piece, the opt-out process doesn’t appear to fit the definition of consent for personal data use in Europe’s General Data Protection Regulation, which states that consent must be actively given, not assumed by default. Also unclear is how Spawning intends to legally verify the identities of artists who make opt-out requests — or indeed, if it intends to attempt this at all.\\n\\nSpawning’s solution is multipronged. First, it plans to make it easier for AI model trainers to honor opt-out requests and streamline the process for creators. Then, Spawning will offer more services to organizations seeking to protect the work of their artists, Meyer says.\\n\\n“We want to build the consent layer for AI, which we feel will be a fundamentally helpful piece of infrastructure moving forward,” he added. “We plan to grow Spawning to address the many different domains touched by the AI economy, as each domain has their own particular needs.”\\n\\nIn a first step toward this ambitious vision, Spawning in March enabled “domain opt-outs,” allowing creators and content partners to quickly opt-out content from whole websites. Spawning says that 30,000 domains to date have been registered in the system.\\n\\nApril will mark the release of an API and open source Python package that’ll greatly expand the breadth of content that Spawning touches. Previously, opt-out requests through Spawning only applied to the LAION-5B dataset — the dataset used to train Stable Diffusion. As of April, any website, app or service will be able to use Spawning’s API to automatically comply with opt-outs not just for image data, but for text, audio, videos and more.\\n\\nMeyer says that Spawning will aggregate every new opt-out method (e.g. Adobe’s and DeviantArt’s) into its Python package for model trainers, with the goal of cutting down on the number of accounts model creators have to manage to comply with opt-out requests.\\n\\nTo boost visibility, Spawning is partnering with Hugging Face, one of the larger platforms for hosting and running AI models, to add a new info box on Hugging Face that’ll alert users to the proportion of “opted-out” data within text-to-image datasets. The box will also link to a Spawning API sign-up page so that model trainers can remove opted-out images at training time.\\n\\n“We feel that once companies and developers know that the option to honor creator wishes is available, there is little reason not to honor them,” Meyer said. “We are excited about the future of generative AI, but creators and organizations alike need standards in place to have their data work in their favor.”\\n\\nLooking ahead, Spawning intends to release an “exact-duplicate” detection feature to match opted-out images with copies that the platform finds across the web, followed by a “near-duplicate” detection feature to notify artists when Spawning finds likely copies of their work that’ve been cropped, compressed or otherwise slightly modified.\\n\\nBeyond that, there’s plans for a Chrome extension to let creators pre-emptively opt out of their work posted anywhere on the web and a caption search on the HaveIBeenTrained website to directly search image descriptions. The site’s current search tool uses only approximate matches between text and images as well as URL searches to find content hosted on specific websites.\\n\\nSpawning — now beholden to investors — plans to make money by building services on top of its content infrastructure, although Meyer wouldn’t divulge much. How that’ll sit with content creators remains to be seen.\\n\\n“We’ve spoken to quite a few organizations, with many conversations being too premature to announce, and think that our funding announcement and increased visibility will go some way to offer assurances that what we are building is a robust and dependable standard to work with,” Meyer said. “After we complete these features, we’ll begin building infrastructure to support more datasets — including music, video and text.”', metadata={'source': '/content/new_articles/05-03-spawning-lays-out-its-plans-for-letting-creators-opt-out-of-generative-ai-training.txt'}),\n",
       " Document(page_content='Well that was fast. The U.K.’s competition watchdog has announced an initial review of “AI foundational models”, such as the large language models (LLMs) which underpin OpenAI’s ChatGPT and Microsoft’s New Bing. Generative AI models which power AI art platforms such as OpenAI’s DALL-E or Midjourney will also likely fall in scope.\\n\\nThe Competition and Markets Authority (CMA) said its review will look at competition and consumer protection considerations in the development and use of AI foundational models — with the aim of understanding “how foundation models are developing and producing an assessment of the conditions and principles that will best guide the development of foundation models and their use in the future”.\\n\\nIt’s proposing to publish the review in “early September”, with a deadline of June 2 for interested stakeholders to submit responses to inform its work.\\n\\n“Foundation models, which include large language models and generative artificial intelligence (AI), that have emerged over the past five years, have the potential to transform much of what people and businesses do. To ensure that innovation in AI continues in a way that benefits consumers, businesses and the UK economy, the government has asked regulators, including the [CMA], to think about how the innovative development and deployment of AI can be supported against five overarching principles: safety, security and robustness; appropriate transparency and explainability; fairness; accountability and governance; and contestability and redress,” the CMA wrote in a press release.”\\n\\nStanford University’s Human-Centered Artificial Intelligence Center’s Center for Research on Foundation Models is credited with coining the term “foundational models”, back in 2021, to refer to AI systems that focus on training one model on a huge amount of data and adapting it to many applications.\\n\\n“The development of AI touches upon a number of important issues, including safety, security, copyright, privacy, and human rights, as well as the ways markets work. Many of these issues are being considered by government or other regulators, so this initial review will focus on the questions the CMA is best placed to address — what are the likely implications of the development of AI foundation models for competition and consumer protection?” the CMA added.\\n\\nIn a statement, its CEO, Sarah Cardell, also said:\\n\\nAI has burst into the public consciousness over the past few months but has been on our radar for some time. It’s a technology developing at speed and has the potential to transform the way businesses compete as well as drive substantial economic growth. It’s crucial that the potential benefits of this transformative technology are readily accessible to UK businesses and consumers while people remain protected from issues like false or misleading information. Our goal is to help this new, rapidly scaling technology develop in ways that ensure open, competitive markets and effective consumer protection.\\n\\nSpecifically, the U.K. competition regulator said its initial review of AI foundational models will:\\n\\nexamine how the competitive markets for foundation models and their use could evolve\\n\\nexplore what opportunities and risks these scenarios could bring for competition and consumer protection\\n\\nproduce guiding principles to support competition and protect consumers as AI foundation models develop\\n\\nWhile it may seen early for the antitrust regulator to conduct a review of such a fast-moving emerging technology the CMA is acting on government instruction.\\n\\nAn AI white paper published in March signalled ministers’ preference to avoid setting any bespoke rules (or oversight bodies) to govern uses of artificial intelligence at this stage. However ministers said existing U.K. regulators — including the CMA, which was directly name-checked — would be expected to issue guidance to encourage safe, fair and accountable uses of AI.\\n\\nThe CMA says its initial review of foundational AI models is in line with instructions in the white paper, where the government talked about existing regulators conducting “detailed risk analysis” in order to be in a position to carry out potential enforcements, i.e. on dangerous, unfair and unaccountable applications of AI, using their existing powers.\\n\\nThe regulator also points to its core mission — to support open, competitive markets — as another reason for taking a look at generative AI now.\\n\\nNotably, the competition watchdog is set to get additional powers to regulate Big Tech in the coming years, under plans taken off the back-burner by prime minister Rishi Sunak’s government last month, when ministers said it would move forward with a long-trailed (but much delayed) ex ante reform aimed at digital giants’ market power.\\n\\nThe expectation is that the CMA’s Digital Markets Unit, up and running since 2021 in shadow form, will (finally) gain legislative powers in the coming years to apply pro-active “pro-competition” rules which are tailored to platforms that are deemed to have “strategic market status” (SMS). So we can speculate that providers of powerful foundational AI models may, down the line, be judged to have SMS — meaning they could expect to face bespoke rules on how they must operate vis-a-vis rivals and consumers in the U.K. market.\\n\\nThe U.K.’s data protection watchdog, the ICO, also has its eye on generative AI. It’s another existing oversight body which the government has tasked with paying special mind to AI under its plan for context-specific guidance to steer development of the tech through the application of existing laws.\\n\\nIn a blog post last month, Stephen Almond, the ICO’s executive director of regulatory risk, offered some tips and a little warning for developers of generative AI when it comes to compliance with U.K. data protection rules. “Organisations developing or using generative AI should be considering their data protection obligations from the outset, taking a data protection by design and by default approach,” he suggested. “This isn’t optional — if you’re processing personal data, it’s the law.”\\n\\nOver the English Channel in the European Union, meanwhile, lawmakers are in the process of deciding a fixed set of rules that are likely to apply to generative AI.\\n\\nNegotiations toward a final text for the EU’s incoming AI rulebook are ongoing — but currently there’s a focus on how to regulate foundational models via amendments to the risk-based framework for regulating uses of AI the bloc published in draft over two years ago.\\n\\nIt remains to be seen where the EU’s co-legislators will end up on what’s sometimes also referred to as general purpose AI. But, as we reported recently, parliamentarians are pushing for a layered approach to tackle safety issues with foundational models; the complexity of responsibilities across AI supply chains; and to address specific content concerns (like copyright) which are associated with generative AI.\\n\\nAdd to that, EU data protection law already applies to AI, of course. And privacy-focused investigations of models like ChatGPT are underway in the bloc — including in Italy where an intervention by the local watchdog led to OpenAI rushing out a series of privacy disclosures and controls last month.\\n\\nThe European Data Protection Board also recently set up a task force to support coordination between different data protection authorities on investigations of the AI chatbot. Others investigating ChatGPT include Spain’s privacy watchdog.', metadata={'source': '/content/new_articles/05-04-cma-generative-ai-review.txt'}),\n",
       " Document(page_content='It’s that time of week again, folks — Week in Review (WiR) time. For those new to the scene, WiR is TechCrunch’s regular newsletter that recaps the biggest tech stories over the past few days. There’s no better digest for the person on the go, we’d argue — but of course, we’re a little biased.\\n\\nBefore we get into the meat of the thing, a quick reminder that TC City Spotlight: Atlanta is fast approaching. On June 7, TechCrunch is headed to Atlanta, where we’ll host a pitch competition, a talk on the economics of equality, a panel discussion on investing in the Atlanta ecosystem and more.\\n\\nElsewhere, there’s a TechCrunch Live event with Persona and Index Ventures on May 10, which will touch on how Persona keeps pace with new threats and how Index made a prescient move to spot and back Persona early on. And we have Disrupt in San Francisco from September 19–21 — our annual conference is jam-packed with expert-led sessions and interviews with movers and shakers in the tech space.\\n\\nNow, with that out of the way, here’s the top headlines.\\n\\nmost read\\n\\nAmazon debuts free channels: Amazon is doubling down on free, ad-supported content with this week’s introduction of Fire TV Channels. The new, free and ad-supported video experience, which came to Fire TV devices this week, will be continuously updated throughout the day and integrated into several areas across the Fire TV interface, Sarah reports.\\n\\nBio update for a check: Briefly, a bug on Twitter let legacy blue check holders get their badge back by updating their bio. Readers will recall that blue checks on Twitter once signified that a user was “verified,” but now serve as an indication that they’re paying for Twitter’s premium subscription service, Twitter Blue. Verified users who chose not to pay recently faced the prospect of blue check removal — but not necessarily permanently, judging by the bug.\\n\\nGoogle ditches passwords for passkeys: This week Google rolled out passkeys to Google Account users globally, roughly a year after the company — alongside Apple, Microsoft and the FIDO Alliance — announced a partnership to broadly advance passkey adoption. With passkeys, users’ authentication synchronizes across devices through the cloud using cryptographic key pairs, allowing them to sign in to websites and apps using the same biometrics or screen-lock PIN they use to unlock their devices.\\n\\nMicrosoft debuts Pegasus: Microsoft this week announced that it’ll extend the Startup Founders Hub, its self-service platform that provides founders with free resources, including Azure credits, with a new incubator program called the Pegasus Program. Pegasus will select startups with products that “fill a market need” and give them up to $350,000 in Azure, GitHub and LinkedIn credits plus backing from advisors, as well as “access to the best Microsoft tech,” Microsoft says.\\n\\nBlue check marks come to Gmail: Google is going to start displaying a blue check mark next to select senders’ names on Gmail to verify their identity, the company said on Wednesday. The check marks will automatically appear next to companies that have adopted Gmail’s existing brand indicators for message identification feature, reports Aisha.\\n\\nOpenAI rakes in the dough: OpenAI, the startup behind the widely used conversational AI model ChatGPT, has picked up new backers. In an exclusive report, Jagmeet and Ingrid reveal that VC firms, including Sequoia Capital, Andreessen Horowitz, Thrive, K2 Global and Founders Fund, have put just over $300 million into OpenAI, valuing the company at between $27 billion and $29 billion.\\n\\nApple releases security fix: On Monday, Apple released its first batch of publicly available “rapid security” patches, aimed at quickly fixing security vulnerabilities that are under active exploitation or pose significant risks to its customers. Apple says that these patches, which are enabled by default, were intended to let customers update their devices faster than a typical software upgrade.\\n\\nMusk settles for less: A defamation case brought against Tesla chief executive Elon Musk by critic Randeep Hothi is coming to a close, reportedly costing the billionaire ten big ones. Lawyers representing Hothi — a vocal member of the TSLAQ short-seller community on Twitter who rose to prominence as a skeptic of Tesla’s gigafactory plans and “full self-driving” tech — said in a statement that Musk asked to settle the nearly three-year-old case back in March.\\n\\nA new LLM for Alexa: Amazon is building a more “generalized and capable” large language model to power Alexa, said Amazon CEO Andy Jassy during the company’s first-quarter earnings call this week. He added that although Amazon has had an LLM powering Alexa, Amazon is working on one that’s more capable than the current one.\\n\\naudio\\n\\nTechCrunch’s stable of podcasts grows by the day — and it’s all quality stuff. This week, the Equity folks covered First Republic Bank, Poparazzi’s shutdown, Databricks’ acquisition, who’s going head-to-head with Stripe, the rise of down rounds and why Bluesky had them feeling less gray. Meanwhile, Found spoke with Stefan Bauer about how his company, Marker Learning, is cutting the cost of learning disability assessments by conducting them remotely. Chain Reaction interviewed Jake Chervinsky, the chief policy officer at Blockchain Association, a nonprofit organization focused on promoting “pro-innovation” policy for the digital asset world. On The TechCrunch Podcast — which, like WiR, covers the week in tech news — Devin talked about whether Meta’s cavalier approach to compliance might finally be coming to a close. And last but not least, TechCrunch Live profiled Sam Chaudhary, the founder of ClassDojo, and Chris Farmer, the founder and CEO of SignalFire, about playing the long game in edtech, investing in companies that aren’t rushing to monetize and the “outsider advantage.”\\n\\nTechCrunch+\\n\\nTC+ subscribers get access to in-depth commentary, analysis and surveys — which you know if you’re already a subscriber. If you’re not, consider signing up. Here are a few highlights from this week:\\n\\nA cloudy future: Lyft’s equity is selling off in the wake of the U.S. ride-hailing giant’s first-quarter results and its comments regarding the current quarter, and how its new strategic posture will affect its growth and economics in the coming quarters. But there’s not necessarily cause for panic. Alex and Anna write about Lyft’s new tack and the potential upsides, of which there are several.\\n\\nDown but not out: For the past year, everyone’s been predicting that the muted exit environment and bone-dry funding market would bring a reckoning for many late-stage companies. Down rounds carry a negative connotation and are often interpreted as the fault of the company or founder. But in a market where everything seems to be heading downward, they shouldn’t imply a company or its founders made a mistake — you often simply can’t help it, Rebecca writes.\\n\\nChatGPT, meet edtech: Shares of edtech company Chegg fell off a cliff this week even after the company reported Q1 results that bested analyst expectations. In its earnings call, the company’s executives noted that ChatGPT was slowing its ability to add new subscribers, not only potentially slowing growth but also throwing uncertainty into its ability to predict its future financial results. Alex and Natasha M dig deeper.\\n\\nGet your TechCrunch fix IRL. Join us at Disrupt 2023 in San Francisco this September to immerse yourself in all things startup. From headline interviews to intimate roundtables to a jam-packed startup expo floor, there’s something for everyone at Disrupt. Save up to $800 when you buy your pass now through May 15, and save 15% on top of that with promo code WIR. Learn more.', metadata={'source': '/content/new_articles/05-06-amazon-launches-free-channels-check-marks-come-to-gmail-and-openai-raises-more-moolah.txt'}),\n",
       " Document(page_content='ChatGPT: Everything you need to know about the AI-powered chatbot\\n\\nChatGPT, OpenAI’s text-generating AI chatbot, has taken the world by storm. It’s able to write essays, code and more given short text prompts, hyper-charging productivity. But it also has a more…nefarious side.\\n\\nIn any case, AI tools are not going away — and indeed has expanded dramatically since its launch just a few months ago. Major brands are experimenting with it, using the AI to generate ad and marketing copy, for example.\\n\\nAnd OpenAI is heavily investing in it. ChatGPT was recently super-charged by GPT-4, the latest language-writing model from OpenAI’s labs. Paying ChatGPT users have access to GPT-4, which can write more naturally and fluently than the model that previously powered ChatGPT. In addition to GPT-4, OpenAI recently connected ChatGPT to the internet with plugins available in alpha to users and developers on the waitlist.\\n\\nHere’s a timeline of ChatGPT product updates and releases, starting with the latest, to be updated regularly. We also answer the most common FAQs (see below).\\n\\nTimeline of the most recent ChatGPT updates\\n\\nMay 3, 2023\\n\\nMeta said in a report on May 3 that malware posing as ChatGPT was on the rise across its platforms.The company said that since March 2023, its security teams have uncovered 10 malware families using ChatGPT (and similar themes) to deliver malicious software to users’ devices.\\n\\n“In one case, we’ve seen threat actors create malicious browser extensions available in official web stores that claim to offer ChatGPT-based tools,” said Meta security engineers Duc H. Nguyen and Ryan Victory in a blog post. “They would then promote these malicious extensions on social media and through sponsored search results to trick people into downloading malware.”\\n\\nApril 28, 2023\\n\\nVC firms including Sequoia Capital, Andreessen Horowitz, Thrive and K2 Global are picking up new shares, according to documents seen by TechCrunch. A source tells us Founders Fund is also investing. Altogether the VCs have put in just over $300 million at a valuation of $27 billion to $29 billion. This is separate to a big investment from Microsoft announced earlier this year, a person familiar with the development told TechCrunch, which closed in January. The size of Microsoft’s investment is believed to be around $10 billion, a figure we confirmed with our source.\\n\\nApril 25, 2023\\n\\nCalled ChatGPT Business, OpenAI describes the forthcoming offering as “for professionals who need more control over their data as well as enterprises seeking to manage their end users.”\\n\\n“ChatGPT Business will follow our API’s data usage policies, which means that end users’ data won’t be used to train our models by default,” OpenAI wrote in a blog post. “We plan to make ChatGPT Business available in the coming months.”\\n\\nApril 24, 2023\\n\\nOpenAI applied for a trademark for “GPT,” which stands for “Generative Pre-trained Transformer,” last December. Last month, the company petitioned the USPTO to speed up the process, citing the “myriad infringements and counterfeit apps” beginning to spring into existence.\\n\\nUnfortunately for OpenAI, its petition was dismissed last week. According to the agency, OpenAI’s attorneys neglected to pay an associated fee as well as provide “appropriate documentary evidence supporting the justification of special action.”\\n\\nThat means a decision could take up to five more months.\\n\\nApril 22, 2023\\n\\nAuto-GPT is an open source app created by game developer Toran Bruce Richards that uses OpenAI’s latest text-generating models, GPT-3.5 and GPT-4, to interact with software and services online, allowing it to “autonomously” perform tasks.\\n\\nDepending on what objective the tool’s provided, Auto-GPT can behave in very… unexpected ways. One Reddit user claims that, given a budget of $100 to spend within a server instance, Auto-GPT made a wiki page on cats, exploited a flaw in the instance to gain admin-level access and took over the Python environment in which it was running — and then “killed” itself.\\n\\nApril 18, 2023\\n\\nFTC chair Lina Khan and fellow commissioners warned House representatives of the potential for modern AI technologies, like ChatGPT, to be used to “turbocharge” fraud in a congressional hearing.\\n\\n“AI presents a whole set of opportunities, but also presents a whole set of risks,” Khan told the House representatives. “And I think we’ve already seen ways in which it could be used to turbocharge fraud and scams. We’ve been putting market participants on notice that instances in which AI tools are effectively being designed to deceive people can place them on the hook for FTC action,” she stated.\\n\\nApril 17, 2023\\n\\nThe company behind the popular iPhone customization app Brass, sticker maker StickerHub and others is out today with a new AI chat app called SuperChat, which allows iOS users to chat with virtual characters powered by OpenAI’s ChatGPT. However, what makes the app different from the default ChatGPT experience or the dozens of generic AI chat apps now available are the characters offered which you can use to engage with SuperChat’s AI features.\\n\\nApril 12, 2023\\n\\nItaly’s data protection watchdog has laid out what OpenAI needs to do for it to lift an order against ChatGPT issued at the end of last month — when it said it suspected the AI chatbot service was in breach of the EU’s GSPR and ordered the U.S.-based company to stop processing locals’ data.\\n\\nThe DPA has given OpenAI a deadline — of April 30 — to get the regulator’s compliance demands done. (The local radio, TV and internet awareness campaign has a slightly more generous timeline of May 15 to be actioned.)\\n\\nApril 12, 2023\\n\\nA study co-authored by scientists at the Allen Institute for AI shows that assigning ChatGPT a “persona” — for example, “a bad person,” “a horrible person” or “a nasty person” — through the ChatGPT API increases its toxicity sixfold. Even more concerning, the co-authors found having ChatGPT pose as certain historical figures, gendered people and members of political parties also increased its toxicity — with journalists, men and Republicans in particular causing the machine learning model to say more offensive things than it normally would.\\n\\nThe research was conducted using the latest version of ChatGPT, but not the model currently in preview based on OpenAI’s GPT-4.\\n\\nApril 4, 2023\\n\\nYC Demo Day’s Winter 2023 batch features no fewer than four startups that claim to be building “ChatGPT for X.” They’re all chasing after a customer service software market that’ll be worth $58.1 billion by 2023, assuming the rather optimistic prediction from Acumen Research comes true.\\n\\nHere are the YC-backed startups that caught our eye:\\n\\nYuma, whose customer demographic is primarily Shopify merchants, provides ChatGPT-like AI systems that integrate with help desk software, suggesting drafts of replies to customer tickets.\\n\\nBaselit, which uses one of OpenAI’s text-understanding models to allow businesses to embed chatbot-style analytics for their customers.\\n\\nLasso customers send descriptions or videos of the processes they’d like to automate and the company combines ChatGPT-like interface with robotic process automation (RPA) and a Chrome extension to build out those automations.\\n\\nBerriAI, whose platform is designed to help developers spin up ChatGPT apps for their organization data through various data connectors.\\n\\nApril 1, 2023\\n\\nOpenAI has started geoblocking access to its generative AI chatbot, ChatGPT, in Italy.\\n\\nItaly’s data protection authority has just put out a timely reminder that some countries do have laws that already apply to cutting edge AI: it has ordered OpenAI to stop processing people’s data locally with immediate effect. The Italian DPA said it’s concerned that the ChatGPT maker is breaching the European Union’s General Data Protection Regulation (GDPR), and is opening an investigation.\\n\\nMarch 29, 2023\\n\\nThe letter’s signatories include Elon Musk, Steve Wozniak and Tristan Harris of the Center for Humane Technology, among others. The letter calls on “all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4.”\\n\\nThe letter reads:\\n\\nContemporary AI systems are now becoming human-competitive at general tasks,[3] and we must ask ourselves: Should we let machines flood our information channels with propaganda and untruth? Should we automate away all the jobs, including the fulfilling ones? Should we develop nonhuman minds that might eventually outnumber, outsmart, obsolete and replace us? Should we risk loss of control of our civilization? Such decisions must not be delegated to unelected tech leaders. Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable.\\n\\nMarch 23, 2023\\n\\nOpenAI launched plugins for ChatGPT, extending the bots functionality by granting it access to third-party knowledge sources and databases, including the web. Available in alpha to ChatGPT users and developers on the waitlist, OpenAI says that it’ll initially prioritize a small number of developers and subscribers to its premium ChatGPT Plus plan before rolling out larger-scale and API access.\\n\\nMarch 14, 2023\\n\\nGPT-4 is a powerful image- and text-understanding AI model from OpenAI. Released March 14, GPT-4 is available for paying ChatGPT Plus users and through a public API. Developers can sign up on a waitlist to access the API.\\n\\nMarch 9, 2023\\n\\nChatGPT is generally available through the Azure OpenAI Service, Microsoft’s fully managed, corporate-focused offering. Customers, who must already be “Microsoft managed customers and partners,” can apply here for special access.\\n\\nMarch 1, 2023\\n\\nOpenAI makes another move toward monetization by launching a paid API for ChatGPT. Instacart, Snap (Snapchat’s parent company) and Quizlet are among its initial customers.\\n\\nFebruary 7, 2023\\n\\nAt a press event in Redmond, Washington, Microsoft announced its long-rumored integration of OpenAI’s GPT-4 model into Bing, providing a ChatGPT-like experience within the search engine. The announcement spurred a 10x increase in new downloads for Bing globally, indicating a sizable consumer demand for new AI experiences.\\n\\nOther companies beyond Microsoft joined in on the AI craze by implementing ChatGPT, including OkCupid, Kaito, Snapchat and Discord — putting the pressure on Big Tech’s AI initiatives, like Google.\\n\\nFebruary 1, 2023\\n\\nAfter ChatGPT took the internet by storm, OpenAI launched a new pilot subscription plan for ChatGPT called ChatGPT Plus, aiming to monetize the technology starting at $20 per month.\\n\\nDecember 8, 2022\\n\\nA week after ChatGPT was released into the wild, two developers — Steven Tey and Dom Eccleston — made a Chrome extension called ShareGPT to make it easier to capture and share the AI’s answers with the world.\\n\\nNovember 30, 2022\\n\\nGPT-3.5 broke cover with ChatGPT, a fine-tuned version of GPT-3.5 that’s essentially a general-purpose chatbot. ChatGPT can engage with a range of topics, including programming, TV scripts and scientific concepts.\\n\\nWriters everywhere rolled their eyes at the new technology, much like artists did with OpenAI’s DALL-E model, but the latest chat-style iteration seemingly broadened its appeal and audience.\\n\\nFAQs:\\n\\nWhat is ChatGPT? How does it work?\\n\\nChatGPT is a general-purpose chatbot that uses artificial intelligence to generate text after a user enters a prompt, developed by tech startup OpenAI. The chatbot uses GPT-4, a large language model that uses deep learning to produce human-like text.\\n\\nWhen did ChatGPT get released?\\n\\nNovember 30, 2022 is when ChatGPT was released for public use.\\n\\nWhat is the latest version of ChatGPT?\\n\\nBoth the free version of ChatGPT and the paid ChatGPT Plus are regularly updated with new GPT models. The most recent model is GPT-4.\\n\\nIs ChatGPT free?\\n\\nThere is a free version of ChatGPT that only requires a sign-in in addition to the paid version, ChatGPT Plus.\\n\\nWho uses ChatGPT?\\n\\nAnyone can use ChatGPT! More and more tech companies and search engines are utilizing the chatbot to automate text or quickly answer user questions/concerns.\\n\\nWhat is the difference between ChatGPT and a chatbot?\\n\\nA chatbot can be any software/system that holds dialogue with you/a person but doesn’t necessarily have to be AI-powered. For example, there are chatbots that are rules-based in the sense that they’ll give canned responses to questions.\\n\\nChatGPT is AI-powered and utilizes LLM technology to generate text after a prompt.\\n\\nCan ChatGPT write essays?\\n\\nYes.\\n\\nCan ChatGPT commit libel?\\n\\nDue to the nature of how these models work, they don’t know or care whether something is true, only that it looks true. That’s a problem when you’re using it to do your homework, sure, but when it accuses you of a crime you didn’t commit, that may well at this point be libel.\\n\\nWe will see how handling troubling statements produced by ChatGPT will play out over the next few months as tech and legal experts attempt to tackle the fastest moving target in the industry.\\n\\nDoes ChatGPT have an app?\\n\\nThere is not an app available for iPhone or Android, but users have options to enable the chatbot on their mobile devices via their browser or a third-party app that uses ChatGPT’s public API.\\n\\nWhat is the ChatGPT character limit?\\n\\nIt’s not documented anywhere that ChatGPT has a character limit. However, users have noted that there are some character limitations after around 500 words.\\n\\nDoes ChatGPT have an API?\\n\\nYes, it was released March 1, 2023.\\n\\nWhat are some sample everyday uses for ChatGPT?\\n\\nEveryday examples include programing, scripts, email replies, listicles, blog ideas, summarization, etc.\\n\\nWhat are some advanced uses for ChatGPT?\\n\\nAdvanced use examples include debugging code, programming languages, scientific concepts, complex problem solving, etc.\\n\\nHow good is ChatGPT at writing code?\\n\\nIt depends on the nature of the program. While ChatGPT can write workable Python code, it can’t necessarily program an entire app’s worth of code. That’s because ChatGPT lacks context awareness — in other words, the generated code isn’t always appropriate for the specific context in which it’s being used.\\n\\nCan you save a ChatGPT chat?\\n\\nYes. OpenAI allows users to save chats in the ChatGPT interface, stored in the sidebar of the screen. There are no built-in sharing features yet.\\n\\nAre there alternatives to ChatGPT?\\n\\nYes. There are multiple AI-powered chatbot competitors such as Together, Google’s Bard and Anthropic’s Claude, and developers are creating open source alternatives. But the latter are harder — if not impossible — to run today.\\n\\nHow does ChatGPT handle data privacy?\\n\\nOpenAI has said that individuals in “certain jurisdictions” (such as the EU) can object to the processing of their personal information by its AI models by filling out this form. This includes the ability to make requests for deletion of AI-generated references about you. Although OpenAI notes it may not grant every request since it must balance privacy requests against freedom of expression “in accordance with applicable laws”.\\n\\nThe web form for making a deletion of data about you request is entitled “OpenAI Personal Data Removal Request”.\\n\\nIn its privacy policy, the ChatGPT maker makes a passing acknowledgement of the objection requirements attached to relying on “legitimate interest” (LI), pointing users towards more information about requesting an opt out — when it writes: “See here for instructions on how you can opt out of our use of your information to train our models.”\\n\\nWhat controversies have surrounded ChatGPT?\\n\\nRecently, Discord announced that it had integrated OpenAI’s technology into its bot named Clyde where two users tricked Clyde into providing them with instructions for making the illegal drug methamphetamine (meth) and the incendiary mixture napalm.\\n\\nAn Australian mayor has publicly announced he may sue OpenAI for defamation due to ChatGPT’s false claims that he had served time in prison for bribery. This would be the first defamation lawsuit against the text-generating service.\\n\\nCNET found itself in the midst of controversy after Futurism reported the publication was publishing articles under a mysterious byline completely generated by AI. The private equity company that owns CNET, Red Ventures, was accused of using ChatGPT for SEO farming, even if the information was incorrect.\\n\\nSeveral major school systems and colleges, including New York City Public Schools, have banned ChatGPT from their networks and devices. They claim that the AI impedes the learning process by promoting plagiarism and misinformation, a claim that not every educator agrees with.\\n\\nThere have also been cases of ChatGPT accusing individuals of false crimes.\\n\\nWhere can I find examples of ChatGPT prompts?\\n\\nSeveral marketplaces host and provide ChatGPT prompts, either for free or for a nominal fee. One is PromptBase. Another is ChatX. More launch every day.\\n\\nCan ChatGPT be detected?\\n\\nPoorly. Several tools claim to detect ChatGPT-generated text, but in our tests, they’re inconsistent at best.\\n\\nAre ChatGPT chats public?\\n\\nNo. But OpenAI recently disclosed a bug, since fixed, that exposed the titles of some users’ conversations to other people on the service.\\n\\nWho owns the copyright on ChatGPT-created content or media?\\n\\nThe user who requested the input from ChatGPT is the copyright owner.\\n\\nWhat lawsuits are there surrounding ChatGPT?\\n\\nNone specifically targeting ChatGPT. But OpenAI is involved in at least one lawsuit that has implications for AI systems trained on publicly available data, which would touch on ChatGPT.\\n\\nAre there issues regarding plagiarism with ChatGPT?\\n\\nYes. Text-generating AI models like ChatGPT have a tendency to regurgitate content from their training data.', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content=\"Vint Cerf on the ‘exhilarating mix’ of thrill and hazard at the frontiers of tech 'That's always an exciting place to be — a place where nobody's ever been before.'\\n\\nVint Cerf has been a near-constant influence on the internet since the days when he was helping create it in the first place. Today he wears many hats, among them VP and chief internet evangelist at Google. He is to be awarded the IEEE’s Medal of Honor at a gala in Atlanta, and ahead of the occasion he spoke with TechCrunch in a wide-ranging interview touching on his work, AI, accessibility and interplanetary internet.\\n\\nTechCrunch: To start out with, can you tell us how Google has changed in your time there?\\n\\nCerf: Well, when I joined the company in 2005, there were 5,000 people already, which is pretty damn big. And of course, my normal attire is three piece suits. The important thing is that I thought I would be raising the sartorial quotient of the company by joining. And now, almost 18 years later, there are 170-some-odd thousand people, and I have failed miserably. So I hope you don’t mind if I take my jacket off.\\n\\nGo right ahead.\\n\\nSo as you might have noticed, Sergey has come back to do a little bit more on the artificial intelligence side of things, which is something he’s always been interested in; I would say historically, we’ve always had an interest in artificial intelligence. But that has escalated significantly over the past decade or so. The acquisition of DeepMind was a brilliant choice. And you can see some of the outcomes first of the spectacular stuff, like playing Go and winning. And then the more productive stuff, like figuring out how 200 million proteins are folded up.\\n\\nThen there’s the large language models and the chatbots. And I think we’re still in a very peculiar period of time, where we’re trying to characterize what these things can and can’t do, and how they go off the rails, and how do you take advantage of them to do useful work? How do we get them to distinguish fact from fiction? All of that is in my view open territory, but then that’s always an exciting place to be — a place where nobody’s ever been before. The thrill of discovery and the risk of hazard create a fairly exciting mix — an exhilarating mix.\\n\\nYou gave a talk recently about, I don’t want to say the dangers of the large language models, but…\\n\\nWell, I did say there are hazards there. I was talking to a bunch of investment bankers, or VCs, and I said, you know, don’t try to sell stuff to your investors just because it’s flashy and shiny. Be cautious about going too fast and trying to apply it without figuring out how to put guardrails in place.\\n\\nI raised a question of hazard and wanting people to be more thoughtful about which applications made sense. I even suggested an analogy: you know how the Society of Automotive Engineers, they have different risk levels for the self driving cars — a risk level idea could apply to artificial intelligence and machine learning.\\n\\nFor entertainment purposes, perhaps it’s not too concerning, unless it goes down some dark path, in which case, you might want to put some friction into the system to deal with that, especially a younger user. But then, as you get to the point where you’re training these things to do medical diagnosis or make investment advice, or make decisions about whether somebody gets out of jail… now suddenly, the risk factors are extremely high.\\n\\nWe shouldn’t be unaware of those risk factors. We can, as we build applications, be prepared to detect excursions away from safe territory, so that we don’t accidentally inflict some harm by the use of these kinds of technologies.\\n\\nSo we need some kind of guardrails.\\n\\nAgain, I’m not expert in this space, but I am beginning to wonder whether we need something kind of like that in order to provide a “super-ego” for the natural language network. So when it starts to go off the rails somewhere, we can observe that that’s happening. And a second network that’s observing both the input and the output might intervene, somehow, and stop the the production of the output.\\n\\nSort of a conscience function?\\n\\nWell, it’s not quite conscience, it’s closer to executive function — the prefrontal cortex. I want to be careful, I’m only reasoning by metaphor here.\\n\\nI know that Microsoft has embarked on something like this. Their version of GPT-4 has an intermediary model like that, they call it Prometheus.\\n\\nPurely as an observation, I had the impression that the Prometheus natural language model would detect and intervene if it thought that the interactions were going down with dark path. I thought that they would implement it in such a way that before you actually say something to the interlocutor that is going down the dark path, you intervene and prevent it from going there at all.\\n\\nMy impression, though, is that it actually produces the output and then discovers that it’s produced it, but and then it says, “Oh, I shouldn’t have done that. Oh, dear, I take that back,” or “I don’t want to talk to you anymore about that.” It’s a little bit like the email that you get occasionally from the Microsoft Outlook system that says, “This person would like to withdraw the message.”\\n\\nI love when that happens… it makes me want to read the original message so badly, even if I wouldn’t have before.\\n\\nYeah, exactly. It’s sort of like putting a big red flag in there saying, boy there’s something juicy in here.\\n\\nYou mentioned the AI models, that it’s an interesting place to work. Do you get the same sort of foundational flavor that you got from working on protocols and other big shared things over the years?\\n\\nWell, what we are seeing is emergent properties of these large language models, that are not necessarily anticipated. And there have been emergent properties showing up in the protocol world. Flow control in particular is a vast headache in the online packet switch environment, and people have been tackling these problems inside and outside of Google for years.\\n\\nOne of the examples of emergent properties that I think very few of us thought about is the domain name business. Once they had value, suddenly, all kinds of emergent properties show up, people with interests that conflict and have to be resolved. Same for internet address space, it’s an even more weird environment where people actually buy IPv4 addresses for like $50 each.\\n\\nI confess to you that as I watched the auctions for IPv4 address space, I was thinking how stupid I was. When I was at the Defense Department in charge of all this, I should have allocated the slash eight, which is 16 million addresses, to myself, and just sit on it, you know, for 50 years, then sell it and retire.\\n\\nEven simple systems have the ability to surprise you. Especially when you have simple systems when a large number of them are interacting with each other. I’ve found myself not necessarily recognizing when these emergent properties will come, but I will say that whenever something gets monetized, you should anticipate there will be emergent properties and possibly unexpected behavior, all driven by greed.\\n\\nLet me ask you about some some other stuff you’re working on. I’m always happy when I see cutting-edge tech being applied to people who need it, people with disabilities, people who like just have not been addressed by the current use cases of tech. Are you still working in the accessibility community?\\n\\nI am very active in the accessibility space. At Google, we have a number of what we call employee resource groups, or ERGs. Yeah, some of them I, executive sponsor for one for Googlers who have hearing problems. And there is a disabilities oriented group, which involves employees who either have disabilities or family members that have disabilities, and they share their stories with each other because often people have similar problems, but don’t know what the solutions were for other people. Also, it’s just nice to know that you’re not alone in some of these challenges. There’s another group called the Grayglers for people that have a little gray in their hair, and I’m the executive sponsor for that. And of course, the focus of attention there is the challenges that arise as you get older, even as you think about retirement and things like that.\\n\\nWhen a lot of so-called Web 2.0 stuff came out 10 years ago, it was totally inaccessible, broke all the screen readers, all this kind of stuff. Somebody has to step in and say, look, we need to have this standard, or else you’re leaving out millions of people. So I’m always interested to hear about what interesting projects or organizations or people are out there.\\n\\nWhat I have come to believe is that engineers, being just given a set of specs that say if you do it this way, it will meet this level of the standard… that doesn’t necessarily produce intuition. You really have to have some intuition in order to make things accessible.\\n\\nSo I’ve come to the conclusion that what we really need is to show people examples of something which is not accessible, and something that is, and let them ingest as many examples as we can give them, because their neural networks will eventually figure out, what is it about this design that makes it accessible? And how do I apply that insight into the next design that I do? So, seeing what works and what doesn’t work is really important. And you often learn a lot more from what doesn’t work than you do from what does.\\n\\nThere’s a guy named Gregg Vanderheiden, who’s at the University of Maryland, he and I did a two-day event [the Future of Interface Workshop] looking at research on accessibility and trying to frame what this is going to look like over the next 10 or 20 years. It really is quite astonishing what the technology might be able to do to act as an augmenting capability for people that that need assistance. There’s great excitement, but at the same time great disappointment, because we haven’t used it as effectively as I think we could have. It’s kind of like how Alexander Graham Bell invented a telephone that can’t be used by people who are deaf, which is why he was working on it in the first place.\\n\\nIt is a funny contradiction of priorities. One thing where I do see some of the the large language and multimodal AI models helping out is that they can describe what they are seeing, even if you can’t see it. I know that one of GPT-4’s first applications was in an application for blind people to view the world around them.\\n\\nWe’re experiencing something close to that right this minute. Since I wear hearing aids, I’m making use of the captioning capability. And at the moment since this is Zoom rather than a Google Meet, there isn’t any setting on this one for closed captioning. I’m exercising the Zoom application through the Chrome browser, and Google has developed a capability for the Chrome browser to detect speech in the incoming sound.\\n\\nSo packets are coming in and they’re known to be sound, it passes through an identification system that produces a caption bar, which you can move around on the screen. And that’s been super helpful for me. For cases like this, where the application doesn’t have captioning, or for random video streaming video that might be coming in and hasn’t been captioned, the caption window automatically pops up. In theory, I think we can do this in 100 different languages, although I don’t know that we’ve activated it for more than four or five. As you say, these tools will become more and more normal, and as time goes on, people will expect the system to adapt to their needs.\\n\\nSo language translation, and speech recognition is quite powerful, but I do want to mention something that I found vaguely unsettling. Recently, I encountered an example of a conversation between a reporter and a chatbot. But he chose deliberately to take the output of the chat bot and have it spoken by the system. And he chose the style of a famous British explorer [David Attenborough].\\n\\nThe text itself was quite well formed, but coming with Attenborough’s accent just added to the weight of the assertions even when they were wrong. The confidence levels, as I’m sure you’ve seen, are very high, even when the thing doesn’t know what it’s talking about.\\n\\nThe reason I bring this up is that we are allowing in these indicators of, how should we say this, of quality, to fool us. Because in the past, they really did mean it was David Attenborough. But here it’s not, it’s just his voice. I got to thinking about this, and I realized there was an ancient example of exactly this problem that showed up 50 years ago at Xerox PARC.\\n\\nThey had a laser printer, and they had the Alto workstation, and the Bravo text editor, it meant the first draft of anything you type to be printed out beautifully formatted with lovely forms and everything else. Normally, you would never see that production quality until after everything had been edited, you know, wrestled with by everybody to get the text formatted, picture-perfect stuff. That meant the first draft stuff came out looking like it was final draft. People didn’t didn’t understand that they were nuts, that they were seeing first-round stuff, and that it wasn’t complete, or necessarily even satisfactory.\\n\\nSo it occurred to me that we’ve reached a point now where technology is fooling us into giving it more weight than it deserves, because of certain indicia that used to be indicative of the investment made in producing it. And… I’m not quite sure what to do about that.\\n\\nI don’t think anyone is!\\n\\nI think somehow or another, we need to make it clear what the provenance is of the thing that we’re looking at. Like how we needed to say this is first-draft material, you know, don’t make any assumptions. So provenance turns out to be a very important concept, especially in a world where we have the ability to imbue content with attributes that we would normally interpret in one way. Like, it’s David Attenborough speaking, and we should listen to that. And yet, which have to be, we have to think more critically about them. Because in fact, the attribute is being delivered artificially.\\n\\nAnd perhaps maliciously.\\n\\nCertainly that too. And this is why critical thinking has become an important skill. But it doesn’t work very well, unless you have enough information to understand the provenance of the material that you’re looking at. I think we are going to have to invest more in provenance and identity in order to evaluate the quality of that which we are experiencing.\\n\\nI wanted to ask you about interplanetary internet, because that whole area is extremely interesting to me.\\n\\nWell, this one, of course, gets started way back in 1998. But I’m a science fiction reader from way back way to age 10 or something, so I got quite excited when it was possible to even think about the possibility of designing and building a communication system that would span the solar system.\\n\\nThe team got started very small, and now 25 years later involves many of the space agencies around the world: JAXA, the Korean Space Agency, NASA and so on. And a growing team of people who are either government funded to do space-based research, or volunteers. There’s a special interest group called the interplanetary networking Special Interest Group, which is part of the Internet Society — that thing got started in 1998. But it has now grown to like 900 people around the world who are interested in this stuff.\\n\\nWe’ve standardized this stuff, we’re on version seven of it, we’re running it up in the International Space Station. It’s intended to be available for the return to the moon and Artemis missions. I’m not going to see the end result of all this, but I’m going to see the first couple of chapters. And I’m very excited about that, because it’s not crazy to actually think about. Like all my other projects, it takes a long time. Patience and persistence!\\n\\nFor something like this it must have been a real challenge, but also a very familiar one. In some ways building something like this is what you’ve been doing your whole career. This is just a different set of restraints and capabilities.\\n\\nYou put your finger on it, exactly right. This is in a different parametric space than the one that works for TCP/IP. And we’re still bumping into some really interesting problems, especially where you have TCP/IP networks running on the moon, for example, locally and interconnecting with other internets on other planets, going through the interplanetary protocol. What does that look like? You know, which IP addresses should be used? We have to figure out, well, how the hell does the Domain Name System work in the context of internets that aren’t on the planet? And it’s really fun!\", metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='Partners of 3one4 Capital, a venture capital firm in India, recently went on a road show to raise a new fund. Within two and a half months, at the height of the worsening global economy, they had secured $200 million. It’s the fourth marquee fund for the Bengaluru-headquartered fund, whose portfolio includes four unicorn startups.\\n\\nThe fund, sixth overall for 3one4 Capital, was oversubscribed to $250 million but the firm is accepting only $200 million to keep itself lean and disciplined, said Pranav Pai, co-founder and partner at 3one4 Capital. The firm’s decision to limit the fund size is emblematic of its strategic choices, which have set it apart from other Indian venture firms.\\n\\n“We are known to give good returns. Our performance has been benchmarked among the best leading performing funds in the space. So we asked ourselves the hard questions, can we continue our performance with a larger fund size? Do we even need that much capital for the early-stage?” said Pai in an interview with TechCrunch.\\n\\nIn recent years, a surge of venture capital firms in India have raised unprecedentedly large funds, sparking concerns about the responsible allocation of this capital, particularly for early-stage startups. Critics question whether there are enough viable companies in the Indian market to absorb and effectively utilize such significant investments.\\n\\nPai, pictured above, asserts that there is ample room for more Indian companies to pursue IPOs, as the nation’s IPO market has proven successful and well-regulated for institutional investors. He anticipates a transformation in India’s stock index, with an increasing number of tech companies, apps, services, fintech, and payment solutions becoming part of the index.\\n\\nDespite this, Pai acknowledges that the Indian market has yet to fully realize its potential for mergers and acquisitions. Although there has been growth in M&A activity—increasing three to four times in the past five years—it remains below expectations. For the Indian market to flourish, Pai emphasizes the need for a more robust M&A landscape.\\n\\nOver the last half-decade, numerous Indian venture firms have shifted their attention to early-stage investments. Despite this increased focus, the market continues to depend on international investors to support mid- and growth-stage deals, highlighting the need for further growth in India’s venture capital ecosystem. “We have high performing mutual funds and PEs. We hope that more of these firms will launch dedicated funds for Indian startups,” he said.\\n\\nHalf of the capital in the new fund for 3one4 has come from Indian investors, another aspect that differentiates the firm from many of its peers. All the systemically important Indian banks, and the top five local banks by market cap overall have invested in the new fund. Eight of the top 10 mutual fund operators are also LPs in the new fund, said Pai. “We are also proud to have leading global endowments, sovereigns and insurance companies as LPs,” he said.\\n\\n“We want to be India’s leading homegrown venture capital firm. We are based here, we invest here – we don’t want to invest in Southeast Asia – and our fund size and strategy are aligned with opportunities in India. As our companies have IPO-ed over the years, we have seen the importance of having India’s largest institutions working with us to help build those companies. It would be difficult if we didn’t have banks to help our companies from everything from revenue collection to payrolls. And mutual funds are buyers, book runners and market makers for IPOs and them buying the stock gives a vote of confidence to the market,” he said.\\n\\n3one4, which focuses largely on early-stage and in sectors including direct-to-consumer tech, media and content, fintech, deep technology and SaaS and enterprise automation, today manages about $750 million in AUM and its portfolio includes HR platform Darwinbox, business-to-business focused neobank Open, consumer-focused neobank Jupiter, Licious, a direct-to-consumer brand that sells meat, local social networks Koo and Lokal, entertainment service Kuku FM, fintech Raise Financial, and gaming firm Loco.\\n\\n3one4 Capital has gained a reputation for its contrarian investment approach, as exemplified by its early investment in Licious. Over five years ago, the prevailing opinion held that India’s price-sensitive market would not pay a premium for online meat delivery. However, Licious has since grown into one of South Asia’s largest direct-to-consumer brands, with a presence in approximately two dozen cities across India.\\n\\nAnother example of 3one4’s daring investments is Darwinbox, a bet made at a time when most investors doubted the ability of Indian SaaS companies to expand internationally or garner sufficient local business subscriptions.\\n\\n3one4 Capital’s contrarian approach extends to the investments it has deliberately avoided as well. In 2021, amidst a frenzy of investment activity in the crypto space, nearly every fund in India sought opportunities and backed crypto startups. However, 3one4 Capital, after thorough evaluation of the sector, chose not to make any investments in crypto.\\n\\nThe firm, which employs 28 people, is also focusing on setting new standards in transparency and governance for itself. It’s the first VC to be a signatory to UN PRI, it said. “We have to report, behave, act and look a certain way. We have to look like the fiduciary of best institutions in the world, and then and only then we quality to tell our portfolio founders that this is how we want to create best in class companies with you,” said Pai.', metadata={'source': '/content/new_articles/05-07-3one4-capital-driven-by-contrarian-bets-raises-200-million-new-fund.txt'}),\n",
       " Document(page_content='Databricks today announced that it has acquired Okera, a data governance platform with a focus on AI. The two companies did not disclose the purchase price. According to Crunchbase, Okera previously raised just under $30 million. Investors include Felicis, Bessemer Venture Partners, Cyber Mentor Fund, ClearSky and Emergent Ventures.\\n\\nData governance was already a hot topic, but the recent focus on AI has highlighted some of the shortcomings of the previous approach to it, Databricks notes in today’s announcement. “Historically, data governance technologies, regardless of sophistication, rely on enforcing control at some narrow waist layer and require workloads to fit into the ‘walled garden’ at this layer,” the company explains in a blog post. That approach doesn’t work anymore in the age of large language models (LLMs) because the number of assets is growing too quickly (in part because so much of it is machine-generated) and because the overall AI landscape is changing so quickly, standard access controls aren’t able to capture these changes quickly enough.\\n\\nOkera then uses an AI-powered system that can automatically discover and classify personally identifiable information, tag it and apply rules to this (with a focus on the metadata), using a no-code interface.\\n\\nAs the Databricks team stressed, that’s one of the reasons the company was interested in acquiring Okera, but the other is the service’s isolation technology, which can enforce governance control on arbitrary workloads without any major overhead. This technology is still in private preview but was likely one of the major reasons Databricks acquired the company.\\n\\nDatabricks, which launched its own LLM a few weeks ago, plans to integrate Okera’s technology into its Unity Catalog, its existing governance solution of data and AI assets. The company also noted that the acquisition will enable Databricks to expose additional APIs that its own data governance partners will be able to use to provide solutions to their customers.\\n\\nWith this acquisition, Databricks is also bringing Okera co-founder and CEO Nong Li on board. Li created the Apache Parquet data storage format and was actually briefly an engineer at Databricks between working at Cloudera and before starting Okera, where he was the founding CTO and became the CEO in February 2022.\\n\\n“As data continues to grow in volume, velocity, and variety across different applications, CIOs, CDOs, and CEOs across the board have to balance those two often conflicting initiatives – not to mention that historically, managing access policies across multiple clouds has been painful and time-consuming,” writes Li in today’s announcement. “Many organizations don’t have enough technical talent to manage access policies at scale, especially with the explosion of LLMs. What they need is a modern, AI-centric governance solution. We could not be more excited to join the Databricks team and to bring our expertise in building secure, scalable and simple governance solutions for some of the world’s most forward-thinking enterprises.”\\n\\nIf you know more about this acquisition, you can contact Frederic on Signal at (860) 208-3416 or by email (frederic@techcrunch.com). You can also reach us via SecureDrop.', metadata={'source': '/content/new_articles/05-03-databricks-acquires-ai-centric-data-governance-platform-okera.txt'}),\n",
       " Document(page_content='AI startup Hugging Face and ServiceNow Research, ServiceNow’s R&D division, have released StarCoder, a free alternative to code-generating AI systems along the lines of GitHub’s Copilot.\\n\\nCode-generating systems like DeepMind’s AlphaCode; Amazon’s CodeWhisperer; and OpenAI’s Codex, which powers Copilot, provide a tantalizing glimpse at what’s possible with AI within the realm of computer programming. Assuming the ethical, technical and legal issues are someday ironed out (and AI-powered coding tools don’t cause more bugs and security exploits than they solve), they could cut development costs substantially while allowing coders to focus on more creative tasks.\\n\\nAccording to a study from the University of Cambridge, at least half of developers’ efforts are spent debugging and not actively programming, which costs the software industry an estimated $312 billion per year. But so far, only a handful of code-generating AI systems have been made freely available to the public — reflecting the commercial incentives of the organizations building them (see: Replit).\\n\\nStarCoder, which by contrast is licensed to allow for royalty-free use by anyone, including corporations, was trained on over 80 programming languages as well as text from GitHub repositories, including documentation and programming notebooks. StarCoder integrates with Microsoft’s Visual Studio Code code editor and, like OpenAI’s ChatGPT, can follow basic instructions (e.g., “create an app UI”) and answer questions about code.\\n\\nCongratulations to all the @BigCodeProject contributors that worked tirelessly over the last 6+ months to bring the vision of releasing a responsibly developed 15B parameter Code LLM to fruition. We cannot thank you enough for the collaboration & contributions to the community. https://t.co/282sCRJq3k — ServiceNow Research (@ServiceNowRSRCH) May 4, 2023\\n\\nLeandro von Werra, a machine learning engineer at Hugging Face and a co-lead on StarCoder, claims that StarCoder matches or outperforms the AI model from OpenAI that was used to power initial versions of Copilot.\\n\\n“One thing we learned from releases such as Stable Diffusion last year is the creativity and capability of the open-source community,” von Werra told TechCrunch in an email interview. “Within weeks of the release the community had built dozens of variants of the model as well as custom applications. Releasing a powerful code generation model allows anybody to fine-tune and adapt it to their own use-cases and will enable countless downstream applications.”\\n\\nBuilding a model\\n\\nStarCoder is a part of Hugging Face’s and ServiceNow’s over-600-person BigCode project, launched late last year, which aims to develop “state-of-the-art” AI systems for code in an “open and responsible” way. Hugging Face supplied an in-house compute cluster of 512 Nvidia V100 GPUs to train the StarCoder model.\\n\\nVarious BigCode working groups focus on subtopics like collecting datasets, implementing methods for training code models, developing an evaluation suite and discussing ethical best practices. For example, the Legal, Ethics and Governance working group explored questions on data licensing, attribution of generated code to original code, the redaction of personally identifiable information (PII), and the risks of outputting malicious code.\\n\\nInspired by Hugging Face’s previous efforts to open source sophisticated text-generating systems, BigCode seeks to address some of the controversies arising around the practice of AI-powered code generation. The nonprofit Software Freedom Conservancy among others has criticized GitHub and OpenAI for using public source code, not all of which is under a permissive license, to train and monetize Codex. Codex is available through OpenAI’s and Microsoft’s paid APIs, while GitHub recently began charging for access to Copilot.\\n\\nFor their parts, GitHub and OpenAI assert that Codex and Copilot — protected by the doctrine of fair use, at least in the U.S. — don’t run afoul of any licensing agreements.\\n\\n“Releasing a capable code-generating system can serve as a research platform for institutions that are interested in the topic but don’t have the necessary resources or know-how to train such models,” von Werra said. “We believe that in the long run this leads to fruitful research on safety, capabilities and limits of code-generating systems.”\\n\\nUnlike Copilot, the 15-billion-parameter StarCoder was trained over the course of several days on an open source dataset called The Stack, which has over 19 million curated, permissively licensed repositories and more than six terabytes of code in over 350 programming languages. In machine learning, parameters are the parts of an AI system learned from historical training data and essentially define the skill of the system on a problem, such as generating code.\\n\\nBecause it’s permissively licensed, code from The Stack can be copied, modified and redistributed. But the BigCode project also provides a way for developers to “opt out” of The Stack, similar to efforts elsewhere to let artists remove their work from text-to-image AI training datasets.\\n\\nThe BigCode team also worked to remove PII from The Stack, such as names, usernames, email and IP addresses, and keys and passwords. They created a separate dataset of 12,000 files containing PII, which they plan to release to researchers through “gated access.”\\n\\nBeyond this, the BigCode team used Hugging Face’s malicious code detection tool to remove files from The Stack that might be considered “unsafe,” such as those with known exploits.\\n\\nThe privacy and security issues with generative AI systems, which for the most part are trained on relatively unfiltered data from the web, are well-established. ChatGPT once volunteered a journalist’s phone number. And GitHub has acknowledged that Copilot may generate keys, credentials and passwords seen in its training data on novel strings.\\n\\n“Code poses some of the most sensitive intellectual property for most companies,” von Werra said. “In particular, sharing it outside their infrastructure poses immense challenges.”\\n\\nTo his point, some legal experts have argued that code-generating AI systems could put companies at risk if they were to unwittingly incorporate copyrighted or sensitive text from the tools into their production software. As Elaine Atwell notes in a piece on Kolide’s corporate blog, because systems like Copilot strip code of its licenses, it’s difficult to tell which code is permissible to deploy and which might have incompatible terms of use.\\n\\nIn response to the criticisms, GitHub added a toggle that lets customers prevent suggested code that matches public, potentially copyrighted content from GitHub from being shown. Amazon, following suit, has CodeWhisperer highlight and optionally filter the license associated with functions it suggests that bear a resemblance to snippets found in its training data.\\n\\nCommercial drivers\\n\\nSo what does ServiceNow, a company that deals mostly in enterprise automation software, get out of this? A “strong-performing model and a responsible AI model license that permits commercial use,” said Harm de Vries, the lead of the Large Language Model Lab at ServiceNow Research and the co-lead of the BigCode project.\\n\\nOne imagines that ServiceNow will eventually build StarCoder into its commercial products. The company wouldn’t reveal how much, in dollars, it’s invested in the BigCode project, save that the amount of donated compute was “substantial.”\\n\\n“The Large Language Models Lab at ServiceNow Research is building up expertise on the responsible development of generative AI models to ensure the safe and ethical deployment of these powerful models for our customers,” de Vries said. “The open-scientific research approach to BigCode provides ServiceNow developers and customers with full transparency into how everything was developed and demonstrates ServiceNow’s commitment to making socially responsible contributions to the community.”\\n\\nStarCoder isn’t open source in the strictest sense. Rather, it’s being released under a licensing scheme, OpenRAIL-M, that includes “legally enforceable” use case restrictions that derivatives of the model — and apps using the model — are required to comply with.\\n\\nFor example, StarCoder users must agree not to leverage the model to generate or distribute malicious code. While real-world examples are few and far between (at least for now), researchers have demonstrated how AI like StarCoder could be used in malware to evade basic forms of detection.\\n\\nWhether developers actually respect the terms of the license remains to be seen. Legal threats aside, there’s nothing at the base technical level to prevent them from disregarding the terms to their own ends.\\n\\nThat’s what happened with the aforementioned Stable Diffusion, whose similarly restrictive license was ignored by developers who used the generative AI model to create pictures of celebrity deepfakes.\\n\\nBut the possibility hasn’t discouraged von Werra, who feels the downsides of not releasing StarCoder aren’t outweighed by the upsides.\\n\\n“At launch, StarCoder will not ship as many features as GitHub Copilot, but with its open-source nature, the community can help improve it along the way as well as integrate custom models,” he said.\\n\\nThe StarCoder code repositories, model training framework, dataset-filtering methods, code evaluation suite and research analysis notebooks are available on GitHub as of this week. The BigCode project will maintain them going forward as the groups look to develop more capable code-generating models, fueled by input from the community.\\n\\nThere’s certainly work to be done. In the technical paper accompanying StarCoder’s release, Hugging Face and ServiceNow say that the model may produce inaccurate, offensive, and misleading content as well as PII and malicious code that managed to make it past the dataset filtering stage.', metadata={'source': '/content/new_articles/05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}),\n",
       " Document(page_content=\"Microsoft doubles down on AI with new Bing features The company's betting the farm on generative AI\\n\\nMicrosoft is embarking on the next phase of Bing’s expansion. And — no surprise — it heavily revolves around AI.\\n\\nAt a preview event this week in New York City, Microsoft execs including Yusuf Mehdi, the CVP and consumer chief marketing officer, gave members of the press, including this reporter, a look at the range of features heading to Bing over the next few days, weeks and months.\\n\\nThey don’t so much reinvent the wheel as they build on what Microsoft has injected into the Bing experience over the past three months or so. Since launching Bing Chat, its AI-powered chatbot powered by OpenAI’s GPT-4 and DALL-E 2 models, Microsoft says that visitors to Bing — which has grown to exceed 100 million daily active users — have engaged in over half a billion chats and created more than 200 million images.\\n\\nLooking ahead, Bing will become more visual, thanks to more image- and graphic-centric answers in Bing Chat. It’ll also become more personalized, with capabilities that’ll allow users to export their Bing Chat histories and draw in content from third-party plugins (more on those later). And it’ll embrace multimodality, at least in the sense that Bing Chat will be able to answer questions within the context of images.\\n\\n“I think it’s safe to say that we’re underway with the transformation of search,” Mehdi said in prepared remarks. “In our minds, we think that today will be the start of the next generation of this ‘search mission.'”\\n\\nOpen, and visual\\n\\nAs of today, the new Bing — the one with Bing Chat — is now available waitlist-free. Anyone can try it out by signing in with a Microsoft Account.\\n\\nIt’s more or less the experience that launched several months ago. But as alluded to earlier, Bing Chat will soon respond with images — at least where it makes sense. Answers to questions (e.g. “Where is Machu Picchu?”) will be accompanied by relevant images if any exist, much like the standard Bing search flow but condensed into a card-like interface.\\n\\nIn a demo at the event, a spokesperson typed the question “Does the saguaro cactus grow flowers?” and Bing Chat pulled up a paragraph-long response alongside an image of the cactus in question. For me, it evoked the “knowledge panels” in Google Search.\\n\\nMicrosoft isn’t saying which categories of content, exactly, might trigger an image. But it does have filtering in place to prevent explicit images from appearing — or so it claims.\\n\\nSarah Bird, the head of responsible AI at Microsoft, told me that Bing Chat benefits from the filtering and moderation already in place with Bing search. Beyond this, Bing Chat uses a combination of “toxicity classifiers,” or AI models trained to detect potentially harmful prompts, and blacklists to keep the chat relatively clean.\\n\\nThose measures didn’t prevent Bing Chat from going off the rails when it first rolled out in preview in early February, it’s worth noting. Our coverage found the chatbot spouting vaccine misinformation and writing a hateful screed from the perspective of Adolf Hitler. Other reporters got it to make threats, claim multiple identities and even shame them for admonishing it.\\n\\nIn another knock against Microsoft, the company just a few months ago laid off the ethics and society team within its larger AI organization. The move left Microsoft without a dedicated team to ensure its AI principles are closely tied to product design.\\n\\nBird, though, asserts that meaningful progress has been made and that these sorts of AI issues aren’t solved overnight — public though Bing Chat may be. Among other measures, a team of human moderators is in place to watch for abuse, she said, such as users attempting to use Bing Chat to generate phishing emails.\\n\\nBut — as members of the press weren’t given the chance to interact with the latest version of Bing beyond curated demos — I can’t say to what extent all that’s made a difference. It’ll doubtless become clear once more folks get their hands on it.\\n\\nOne aspect of Bing Chat that is improving is the transparency around its responses — specifically responses of a fact-based nature. Soon, when asked to summarize a document or about the contents a document (e.g. “what does this page say about the Brooklyn Bridge?”), whether a 20-page PDF or a Wikipedia article, Bing Chat will include citations indicating from where in the text the information came from. Clicking on them will highlight the corresponding passage.\\n\\nProductivity emergent\\n\\nIn another new feature on the visual front, Bing Chat will be able to create charts and graphs when fed the right prompt and data. Previously, asking something like “Which are the most populous cities in Brazil?” would yield a basic list of results. But in a near-future preview, Bing Chat will present those results visually and in the chart type of a user’s choosing.\\n\\nThis seemingly represents a step for Bing toward a full-blown productivity platform, particularly when paired with the enhanced text-to-image generation capabilities coming down the pipeline.\\n\\nIn the coming weeks, Bing Image Creator — Microsoft’s tool that can generate images from text prompts, powered by DALL-E 2 — will understand more languages aside from English (over 100 total). As with English, users will be able to refine the images they generate with follow-up prompts (e.g. “Make an image of a bunny rabbit,” followed by “now make the fur pink”).\\n\\nGenerative art AI has been in the headlines a lot, lately — and not for the most optimistic of reasons necessarily.\\n\\nPlaintiffs have brought several lawsuits against OpenAI and its rival vendors, alleging that copyrighted data — mostly art — was used without their permission to train generative models like DALL-E 2. Generative models “learn” to create art and more by “training” on sample images and text, usually scraped indiscriminately from the public web.\\n\\nI asked Bird about whether Microsoft is exploring ways to compensate creators whose work was swept up in training data, even if the company’s official position is that it’s a matter of fair use. Several platforms launching generative AI tools, including Shutterstock, have kick-started creators funds along these lines. Others, like Spawning, are creating mechanisms to let artists opt out of AI model training altogether.\\n\\nBird implied that these issues will eventually have to be confronted — and that content creators deserve some form of recompense. But she wasn’t willing to commit to anything concrete this week.\\n\\nMultimodal search\\n\\nElsewhere on the image front, Bing Chat is gaining the ability to understand images as well as text. Users will be able to upload images and search the web for related content, for example copying a link to an image of a crocheted octopus and asking Bing Chat the question “how do I make that?” to get step-by-step instructions.\\n\\nMultimodality powers the new page context function in the Edge app for mobile, as well. Users will be able to ask questions in Bing Chat related to the mobile page they’re viewing.\\n\\nMicrosoft wouldn’t say either way, but it seems likely that these new multimodal abilities stem from GPT-4, which can understand images in addition to text. When OpenAI announced GPT-4, it didn’t make the model’s image understanding capabilities available to all customers — and still hasn’t. I’d wager that Microsoft, though, being a major investor in and close collaborator with OpenAI, has some sort of privileged access.\\n\\nAny image upload tool can be abused, of course, which is why Microsoft is employing automated filtering and hashing to block illicit uploads, according to Bird. The jury’s out on how well these work, though — we weren’t given the chance to test image uploads ourselves.\\n\\nNew chat features\\n\\nMultimodality and new visual features aren’t all that’s coming to Bing Chat.\\n\\nSoon, Bing Chat will store users’ chat histories, letting them pick up where they left off and return to previous chats when they wish. It’s an experience akin to the chat history feature OpenAI recently brought to ChatGPT, showing a list of chats and the bot’s responses to each of those chats.\\n\\nThe specifics of the chat history feature have yet to be ironed out, like how long chats will be stored, exactly. But users will be able to delete their history at any time regardless, Microsoft says — addressing the criticisms several European Union governments had against ChatGPT.\\n\\nBing Chat will also gain export and share functionalities, letting users share conversations on social media or to a Word document. Dena Saunders, a partner GM in Microsoft’s web experiences team, told TechCrunch that a more robust copy-and-paste system is in the works — but not in preview just yet — for graphs and images created through Bing Chat.\\n\\nPerhaps the most transformative addition to Bing Chat, though, is plugins. From partners like OpenTable and Wolfram Alpha, plugins greatly extend what Bing Chat can do, for example helping users book a reservation or create visualizations and get answers to challenging science and math questions.\\n\\nLike chat history, the not-yet-live plugins functionality is in the very preliminary stages. There’s no plugins marketplace to speak of; plugins can be toggled on or off from the Bing Chat web interface.\\n\\nSaunders hinted, but wouldn’t confirm, that the Bing Chat plugins scheme was associated with — or perhaps identical to — OpenAI’s recently introduced plugins for ChatGPT. That’d certainly make sense, given the similarities between the two.\\n\\nEdge, refreshed\\n\\nBing Chat is available through Edge as well as the web, of course. And Edge is getting a fresh coat of paint alongside Bing Chat.\\n\\nFirst previewed in February, the new and improved Edge features rounded corners in line with Microsoft’s Windows 11 design philosophy. Elements in the browser are now more “containerized,” as one Microsoft spokesperson put it, and there’s subtle tweaks throughout, like the Microsoft Account image moving left-of-center.\\n\\nIn Compose, Edge’s Bing Chat-powered tool that can write emails and more given a basic prompt (e.g. “write an invitation to my dog’s birthday party”), a new option lets users adjust the length, phrasing and tone of the generated text to nearly anything they’d like. Type in the desired tone, and Bing Chat will write a message to match — Bird says filters are in place to prevent the use of clearly problematic tones, like “hateful” or “racist.”\\n\\nFar more intriguing than Compose, though — at least to me — are actions in Edge, which translate certain Bing Chat prompts into automations.\\n\\nTyping a command like “bring my passwords from another browser” in Bing Chat in the Edge sidebar opens Edge’s browsing data settings page, while the prompt “play ‘The Devil Wears Prada'” pulls up a list of streaming options including Vudu and (predictably) the Microsoft Store. There’s even an action that automatically organizes — and color-coordinates — browsing tabs.\\n\\nActions are in a primitive stage at present. But it’s clear where Microsoft’s going, here. One imagines actions eventually expanding beyond Edge to reach other Microsoft products, like Office 365, and perhaps one day the whole Windows desktop.\\n\\nSaunders wouldn’t confirm or deny that this is the endgame. “Stay tuned for Microsoft Build,” she told me, referring to Microsoft’s upcoming developer conference. We shall.\", metadata={'source': '/content/new_articles/05-04-microsoft-doubles-down-on-ai-with-new-bing-features.txt'}),\n",
       " Document(page_content=\"Generative AI is pretty impressive in terms of its fidelity these days, as viral memes like Balenciaga Pope would suggest. The latest systems can conjure up scenescapes from city skylines to cafes, creating images that appear startlingly realistic — at least on first glance.\\n\\nBut one of the longstanding weaknesses of text-to-image AI models is, ironically, text. Even the best models struggle to generate images with legible logos, much less text, calligraphy or fonts.\\n\\nBut that might change.\\n\\nLast week, DeepFloyd, a research group backed by Stability AI, unveiled DeepFloyd IF, a text-to-image model that can “smartly” integrate text into images. Trained on a dataset of more than a billion images and text, DeepFloyd IF, which requires a GPU with at least 16GB of RAM to run, can create an image from a prompt like “a teddy bear wearing a shirt that reads ‘Deep Floyd'” — optionally in a range of styles.\\n\\nDeepFloyd IF is available in open source, licensed in a way that prohibits commercial use — for now. The restriction was likely motivated by the current tenuous legal status of generative AI art models. Several commercial model vendors are under fire from artists who allege the vendors are profiting from their work without compensating them by scraping that work from the web without permission.\\n\\nBut NightCafe, the generative art platform, was granted early access to DeepFloyd IF.\\n\\nNightCafe CEO Angus Russell spoke to TechCrunch about what makes DeepFloyd IF different from other text-to-image models and why it might represent a significant step forward for generative AI.\\n\\nAccording to Russell, DeepFloyd IF’s design was heavily inspired by Google’s Imagen model, which was never released publicly. In contrast to models like OpenAI’s DALL-E 2 and Stable Diffusion, DeepFloyd IF uses multiple different processes stacked together in a modular architecture to generate images.\\n\\nWith a typical diffusion model, the model learns how to gradually subtract noise from a starting image made almost entirely of noise, moving it closer step by step to the target prompt. DeepFloyd IF performs diffusion not once but several times, generating a 64x64px image then upscaling the image to 256x256px and finally to 1024x1024px.\\n\\nWhy the need for multiple diffusion steps? DeepFloyd IF works directly with pixels, Russell explained. Diffusion models are for the most part latent diffusion models, which essentially means they work in a lower-dimensional space that represents a lot more pixels but in a less accurate way.\\n\\nThe other key difference between DeepFloyd IF and models such as Stable Diffusion and DALL-E 2 is that the former uses a large language model to understand and represent prompts as a vector, a basic data structure. Due to the size of the large language model embedded in DeepFloyd IF’s architecture, the model is particularly good at understanding complex prompts and even spatial relationships described in prompts (e.g. “a red cube on top of a pink sphere”).\\n\\n“It’s also very good at generating legible and correctly spelled text in images, and can even understand prompts in multiple languages,” Russell added. “Of these capabilities, the ability to generate legible text in images is perhaps the biggest breakthrough to make DeepFloyd IF stand out from other algorithms.”\\n\\nBecause DeepFloyd IF can pretty capably generate text in images, Russell expects it to unlock a wave of new generative art possibilities — think logo design, web design, posters, billboards and even memes. The model should also be much better at generating things like hands, he says, and — because it can understand prompts in other languages — it might be able to create text in those languages, too.\\n\\n“NightCafe users are excited about DeepFloyd IF largely because of the possibilities that are unlocked by generating text in images,” Russell said. “Stable Diffusion XL was the first open source algorithm to make headway on generating text — it can accurately generate one or two words some of the time — but it’s still not good enough at it for use cases where text is important.”\\n\\nThat’s not to suggest DeepFloyd IF is the holy grail of text-to-image models. Russell notes that the base model doesn’t generate images that are quite as aesthetically pleasing as some diffusion models, although he expects fine-tuning will improve that.\\n\\nBut the bigger question, to me, is to what degree DeepFloyd IF suffers from the same flaws as its generative AI brethren.\\n\\nA growing body of research has turned up racial, ethnic, gender and other forms of stereotyping in image-generating AI, including Stable Diffusion. Just this month, researchers at AI startup Hugging Face and Leipzig University published a tool demonstrating that models including Stable Diffusion and OpenAI’s DALL-E 2 tend to produce images of people that look white and male, especially when asked to depict people in positions of authority.\\n\\nThe DeepFloyd team, to their credit, note the potential for biases in the fine print accompanying DeepFloyd IF:\\n\\nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. This affects the overall output of the model, as white and western cultures are often set as the default.\\n\\nAside from this, DeepFloyd IF, like other open source generative models, could be used for harm, like generating pornographic celebrity deepfakes and graphic depictions of violence. On the official webpage for DeepFloyd IF, the DeepFloyd team says that they used “custom filters” to remove watermarked, “NSFW” and “other inappropriate content” from the training data.\\n\\nBut it’s unclear exactly which content was removed — and how much might’ve been missed. Ultimately, time will tell.\", metadata={'source': '/content/new_articles/05-05-with-deepfloyd-generative-ai-art-gets-a-text-upgrade.txt'}),\n",
       " Document(page_content='Welcome to The Interchange! If you received this in your inbox, thank you for signing up and your vote of confidence. If you’re reading this as a post on our site, sign up here so you can receive it directly in the future. Every week, we’ll take a look at the hottest fintech news of the previous week. This will include everything from funding rounds to trends to an analysis of a particular space to hot takes on a particular company or phenomenon. There’s a lot of fintech news out there and it’s our job to stay on top of it — and make sense of it — so you can stay in the know. — Mary Ann and Christine\\n\\nBusy, busy, busy\\n\\nIt was a busy week in startup and venture lands, and the fintech space was no exception.\\n\\nIn the venture world, I reported on Peter Ackerson’s departure from Fin Capital earlier this year and the fact that he has since started a new venture firm called Audere Capital. The circumstances around his departure remain fuzzy, but one source speculated that tension arose between Ackerson and Fin founding partner Logan Allin over some of the goings-on at alternative financing startup Pipe last year. More details here.\\n\\nWe also wrote about Tellus, a startup that raised $16 million in an Andreessen Horowitz–led seed round of funding last year that is now being scrutinized by the U.S. government. When I interviewed the company’s co-founder, Rocky Lee, last year, I admit I was a little bit skeptical of any company that would bet on people agreeing to high-interest mortgage rates to upgrade their homes (think 9%!) and using customer savings deposits to fund such loans. When I asked Lee if this was risky, he admitted it was but insisted that Tellus utilized “very strict underwriting criteria” and had not yet seen any defaults “because the majority of its borrowers go on to soon refinance their loans at more favorable terms.” Well, last week U.S. Senator Sherrod Brown, chairman of the U.S. Senate Committee on Banking, Housing, and Urban Affairs, wrote a letter to FDIC chairman Martin Gruenberg expressing concerns about Tellus’s claims. In that letter, Brown pressed the FDIC to review Tellus’s business practices “to ensure that customers are protected from financial fraud and abuse.” In a twist, I discovered that Lee was married to a16z general partner Connie Chan (not sure if he still is). Neither he nor the venture firm commented on the senator’s concerns but Tellus CEO/CTO Jeromee Johnson did provide me with a statement via email. Read more here.\\n\\nInfrastructure continues to be resilient, even in a downturn. This week alone, I wrote about two payments infrastructure companies making moves, and my colleague Ingrid Lunden wrote about Stripe’s latest customer win. For starters, I covered Finix officially becoming a payments processor — a natural evolution really for a company that has slowly been expanding its offerings. In case you forgot, Finix is a startup that Sequoia backtracked on investing in after Stripe (an existing portfolio company) expressed concerns about being too competitive. (Finix got to keep its $21 million, though!) Now that it directly connects to all major U.S. card networks — American Express, Discover, Mastercard and Visa — and no longer relies on a third-party processor, Finix says it’s able to offer businesses “instant onboarding, improved economics and opportunities for lowering interchange fees.” I talked with CEO and co-founder Richie Serna all about it, and why he thinks what Finix has built is different from what legacy players and Stripe have on the market. I also wrote about Liquido, a Mountain View, California–based startup aiming to be the “Stripe of Latin America,” and more. Index Ventures’ Mark Fiorentino led two funding rounds totaling $26 million into the company in 2021. Interestingly, prior to joining Index, Fiorentino helped build and lead business strategy and finance at Stripe from 2015 to 2019. And Ingrid wrote about Stripe landing Uber as a customer, which was a bit unexpected considering that rival Lyft has been a longtime marquee customer of the company.\\n\\nAnd, last but not least, corporate card and spend management startup Brex announced last week a global expansion of its Empower product into new markets so that companies that are its customers now “can spend globally and operate locally” in countries such as Brazil, Canada, Israel, Japan, Mexico, Singapore, South Africa, and the Philippines, as well as in 36 European countries. In an interview with TechCrunch, Brex co-founder and co-CEO Henrique Dubugras said that the company believes the move “will really open up TAM” for Brex since so many existing and prospective U.S. clients “have some sort of global operations.”\\n\\n“One of the big problems that companies have when they operate globally is that they actually need to open up an account in all these different countries where they might have employees. It becomes really complicated to set up all your financial systems on a country by country basis,” he added. “Now, if you use Brex, you can actually operate as if you were a local company with a local card.”\\n\\nIn other words, companies using Brex that have employees who work in other countries are giving those workers the ability to use a corporate card freely in their home countries, while also giving the company the ability to pay the statements in local currency from the local bank.\\n\\n“It’s something that we’ve been trying to do for a while,” Dubugras added, noting that insurtech Lemonade is a customer. — Mary Ann\\n\\nOther weekly news\\n\\nChristine, Mary Ann and Natasha Mascarenhas teamed up to write about the collapse of First Republic Bank, speaking with tech founders and investors who had money in the bank about what happens next. We also spoke with an FRB competitor about what all of these startup bank collapses mean for business. More here.\\n\\nReports Carly Page: “Hackers have published a trove of sensitive data stolen from payment software company AvidXchange after the company fell victim to ransomware for the second time this year. AvidXchange provides cloud-based software that helps organizations automate invoice processing and payment management processes. A ransomware group called RansomHouse claimed responsibility for the recent cyberattack on AvidXchange.” More here.\\n\\nChristine wrote about the launch of former Bolt CEO Ryan Breslow’s new company, Love, which is a wellness marketplace that features an initial 200 curated products, like supplements, health testing kits and essential oils, among such categories as reducing stress and gut health. All of the products on the site pass a set of compliance processes and reviews developed in partnership with clinical trials company Radicle Science, which Breslow said is unique to the company. More here.\\n\\nBritish neobank Revolut launched in Brazil, its first country in Latin America, offering customers a global bank account and crypto investments, Silicon Republic reported. The company already had a presence in the country after hiring Glauber Mota as the CEO of its Brazil business in March 2022. Alex Wilhelm and Anna Heim reported in April that Revolut “saw its valuation decline by some 46% in the eyes of one of its backers.” More here.\\n\\nTage Kene-Okafor reported on Fingo, a YC-backed Kenyan fintech, which launched a neobank — the first of its kind in the East African country, according to the company — in collaboration with Pan-African financial institution Ecobank Kenya. “It’s taken a while for Fingo to get here since CEO Kiiru Muhoya and his co-founders James da Costa, Ian Njuguna and Gitari Tirima founded the Kenyan outfit in January 2021 to provide financial services that appeal to a fast-growing African youthful population that happens to be the youngest globally but the most financially marginalized. After a $200,000 pre-seed round, Fingo got into YC S21 and raised $4 million in seed funding toward the end of that year.” More here.\\n\\nManish Singh reported that Paytm, India’s leading mobile payments firm, reported a 13.2% surge in revenue to $285.7 million in the quarter ending March and pared its loss by 57% to $20.5 million “in a sharp turnaround for the company that is increasingly trying to become profitable following a tremulous year and a half after its public debut.” More here.\\n\\nMore headlines\\n\\nApple and fintechs like Robinhood chase yield-hungry depositors as Fed rate hikes continue. Similarly, Arta Finance, a company providing access to alternative assets, debuted the Harvest Treasuries AI-Managed Portfolio, which offers a 4.62% APY (annual percentage yield), and Wealthfront’s cash account now offers 4.55% for all clients and 5.05% APY for clients who refer a friend.\\n\\nFintech projected to become a $1.5 trillion industry by 2030, according to a new report from Boston Consulting Group and QED Investors\\n\\nOpendoor tech earnings beat by $0.77, revenue topped estimates\\n\\nEveree joins Visa’s Fintech Fast Track Program with launch of Everee Visa® pay card\\n\\nFunding and M&A\\n\\nSeen on TechCrunch\\n\\nAfrican payment service provider Nomba raises $30M, backed by Base10 Partners and Shopify\\n\\nBend is taking on Brex and Ramp with a green twist and a $2.5M seed round\\n\\nAnd elsewhere\\n\\nDigital wallet for insurance Marble bags $4.2M. Speaking about the raise to TechCrunch, CEO Stuart Winchester said via email, “American households are under a lot of financial strain right now, and insurance expenses are no small part of that. We will continue to put out features that make it easier to not only save money and maximize value, but also to reduce the mental load of managing multiple insurance policies. We expect to see the insurance industry in general adopt more of the consumer friendly features that we’ve helped pioneer.”\\n\\nInsurtech startup Novidea raises $50 million Series C\\n\\nExclusive: Former Venmo COO raises $20M for Vera Equity\\n\\nTarabut Gateway raises $32 million to expand Saudi open banking\\n\\nMusic financing startup Duetti raises $32 million to buy old songs\\n\\nBilling platform Inbox Health raises $22.5M and more digital health fundings\\n\\nGoogle’s VC firm just led a $12 million Series A investment in Range, a startup that’s training AI to give financial advice\\n\\nOpenEnvoy raises $15 million to grow AP automation solution\\n\\nMiami-based startup Kiddie Kredit raises $1.4M with support from Dwyane Wade and Baron Davis\\n\\nBlack-owned tech firm Greenwood acquires digital banking rival. TechCrunch covered Greenwood’s last raise in March of 2021 here.\\n\\nJoin us at TechCrunch Disrupt 2023 in San Francisco this September as we explore the impact of fintech on our world today. New this year, we will have a whole day dedicated to all things fintech featuring some of today’s leading fintech figures. Save up to $800 when you buy your pass now through May 15, and save 15% on top of that with promo code INTERCHANGE. Learn more.\\n\\nWe are done for this week and it’s a good thing because we are also TIRED! See you next week — same time, same place. Until then, take good care! xoxo, Mary Ann and Christine', metadata={'source': '/content/new_articles/05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt'}),\n",
       " Document(page_content='SpaceX’s super-heavy launch system Starship is poised to fundamentally reshape the space economy. The 394-foot-tall vehicle, which took to the skies for the first time last month, is designed to carry a staggering amount of mass to low Earth orbit and into deep space.\\n\\nTechCrunch+ spoke with three pure-play space VCs — Space Capital founder and managing partner Chad Anderson, Space.VC founder and general partner Jonathan Lacoste and E2MC Ventures founder Raphael Roettgen — to learn more about how they advise founders to think through Starship’s super-heavy implications.\\n\\nWhile the trio diverges on many fine points, they all agreed that founders should be thinking now about how Starship could affect their operations, for better or worse.\\n\\n“Starship has such high importance to the space sector that probably almost everyone who has a space company has to war game what that means for their business,” Roettgen said.\\n\\nChanging the face of launch …\\n\\nThe most obvious way in which Starship is likely to revolutionize the industry is by continuing the trend SpaceX firmly established with the debut of Falcon 9: further lowering the cost of launching mass to space. Starship will be capable of carrying 100 to 150 tons of stuff to orbit, a paradigm-shifting quantity that far outstrips the payload capacity of any rocket that humans have ever designed.', metadata={'source': '/content/new_articles/05-07-spacex-starship-startups-future.txt'}),\n",
       " Document(page_content='In the must-watch final season of “Succession,” Kendall Roy enters a conference room with his siblings. As the scene opens, he takes a seat and declares: “Who will be the successor? Me.”\\n\\nOf course, that scene didn’t appear on HBO’s hit show, but it’s a good illustration of generative AI’s level of sophistication compared to the real thing. Yet as the Writers Guild of America goes on strike in pursuit of livable working conditions and better streaming residuals, the networks won’t budge on writers’ demands to regulate the use of AI in writers’ rooms.\\n\\n“Our proposal is that we not be required to adapt something that’s output by AI, and that the output of an AI not be considered writers’ work,” comedy writer Adam Conover told TechCrunch. “That doesn’t entirely exclude that technology from the production process, but it does mean that our working conditions wouldn’t be undermined by AI.”\\n\\nBut the Alliance of Motion Picture and Television Producers (AMPTP) refused to engage with that proposal, instead offering a yearly meeting to discuss “advances in technology.”\\n\\n“When we first put [the proposal] in, we thought we were covering our bases — you know, some of our members are worried about this, the area is moving quickly, we should get ahead of it,” Conover said. “We didn’t think it’d be a contentious issue because the fact of the matter is, the current state of the text-generation technology is completely incapable of writing any work that could be used in a production.”\\n\\nThe text-generating algorithms behind tools like ChatGPT are not built to entertain us. Instead, they analyze patterns in massive datasets to respond to requests by determining what is most likely the desired output. So, ChatGPT knows that “Succession” is about an aging media magnate’s children fighting for control of his company, but it is unlikely to come up with any dialogue more nuanced than, “Who will be the successor? Me.”\\n\\nAccording to Ben Zhao, a University of Chicago professor and faculty lead of art anti-mimicry tool Glaze, AI advancements can be used as an excuse for corporations to devalue human labor.\\n\\n“It’s to the advantage of the studios and bigger corporations to basically over-claim ChatGPT’s abilities, so they can, in negotiations at least, undermine and minimize the role of human creatives,” Zhao told TechCrunch. “I’m not sure how many people at these larger companies actually believe what they’re saying.”\\n\\nConover emphasized that some parts of a writer’s job are less obvious than literal scriptwriting but equally difficult to replicate with AI.\\n\\n“It’s going and meeting with the set decoration department that says, ‘Hey, we can’t actually build this prop that you’re envisioning, could you do this instead?’ and then you talk to them and go back and rewrite,” he said. “This is a human enterprise that involves working with other people, and that simply cannot be done by an AI.”\\n\\nComedian Yedoye Travis sees how AI could be useful in a writers’ room.\\n\\n“What we do in writers’ rooms is ultimately bouncing ideas around,” he told TechCrunch. “Even if it’s not good per se, an AI can throw together a script in however many minutes, compared to a week for human writers, and then it’s easier to edit than to write.”\\n\\nBut even if there may be some promise for how humans can leverage this technology, he worries that studios see it merely as a way to demand more from writers over a shorter period of time.\\n\\n“It says to me that they’re only concerned with things being made,” Travis said. “They’re not concerned with people being paid for things being made.”\\n\\nWriters are also advocating to regulate the use of AI in entertainment because it remains a legal grey area.\\n\\n“It’s not clear that the work that it outputs is copyrightable, and a movie studio is not going to spend $50 to $100 million shooting a script that they don’t know that they own the copyright to,” Conover said. “So we figured this would be an easy give for [the AMPTP], but they completely stonewalled on it.”\\n\\nAs the Writers Guild of America strikes for the first time since its historic 100-day action in 2007, Conover said he thinks the debate over AI technology is a “red herring.” With generative AI in such a rudimentary stage, writers are more immediately concerned with dismal streaming residuals and understaffed writing teams. Yet studios’ pushback on the union’s AI-related requests only further reinforces the core issue: The people who power Hollywood aren’t being paid their fair share.\\n\\n“I’m not worried about the technology,” Conover said. “I’m worried about the companies using technology, that is not in fact very good, to undermine our working conditions.”', metadata={'source': '/content/new_articles/05-03-ai-replace-tv-writers-strike.txt'}),\n",
       " Document(page_content='After Google cut all but three of the projects at its in-house incubator Area 120 and shifted it to work on AI projects across Google, one of the legacy efforts — coincidentally also an AI project — is now officially exiting to Google. Checks, an AI-powered tool to check mobile apps for compliance with various privacy rules and regulations, is moving into Google proper as a privacy product aimed at mobile developers.\\n\\nChecks originally made its debut in February 2022, although it was in development for some time before that. In its time at Area 120, it became one of the largest projects in the group, co-founders Fergus Hurley and Nia Castelly told me, with 10 people fully dedicated to it and a number of others contributing less formally. The founders’ job titles under Google will now be GM and Legal Lead, respectively, for Checks.\\n\\nThe amount that Google invested in the project was never disclosed, nor was the valuation of the exit to the parent company from the incubator, but the company has confirmed that there was a valuation and that it had grown since launch.\\n\\nThe company is not disclosing how many customers it has in total but notes that they are in the sectors of gaming, health, finance, education and retail. A sampling includes Miniclip, Rovio, Kongregate, Crayola and Yousician and in total the number of customers represented by its customers is over 3 billion.\\n\\nChecks will sit in the Developer X division. “What Fergus, Nia, and the entire Google Checks team have accomplished is one of the hardest things to do. Their focus on customer needs and nimble execution has served them well, and we’re eager to push ahead in this next phase of Checks,” said Jeanine Banks in a statement.\\n\\nChecks is one of those ideas that feels incredibly timely in that it speaks to an issue that’s growing in importance for consumers — who will vote with their feet when they feel that their privacy is in jeopardy. That in turn also puts more pressure on developers to get things right on the privacy front. App publishers these days are faced with a growing array of rules and regulations around data protection and privacy, not just rules like GDPR in Europe and CCPA in California (and the U.S.) set across different countries and jurisdictions, but also by companies that operate platforms within their own compliance efforts.\\n\\nWhen translated into how those regulations impact apps, there are potential issues at the front end, as well as on the back end, with how apps are coded and information moves from one place to another to consider. It’s a spaghetti bowl of issues, with fixes in one area potentially impacting another and making user experience less smooth to boot.\\n\\nChecks leans on artificial intelligence and machine learning to scan apps and their code to identify areas where there might be violations of privacy and data protection rules, and provides remediation to suggest how to fix it — tasks that would be far more difficult for a team of humans to execute on their own. It’s already integrated with Google’s large language models and what it describes as “app understanding technologies” to power what it identifies and make suggestions for fixing issues.\\n\\nA dashboard lets users monitor and triage issues in the areas of compliance monitoring, data monitoring and store disclosure support (which is focused specifically on Google Play data safety). With the service also aimed at iOS developers, it’s not clear if it will add Apple App Store data safety at any point into that mix. All of this can be monitored in real time on live apps, as well as when they are still in development.\\n\\nWe have reached out to Google to get an update on the status of the other two projects that were spared all-out closure after Area 120 changed focus. They include video dubbing solution Aloud and an as-yet unnamed consumer product from the team that had previously built a bookmarking app Liist (which got acquired by Google).\\n\\nAs of right now, Liist’s co-founder David Friedl still describes himself on LinkedIn as working on a stealth product at Area 120, and Aloud is still using an Area 120 URL, so it seems that they remain in a holding pattern. (We’ll update this if and when we hear more.)\\n\\nIn the meantime, Area 120 itself is also seeing some revolving doors. Clay Bavor, who was running Area 120 among other things and who messaged the big changes to staff in January, was out the door just a month later. He has now teamed up with Bret Taylor — another ex-Googler who has an outsized track record that includes being the CTO of Facebook and the co-CEO of Salesforce — to work on a mystery startup.\\n\\nUpdated with more information about Checks’ valuation and quote from Google.', metadata={'source': '/content/new_articles/05-03-checks-the-ai-powered-data-protection-project-incubated-in-area-120-officially-exits-to-google.txt'}),\n",
       " Document(page_content='Slack has evolved from a pure communications platform to one that enables companies to link directly to enterprise applications without having to resort to dreaded task switching. Today, at the Salesforce World Tour event in NYC, the company announced the next step in its platform’s evolution where it will be putting AI at the forefront of the user experience, making it easier to get information and build workflows.\\n\\nIt’s important to note that these are announcements, and many of these features are not available yet.\\n\\nRob Seaman says that rather than slapping on an AI cover, they are working to incorporate it in a variety of ways across the platform. That started last month with a small step, a partnership with OpenAI to bring a ChatGPT app into Slack, the first piece of a much broader vision for AI on the platform. That part is in beta at the moment.\\n\\nToday’s announcement involves several new integrations, including SlackGPT, the company’s own flavor of generative AI built on top of the Slack platform, which users and developers can tap into to build AI-driven experiences. The content in Slack provides a starting point for building models related to the platform.\\n\\n“We think Slack has a unique advantage when it comes to generative AI. A lot of the institutional knowledge on every topic, team, work item and project is already in Slack through the messages, the files and the clips that are shared every day,” he said.\\n\\nWhen you combine that with Slack’s Partner ecosystem and platform, customers have a lot of options for integrating AI into their workflows. He says that Slack is thinking about this in three ways right now.\\n\\n“For starters, Slack is going to bring AI natively into the user experience with SlackGPT to help customers work faster, communicate better, learn faster, etc. And an example of that is AI-powered conversation summaries and writing assistance for composition that’s going to be directly available in Slack,” he said.\\n\\nThe former could as an example help employees get caught up on a long thread without having to read every message to get the gist of what was being discussed. The latter could help generate Slack messages or content for linked Slack applications. That’s a little less obvious use case. It’s probably easier to write a Slack message yourself unless it’s an automated message that’s part of a workflow, but if you are creating content for Slack Canvas, you could let the generative AI help you.\\n\\nDevelopers can get in on the action too, building AI steps into workflows, giving them the option of tapping into external apps and large language models to build generative AI experiences themselves. Just last week the company made its updated developer experience generally available, and this should make it easier to incorporate generative AI into the platform in customized ways, Seaman says.\\n\\n“So this gives us the foundation to give users choice and flexibility to bring AI into their work in their business whenever they’re ready, and however they like. We’ve got 2,600 apps in the ecosystem right now, which includes a lot of the leading LLMs, and we see a lot of customers already choosing to integrate generative AI into Slack themselves,” he said.\\n\\nFinally EinsteinGPT, the Salesforce flavor of generative AI announced in March, will also be incorporated into Slack, letting employees ask questions directly about Salesforce content, like the users most likely to churn or the accounts most likely to buy, and so forth. This is really about more directly integrating Salesforce content into Slack, the company Salesforce paid $27 billion for a couple of years ago.\\n\\n“Slack is really becoming the conversational interface for Salesforce. So that’s bringing those EinsteinGPT-powered insights from the real-time customer data that exists in Salesforce into Slack to enrich every team’s understanding of the customer,” he said.\\n\\nAs with most of the generative AI tooling we’ve seen being added to enterprise software, Slack is announcing these capabilities long before they release them, but this should give customers a sense of what’s coming, and how AI could be transforming Slack in the future. SlackGPT and EinsteinGPT integration are still in the development phase, but developers can build custom integrations with a variety of LLMs, today. Workflow Builder with SlackGPT AI connectors (which will allow customers to instantly connect ChatGPT or Claude to a workflow or build custom connectors that plug in their own LLMs) will be available this summer.', metadata={'source': '/content/new_articles/05-04-slack-updates-aim-to-put-ai-at-the-center-of-the-user-experience.txt'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "text = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Google I/O 2023 is next week; here’s what we’re expecting A whole bunch of new hardware, coupled with a lot of AI and the best look yet at Android 14\\n\\nGoogle’s annual developer conference, Google I/O, returns to Mountain View’s Shoreline Amphitheater next week, and for the first time in four years, we’ll be returning along with it. The kickoff keynote is always jammed-packed full of information, debuting all of the different software projects the company has been working on for the past year.\\n\\nUpdate: Google just went ahead and announced the Pixel Fold over on Twitter. The company gave a good look at the upcoming foldable smartphone from just about every angle. That means all three of the expected pieces of hardware – including the Pixel 7a and Pixel Tablet – have officially been announced.', metadata={'source': '/content/new_articles/05-05-google-i-o-2023-is-next-week-heres-what-were-expecting.txt'}),\n",
       " Document(page_content='The event, which kicks off May 10 at 10 AM PT will be a big showcase for everything that’s on the way for Android 14. The company has, arguably, missed a step when it comes to the current generative AI land rush — hell, who could have predicted after all of these years that Bing would finally have a moment?\\n\\nCEO Sundar Pichai will no doubt be making the case that the company continues to lead the way in the world of artificial intelligence. There’s always been a fair bit of the stuff at the event largely focused on practical real-world applications like mobile imaging and dealing with customer service. This year, however, I’d say it’s safe to say the company is going to go bonkers with the stuff.', metadata={'source': '/content/new_articles/05-05-google-i-o-2023-is-next-week-heres-what-were-expecting.txt'}),\n",
       " Document(page_content='Hardware, meanwhile, is always a bit of a crapshoot at developer conferences. But after an off-year for the industry at large, a deluge of rumors are aligning, pointing to what’s likely to be an unusually consumer electronics-focused keynote. Given the fact that the last bit is my focus at TechCrunch, I’m going to start the list there.\\n\\nThe Pixel 7a is about as sure as bets get. Google has settled into a comfortable release cadence: releasing a flagship in the fall, followed by a budget device in the spring. The former is designed to be an ideal showcase for its latest mobile operating system and first-party silicon, while the latter makes some compromises for price, while maintaining as many of its predecessors as possible.\\n\\nHow to show excitement without shouting? Asking for a friend Coming to @Flipkart on 11th May. pic.twitter.com/il6GUx3MmR — Google India (@GoogleIndia) May 2, 2023', metadata={'source': '/content/new_articles/05-05-google-i-o-2023-is-next-week-heres-what-were-expecting.txt'}),\n",
       " Document(page_content='How to show excitement without shouting? Asking for a friend Coming to @Flipkart on 11th May. pic.twitter.com/il6GUx3MmR — Google India (@GoogleIndia) May 2, 2023\\n\\nIt’s a good system that works, and Google’s newly focused mobile hardware team has created some surprisingly good devices at extremely reasonable prices. Never one to be outdone by the deluge of rumors, the company went ahead and announced via Twitter its next device is due out on May 11 — the day after Google I/O and, perhaps not coincidentally, my birthday. It was Google India that specifically made the announcement — perhaps not surprising, as the company is likely to aggressively target the world’s number one smartphone market with the product. The image points to a very similar design as the 7 — not really a surprise as these things go. Though it does stop short of actually mentioning the name, as it’s done in the past.', metadata={'source': '/content/new_articles/05-05-google-i-o-2023-is-next-week-heres-what-were-expecting.txt'}),\n",
       " Document(page_content='Basically expect the 7 with cheaper materials. Rumors point to a 6.1-inch device featuring a 90Hz refresh rate, coupled with a 64-megapixel rear camera. The 7’s Tensor G2 returns for a command performance, likely bringing with it many of the software features it enabled the first time around.', metadata={'source': '/content/new_articles/05-05-google-i-o-2023-is-next-week-heres-what-were-expecting.txt'}),\n",
       " Document(page_content='We know for sure that a Pixel Tablet is coming…at some point. Google confirmed the device’s existence at last year’s event, providing a broad 2023 release date, along with a render alongside the rest of the current Pixel lineup. Effectively there are two points this year Google is likely to officially announce the thing: next week or September/October. I would be shocked if the company’s long-awaited (?) reentry into the category doesn’t, at the very least, get a bit of stage time. As a category, the Android tablet has been very hit or miss over the years — presumably/hopefully the company’s got a unique spin here. I would be surprised if Google jumped back into the space without some sort of novel angle.', metadata={'source': '/content/new_articles/05-05-google-i-o-2023-is-next-week-heres-what-were-expecting.txt'}),\n",
       " Document(page_content='The leaks point to a design that would effectively turn the system into one giant Nest dock. It’s not entirely original, as Amazon tried something similar with its Fire tablets, but it would certainly buck the iPad model, which is so pervasive in the industry. Other rumors include the aforementioned Tensor G2, coupled with 8GB of RAM.\\n\\nHere’s your wildcard, folks: the Pixel Fold. Google has seemingly been laying the groundwork for its own foldable for years. Here’s what I wrote a couple of weeks ago:', metadata={'source': '/content/new_articles/05-05-google-i-o-2023-is-next-week-heres-what-were-expecting.txt'}),\n",
       " Document(page_content='Some important background here. First, Google announced foldable screen support for Android back in 2018. Obviously, Samsung was both the big partner and recipient in those days, and Google wanted to make Android development as frictionless as possible for other OEMs in exploring the form factor. The following year, Google foldable patents surfaced. Now, we’re all adults here, who implicitly understand that patents don’t mean a company is working on a product. That said, it’s another key data point in this story. In the intervening years, foldables have begun gathering steam, even outside of the Samsung orbit. I was genuinely amazed by how many different models there were populating the halls of MWC back in March. The leaked renders point to a form factor that is more Samsung Galaxy Z Fold than Samsung Galaxy Z Flip. It also looks like it shares some common design DNA with Oppo’s recently foldable, which is frankly the right direction. EV Leaks says the foldable is half an inch thick', metadata={'source': '/content/new_articles/05-05-google-i-o-2023-is-next-week-heres-what-were-expecting.txt'}),\n",
       " Document(page_content='than Samsung Galaxy Z Flip. It also looks like it shares some common design DNA with Oppo’s recently foldable, which is frankly the right direction. EV Leaks says the foldable is half an inch thick when folded and 0.2 inches unfolded, weight in at 283 grams.', metadata={'source': '/content/new_articles/05-05-google-i-o-2023-is-next-week-heres-what-were-expecting.txt'}),\n",
       " Document(page_content='As evidenced by our trip to MWC back in February, foldables are no longer fringe devices. It’s true that they’re still cost-prohibitive for most, but it’s getting to the point soon where nearly ever Android manufacturer will have their take on the category. So why shouldn’t Google?\\n\\nOther less likely hardware rumors include a Google/Nest AirTag competitor (the company announced yesterday that it’s working with Apple to create a standard for the category), new Pixel Buds and a Pixel Watch 2. I’d say all are unlikely — that last one in particular. We didn’t get much in terms of Nest products last year, but so far not much is forthcoming in terms of rumors for home products.', metadata={'source': '/content/new_articles/05-05-google-i-o-2023-is-next-week-heres-what-were-expecting.txt'}),\n",
       " Document(page_content='Android is always a tentpole of Google I/O for obvious reasons. We’ve already caught some major glimpses of the mobile operating system, by way of beta releases. As Frederic noted in March, “So far, most of the features Google has talked about have also been developer-centric, with only a few user-facing features exposed to far. That also holds true for this second preview, which mostly focuses on added new security and privacy features.”\\n\\nThe operating system, which is apparently named Upside Down Cake internally, is likely set for a summer release in late-July or August. At the top of the list of potential features are a boost to battery life (can always use one of those), additional accessibility features and privacy/security features, which include blocking users from installing ancient apps over malware concerns.', metadata={'source': '/content/new_articles/05-05-google-i-o-2023-is-next-week-heres-what-were-expecting.txt'}),\n",
       " Document(page_content='AI is going to be everywhere. Expect generative AI (Bard) in particular to make appearances in virtually every existing piece of Google consumer software, following the lead of Gmail and Docs. Search and the Chrome browser are prime targets here.\\n\\nA preview of a new Wear OS seems likely. I don’t anticipate a ton of news on the AR/VR side of things, but I would also be surprised if it doesn’t at least get a nod, given what Apple reportedly has in the works for June.\\n\\nThe keynote kicks off at 10 AM PT on May 10. As ever, TechCrunch will be bringing you the news as it breaks.', metadata={'source': '/content/new_articles/05-05-google-i-o-2023-is-next-week-heres-what-were-expecting.txt'}),\n",
       " Document(page_content='Signaling that investments in the supply chain sector remain robust, Pando, a startup developing fulfillment management technologies, today announced that it raised $30 million in a Series B round, bringing its total raised to $45 million.\\n\\nIron Pillar and Uncorrelated Ventures led the round, with participation from existing investors Nexus Venture Partners, Chiratae Ventures and Next47. CEO and founder Nitin Jayakrishnan says that the new capital will be put toward expanding Pando’s global sales, marketing and delivery capabilities.\\n\\n“We will not expand into new industries or adjacent product areas,” he told TechCrunch in an email interview. “Great talent is the foundation of the business — we will continue to augment our teams at all levels of the organization. Pando is also open to exploring strategic partnerships and acquisitions with this round of funding.”', metadata={'source': '/content/new_articles/05-03-ai-powered-supply-chain-startup-pando-lands-30m-investment.txt'}),\n",
       " Document(page_content='Pando was co-launched by Jayakrishnan and Abhijeet Manohar, who previously worked together at iDelivery, an India-based freight tech marketplace — and their first startup. The two saw firsthand manufacturers, distributors and retailers were struggling with legacy tech and point solutions to understand, optimize and manage their global logistics operations — or at least, that’s the story Jayakrishnan tells.\\n\\n“Supply chain leaders were trying to build their own tech and throwing people at the problem,” he said. “This caught our attention — we spent months talking to and building for enterprise users at warehouses, factories, freight yards and ports and eventually, in 2018, decided to start Pando to solve for global logistics through a software-as-a-service platform offering.”', metadata={'source': '/content/new_articles/05-03-ai-powered-supply-chain-startup-pando-lands-30m-investment.txt'}),\n",
       " Document(page_content='There’s truth to what Jayakrishnan’s expressing about pent-up demand. According to a recent McKinsey survey, supply chain companies had — and have — a strong desire for tools that deliver greater supply chain visibility. Sixty-seven percent of respondents to the survey say that they’ve implemented dashboards for this purpose, while over half say that they’re investing in supply chain visibility services more broadly.\\n\\nPando aims to meet the need by consolidating supply chain data that resides in multiple silos within and outside of the enterprise, including data on customers, suppliers, logistics service providers, facilities and product SKUs. The platform provides various tools and apps for accomplishing different tasks across freight procurement, trade and transport management, freight audit and payment and document management, as well as dispatch planning and analytics.', metadata={'source': '/content/new_articles/05-03-ai-powered-supply-chain-startup-pando-lands-30m-investment.txt'}),\n",
       " Document(page_content='Customers can customize the tools and apps or build their own using Pando’s APIs. This, along with the platform’s emphasis on no-code capabilities, differentiates Pando from incumbents like SAP, Oracle, Blue Yonder and E2Open, Jayakrishnan asserts.\\n\\n“Pando comes pre-integrated with leading enterprise resource planning (ERPs) systems and has ready APIs and a professional services team to integrate with any new ERPs and enterprise systems,” he added. “Pando’s no-code capabilities enable business users to customize the apps while maintaining platform integrity — reducing the need for IT resources for each customization.”', metadata={'source': '/content/new_articles/05-03-ai-powered-supply-chain-startup-pando-lands-30m-investment.txt'}),\n",
       " Document(page_content='Pando also taps algorithms and forms of machine learning to make predictions around supply chain events. For example, the platform attempts to match customer orders with suppliers, customers through the “right” channel (in terms of aspects like cost and carbon footprint) and fulfillment strategy (e.g. mode of freight, carrier, etc.). Beyond this, Pando can detect anomalies among deliveries, orders and freight invoices and anticipate supply chain risk given demand and supply trends.\\n\\nPando isn’t the only vendor doing this. Altana, which bagged $100 million in venture capital last October, uses an AI system to connect to and learn from logistics and business-to-business data — creating a shared view of supply chain networks. Everstream, another Pando rival, offers its own dashboards for data analysis, integrated with existing ERP, transportation management and supplier relationship management systems.', metadata={'source': '/content/new_articles/05-03-ai-powered-supply-chain-startup-pando-lands-30m-investment.txt'}),\n",
       " Document(page_content='But Pando has a compelling sales pitch, judging by its momentum. The company counts Fortune 500 manufacturers and retailers — including P&G, J&J, Valvoline, Castrol, Cummins, Siemens, Danaher and Accuride — among its customer base. Since the startup’s Series A in 2020, revenue has grown 8x while the number of customers has increased 5x, Jayakrishnan said.\\n\\nAsked whether he expects expansion to continue well into the future, given the signs of potential trouble on the horizon, Jayakrishnan seemed fairly optimistic. He pointed to a Deloitte survey that found that more than 70% of manufacturing companies have been impacted by supply chain disruptions in the past year, with 90% of those companies experiencing increased costs and declining productivity.', metadata={'source': '/content/new_articles/05-03-ai-powered-supply-chain-startup-pando-lands-30m-investment.txt'}),\n",
       " Document(page_content='The result of those major disruptions? The digital logistics market is estimated to climb to $46.5 billion by 2025, per Markets and Markets — up from $17.4 billion in 2019. Crunchbase reports that investors poured more than $7 billion in seed through growth-stage rounds globally for supply chain-focused startups from January to October 2022, nearly eclipsing 2021’s record-setting levels.\\n\\n“Pando has a strong balance sheet and profit and loss statement, with an eye on profitable growth,” Jayakrishnan said. “We’re are scaling operations in North America, Europe and India with marquee customer wins and a network of strong partners … Pando is well-positioned to ride this growth wave, and drive supply chain agility for the 2030 economy.”', metadata={'source': '/content/new_articles/05-03-ai-powered-supply-chain-startup-pando-lands-30m-investment.txt'}),\n",
       " Document(page_content='As brands incorporate generative AI into their creative workflows to generate new content associated with the company, they need to tread carefully to be sure that the new material adheres to the company’s style and brand guidelines.\\n\\nNova is an early-stage startup building a suite of generative AI tools designed to protect brand integrity, and today, the company is announcing two new products to help brands police AI-generated content: BrandGuard and BrandGPT.\\n\\nWith BrandGuard, you ingest your company’s brand guidelines and style guide, and with a series of models Nova has created, it can check the content against those rules to make sure it’s in compliance, while BrandGPT lets you ask questions about the brand’s content rules in ChatGPT style.', metadata={'source': '/content/new_articles/05-03-nova-is-building-guardrails-for-generative-ai-content-to-protect-brand-integrity.txt'}),\n",
       " Document(page_content='Rob May, founder and CEO at the company, who previously founded Backupify, a cloud backup startup that was acquired by Datto back in 2014, recognized that companies wanted to start taking advantage of generative AI technology to create content faster, but they still worried about maintaining brand integrity, so he came up with the idea of building a guard rail system to protect the brand from generative AI mishaps.\\n\\n“We heard from multiple CMOs who were worried about ‘how do I know this AI-generated content is on brand?’ So we built this architecture that we’re launching called BrandGuard, which is a really interesting series of models, along with BrandGPT, which acts as an interface on top of the models,” May told TechCrunch.', metadata={'source': '/content/new_articles/05-03-nova-is-building-guardrails-for-generative-ai-content-to-protect-brand-integrity.txt'}),\n",
       " Document(page_content='BrandGuard is like the back end for this brand protection system. Nova built five models that look for things that might seem out of whack. They run checks for brand safety, quality checking, whether it’s on brand, whether it adheres to style and whether it’s on campaign. Then it assigns each piece with a content score, and each company can decide what the threshold is for calling in a human to check the content before publishing.\\n\\n“When you have generative AI creating stuff, you can now score it on a continuum. And then you can set thresholds, and if something’s below, say 85% on brand, you can have the system flag it so that a human can take a look at it,” he said. Companies can decide whatever threshold they’re comfortable with.', metadata={'source': '/content/new_articles/05-03-nova-is-building-guardrails-for-generative-ai-content-to-protect-brand-integrity.txt'}),\n",
       " Document(page_content='BrandGPT is designed for working with third parties like an agency or a contractor, who can ask questions about the company’s brand guidelines to make sure they are complying with them, May said. “We’re launching BrandGPT, which is meant to be the interface to all this brand-related security stuff that we’re doing, and as people interact with brands, they can access the style guides and better understand the brand, whether they’re a part of the company or not.\\n\\nThese two products are available in public beta starting today. The company launched last year and has raised $2.4 million from Bee Partners, Fyrfly Ventures and Argon Ventures.', metadata={'source': '/content/new_articles/05-03-nova-is-building-guardrails-for-generative-ai-content-to-protect-brand-integrity.txt'}),\n",
       " Document(page_content='Welcome back to This Week in Apps, the weekly TechCrunch series that recaps the latest in mobile OS news, mobile applications and the overall app economy.\\n\\nThe app economy in 2023 hit a few snags, as consumer spending last year dropped for the first time by 2% to $167 billion, according to data.ai’s “State of Mobile” report. However, downloads are continuing to grow, up 11% year-over-year in 2022 to reach 255 billion. Consumers are also spending more time in mobile apps than ever before. On Android devices alone, hours spent in 2022 grew 9%, reaching 4.1 trillion.\\n\\nThis Week in Apps offers a way to keep up with this fast-moving industry in one place with the latest from the world of apps, including news, updates, startup fundings, mergers and acquisitions, and much more.\\n\\nDo you want This Week in Apps in your inbox every Saturday? Sign up here: techcrunch.com/newsletters\\n\\nTop Stories\\n\\nDorsey criticizes Twitter, Musk on the alternative social networks he’s backing', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='Do you want This Week in Apps in your inbox every Saturday? Sign up here: techcrunch.com/newsletters\\n\\nTop Stories\\n\\nDorsey criticizes Twitter, Musk on the alternative social networks he’s backing\\n\\nAs demand for Bluesky, the Jack Dorsey-backed decentralized Twitter rival grows, the former Twitter CEO took to the app to share his thoughts on Twitter’s future, Elon Musk and the decision to take the company private. As TechCrunch’s Darrell Etherington reported, Dorsey responded to questions posed to him from other users and reporters on Bluesky, including one where he was asked if Musk has proven to be the best possible steward for the social network.\\n\\nDorsey said he had not:\\n\\nNo. Nor do I think he acted right after realizing his timing was bad. Nor do I think the board should have forced the sale. It all went south. But it happened and all we can do now is build something to avoid that ever happening again. So I’m happy Jay and team and nostr devs exist and building it.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='However, the Twitter co-founder stressed that Twitter would have never survived as a public company and defended himself from an accusation that he was deflecting blame for Twitter’s current situation.\\n\\nThough Bluesky is having a moment, particularly as a haven for marginalized groups, sex workers and trans users, it’s not the only Twitter alternative Dorsey is now backing. In fact, he’s been more active in recent days on the social network nostr (which he also financially backed), where he’s also been critical of some of Musk’s recent decisions. For example, as The NYT reported, Dorsey posted last month “This is weak,” in response to Musk’s move to stop Twitter users from linking to Substack after it launched a Twitter-like service for its own community of writers and readers.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='Dorsey also touted his belief in these platforms during Block’s recent earnings call, suggesting on his nostr profile this may be the first time the network’s name had been mentioned during a public earnings event.\\n\\n“Open protocols represent another fork in the road moment for people and companies,” Dorsey told investors. “Bitcoin, nostr, Bluesky, web5 and others are all working to level the playing field for competition and give individuals and organizations entirely new capabilities,” he added.\\n\\nOver the past few weeks, Bluesky has been gaining traction, but the network has been difficult to access due to its invite-only nature. That’s turned Bluesky invites into hot commodities, where they’re even selling for hundreds of dollars on eBay, as most users have to wait to receive only one invite every two weeks.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='Bluesky leadership will also sometimes gift a user with a larger number of invites in order to have them invite members of a specific community. Developers who can demonstrate they’re building a Bluesky app may also request additional invites, we understand.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='The network has received outsized press coverage relative to its size — just 50,000+ users — possibly because of the heavy infusion of tech journalists on there and Dorsey’s name attached. But the reality is that Bluesky’s future remains uncertain. The company, for now, is able to build and grow thanks to the $13 million in initial funds it received from Twitter, where it was incubated under Dorsey’s leadership. It has since spun out into its own, independent company (a public benefit LLC). It’s unclear how Bluesky intends to maintain its operations in the long term, not to mention its freewheeling culture and accepting community. Networks can often be pleasant and welcoming when small, like Bluesky — or early Twitter, for that matter — but face challenges once they scale to millions of users.\\n\\nNewFronts round-up', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='NewFronts round-up\\n\\nThis week was IAB’s NewFronts, where digital media companies and social networks pitched their platforms to advertisers looking to reach online audiences. The event saw major brands introducing a range of new offerings, including both ad products and formats, as well as touting their latest features, in some cases, as Snap did with its My AI integration.\\n\\nHere’s what you may have missed from the app makers’ NewFronts this week:\\n\\nSnap said it’s beginning to test a feature that lets partners leverage its new My AI chatbot to place sponsored links in front of users. Snap also announced new ad slots, including the option to reserve the first video ad seen in Snapchat’s Friend Stories and the ability to advertise within its TikTok-like Spotlight feature.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='Snap also announced including the option to reserve the and the ability to YouTube introduced new ad opportunities for Shorts, including the expansion of Shorts into Video reach campaigns that leverage Google AI to serve the best combination of ads and improve reach on YouTube. Plus, YouTube Select is now coming to Shorts, allowing advertisers to place their ads alongside the most popular YouTube Shorts’ content, similar to TikTok Pulse. Another option, First Position on Shorts, will let advertisers be the first ad Shorts users see in their viewing session.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='including the that leverage Google AI to serve the best combination of ads and improve reach on YouTube. Plus, allowing advertisers to place their ads alongside the most popular YouTube Shorts’ content, similar to TikTok Pulse. Another option, will let advertisers be the first ad Shorts users see in their viewing session. TikTok announced partnerships with big-name publishers, including NBCU, Condé Nast, DotDash Meredith, BuzzFeed and others, in an effort to pull in more premium ad dollars. The new premium ad product, Pulse Premiere, would allow marketers, for the first time, to position their brand ads directly after TikTok’s publisher and media partners’ content in over a dozen categories, including lifestyle, sports, entertainment, education and more. Publisher partners would receive a rev share as a result.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='The would allow marketers, for the first time, to position their brand ads directly after TikTok’s publisher and media partners’ content in over a dozen categories, including lifestyle, sports, entertainment, education and more. Publisher partners would receive a rev share as a result. Meta announced AR would become available to Reels Ads and Facebook Stories. They had previously been available only to the Facebook Feed, Instagram Feed and Instagram Stories. It also announced features to make Reels Ads more interactive, including a t est of a larger “call to action” button with additional advertiser information on Facebook and Instagram Reels ads. Other updates included multi-destination product ads, the ability to pause a video ad to preview a link’s destination and support for Reels Ads campaigns with select third-party measurement firms .', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='They had previously been available only to the Facebook Feed, Instagram Feed and Instagram Stories. It also announced features to make Reels Ads more interactive, including a t with additional advertiser information on Facebook and Instagram Reels ads. Other updates included the ability to and support for . NBCU will let Peacock users shop products that appear in its content through “Must ShopTV,” which puts a QR code on the screen when a shoppable product appears.\\n\\nApple & Google team up on Bluetooth tracker safety', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='Apple & Google team up on Bluetooth tracker safety\\n\\nAfter numerous cases of Bluetooth trackers like Apple’s AirTag being used for stalking or other criminal apps, Apple and Google this week released a joint announcement saying they will work together to lead an industry-wide initiative to draft a specification that would alert users in the case of unwanted tracking from Bluetooth devices. The companies said they’re seeking input from other industry participants and advocacy groups in the matter, and noted that other tracker makers like Samsung, Tile, Chipolo, eufy Security and Pebblebee have also expressed interest in the draft.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='The companies submitted a proposed specification as an Internet-Draft via a standards development organization, the Internet Engineering Task Force (IETF). Other interested parties are now being invited to review and comment over the next three months. After this time, Apple and Google will offer feedback and will release a production implementation of the specification by year’s end that will be supported in future versions of iOS and Android, they said.\\n\\nThe spec would build on the AirTag protections Apple had already released but also, critically, would ensure that users would be able to combat unwanted tracking by offering tools across both iOS and Android platforms.\\n\\nGoogle’s participation could signal more than a desire to protect its users — it’s been rumored the company may also be developing an AirTag rival.\\n\\nPlatforms\\n\\nApple\\n\\nGoogle — I/O Preview', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='Google’s participation could signal more than a desire to protect its users — it’s been rumored the company may also be developing an AirTag rival.\\n\\nPlatforms\\n\\nApple\\n\\nGoogle — I/O Preview\\n\\nGoogle I/O kicks off next week and we already know at least one of the announcements — because Google leaked it. The company plans to introduce its first foldable smartphone with the Pixel Fold. The device shares Pixel’s familiar camera bar and features an interface that showcases Material UI design. We expect to learn more at the event.\\n\\nIn addition, Google I/O 2023 should bring a Pixel 7a , a budget device that could also help address Pixel demand in emerging markets, plus possibly a Pixel tablet, an AirTag rival, a Wear OS update, and a lot of new developer tools and features. We also expect to hear quite a bit about Google’s AI plans, with generative AI (like Bard) appearing across Google’s line of products.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content=', a budget device that could also help address Pixel demand in emerging markets, plus possibly a Pixel tablet, an AirTag rival, a Wear OS update, and a lot of new developer tools and features. We also expect to hear quite a bit about Google’s AI plans, with generative AI (like Bard) appearing across Google’s line of products. To get ready for I/O, even if you’re attending virtually, Google offered a new planning guide and a playlist of developer content to help attendees prepare.\\n\\nto help attendees prepare. Checks, Google’s AI-powered data protection project, exited to Google from its in-house incubator Area 120. The tool uses AI to check mobile apps for compliance with various privacy rules and regulations.\\n\\nApp Updates\\n\\nSocial', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='App Updates\\n\\nSocial\\n\\nSocial networking app IRL’s CEO Abraham Shafi stepped down following allegations he used bots to inflate the number of users IRL reported publicly and to its investors , The Information reported. A former employee had alleged he was fired after expressing concern over the use of bots. The SEC is now investigating if the company violated securities laws. IRL raised around $200 million from SoftBank Vision Fund, Founders Fund and others.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content=', The Information reported. A former employee had alleged he was fired after expressing concern over the use of bots. The SEC is now investigating if the company violated securities laws. IRL raised around $200 million from SoftBank Vision Fund, Founders Fund and others. After laying off 50% of staff, declining audio social network Clubhouse says it’s building “Clubhouse 2.0,” but hasn’t shared exactly what that plan may involve. Last year, the company began shifting its focus away from public audio to private rooms but it’s not clear there’s much demand for audio social networking in the post-pandemic market.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='but hasn’t shared exactly what that plan may involve. Last year, the company began shifting its focus away from public audio to private rooms but it’s not clear there’s much demand for audio social networking in the post-pandemic market. Once-hot viral app Poparazzi shuts down and returns remaining funds to investors. The app had let friends tag others to build out their social profiles of real moments, not polished images, but had been on the decline, with only a few thousand MAUs down from a height of 4 million MAUs previously.\\n\\nA Twitter bug saw users able to regain their blue Verification checks just by editing their bio. Shortly afterward, the Twitter desktop website began randomly logging out users. Later in the week, the mobile website was also down.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='Shortly afterward, the Twitter desktop website began randomly logging out users. Later in the week, the mobile website was also down. As Bluesky gains attention, rival decentralized social platform Mastodon announced a new, simpler onboarding experience that provides new users with an account on mastodon.social by default , instead of requiring them to pick a server. This doesn’t eliminate server choice, it simply means that joining another server requires a few extra clicks.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content=', instead of requiring them to pick a server. This doesn’t eliminate server choice, it simply means that joining another server requires a few extra clicks. Neighborhood social network Nextdoor added new features powered by generative AI, including an Assistant feature aimed at helping users write posts that are more likely to drive positive community engagement. The Assistant will offer writing suggestions that users can review and optionally adopt. The company says it will also use AI to better match content to users when providing recommendations.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='including an Assistant feature aimed at helping users write posts that are more likely to drive positive community engagement. The Assistant will offer writing suggestions that users can review and optionally adopt. The company says it will also use AI to better match content to users when providing recommendations. BeReal is testing another new feature in the U.K., “RealPeople,” that shows users a timeline of the “world’s most interesting people” — that is, athletes, artists, activists and other public figures. The company also recently began testing the option to post more often as usage has declined.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='and other public figures. The company also recently began testing the option to post more often as usage has declined. Meta introduced new discovery and personalization options for Facebook Reels. Users can now choose “Show More” or “Show Less” options to control what sort of Reels they want to see. Facebook will also explain why it’s showing you a Reel, like if a friend viewed it, and is adding Reels to the main navigation at the top of Facebook Watch.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='WordPress drops Twitter integration, says sharing to Instagram and Mastodon is coming instead. The Automattic-owned publishing platform said the Twitter connection on Jetpack and WordPress.com will cease to work, meaning users’ blog posts will no longer be auto-shared to Twitter as before. The company said Elon Musk’s decision to “dramatically change the terms and pricing” for Twitter’s API was to blame for this decision. The API now starts at $42,000/month for 50 million tweets. The move will likely hurt Twitter more than WordPress, as the latter powers over 40% of the global internet, including WordPress.com blogs.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='The Automattic-owned publishing platform said the Twitter connection on Jetpack and WordPress.com will cease to work, meaning users’ blog posts will no longer be auto-shared to Twitter as before. The company said Elon Musk’s decision to “dramatically change the terms and pricing” for Twitter’s API was to blame for this decision. The API now starts at $42,000/month for 50 million tweets. The move will likely hurt Twitter more than WordPress, as the latter powers over 40% of the global internet, including WordPress.com blogs. Mozilla announced it’s opening up its own Mastodon server — or “instance,” in Mastodon lingo — into private beta testing. The company had said last year it planned to create and begin testing a publicly accessible instance at mozilla.social. It explains its approach to Mastodon will involve high levels of moderation.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='The company had said last year it planned to create and begin testing a publicly accessible instance at mozilla.social. It explains its approach to Mastodon will involve high levels of moderation. Twitter announced it would make its API free for public service announcements after New York’s Metro Transit Service (MTS) abandoned the service and the National Weather Services (NWS) said it would no longer auto-post warnings.\\n\\nafter New York’s Metro Transit Service (MTS) abandoned the service and the National Weather Services (NWS) said it would no longer auto-post warnings. TikTok’s U.S. head of trust and safety Eric Han is leaving the company on May 12 as lawmakers weigh a TikTok ban. Han had played a key role in TikTok’s strategy to avoid a U.S. ban.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='as lawmakers weigh a TikTok ban. Han had played a key role in TikTok’s strategy to avoid a U.S. ban. Discord is making all users change their usernames, the company announced this week. Originally, Discord users had been identified by a name and random number separated by a hash sign, but now it wants to adopt a simpler format so people can more easily share their usernames with others. The new plan will include a unique alphanumeric username with the @ symbol in front of it, plus a freely assignable display name that can be changed at any time.\\n\\nAI\\n\\nSlack introduced SlackGPT, its own generative AI built on Slack’s platform which developers can use to create AI-driven experiences.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='AI\\n\\nSlack introduced SlackGPT, its own generative AI built on Slack’s platform which developers can use to create AI-driven experiences.\\n\\nwhich developers can use to create AI-driven experiences. Microsoft launched its Bing chatbot to all users globally, meaning there’s no more waitlist to get started. It’s also adding more image- and graphic-centric answers in Bing Chat, including by creating graphs and charts and generating images from text prompts. It will also allow users to export their Bing Chat histories. And it will embrace multimodality, meaning it can understand queries with images and text combined. Bing now sees more than 100 million daily active users and says visitors have engaged in over half a billion chats.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='It’s also adding more image- and graphic-centric answers in Bing Chat, including by creating graphs and charts and generating images from text prompts. It will also allow users to export their Bing Chat histories. And it will embrace multimodality, meaning it can understand queries with images and text combined. Bing now sees more than 100 million daily active users and says visitors have engaged in over half a billion chats. Plexamp, the music player originally incubated by the Labs division of media company Plex, is tapping into ChatGPT with its latest update. The new feature called “Sonic Sage,” powered by OpenAI’s ChatGPT, will build unique music playlists by scanning users’ libraries and leveraging their TIDAL subscription.\\n\\nMedia & Entertainment\\n\\nFintech', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='Media & Entertainment\\n\\nFintech\\n\\nYC-backed Kenyan fintech Fingo launched its neobanking app, developed in collaboration with Pan-African financial institution Ecobank Kenya. The company raised $4 million in seed funding after its YC S21 participation. Fingo offers users a bank account, paired with free peer-to-peer transactions and access to savings, financial education and smart spending analytics.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='The company raised $4 million in seed funding after its YC S21 participation. Fingo offers users a bank account, paired with free peer-to-peer transactions and access to savings, financial education and smart spending analytics. The FDIC is looking into Tellus, an Andreessen Horowitz-backed fintech company that claims it can offer people higher yields on their savings balances by using that money to fund certain U.S. single-family-home loans. U.S. Senator Sherrod Brown, chairman of the Senate Banking, Housing, and Urban Affairs Committee, wrote a letter to FDIC Chairman Martin Gruenberg expressing concerns about Tellus, and asking the FDIC to review Tellus’s business practices which may put customers at risk.\\n\\nMessaging\\n\\nWhatsApp now lets users create single-vote polls and forward media with captions, Meta announced this week. Single-vote polls let users run a poll where people are only allowed to vote once, including multiple choice, as has been the default.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='Meta announced this week. Single-vote polls let users run a poll where people are only allowed to vote once, including multiple choice, as has been the default. Reddit’s latest update provides link previews for messaging apps. Now, when you share a Reddit link via a messaging app, it will include a visual preview of the content, the subreddit name, the total upvotes tally and the number of comments. The update also includes the ability to share directly to IG Stories and other tools for publishers.\\n\\nTravel & Transportation\\n\\nFollowing its acquisition by Via, Citymapper said it’s lowering the paywall for its premium features while also introducing a new subscription plan ($1.49/mo) purely for removing ads.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='Following its acquisition by Via, Citymapper said it’s lowering the paywall for its premium features while also introducing a new subscription plan ($1.49/mo) purely for removing ads.\\n\\nwhile also introducing a new subscription plan ($1.49/mo) purely for removing ads. Uber reported a Q1 earnings beat with its revenue up 29% YoY to $8.82 billion, gross bookings up 19% YoY to $31.4 billion and adjusted EBITDA up 353% YoY to $761 million. It also reported a $157 million net loss.\\n\\ngross bookings up 19% YoY to $31.4 billion and adjusted EBITDA up 353% YoY to $761 million. It also reported a $157 million net loss. Uber Eats is also planning to offer support for Live Activities and Dynamic Island on iPhone and integrated with Alexa for order updates.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='for order updates. Lyft shared worrisome Q2 guidance sending its stock down after Q1 earnings where it had reported a 14% YoY increase in revenue to $1 billion and a net loss drop of 5% to $187.6 million. Ridership was up 9.8% YoY to 19.5 million.\\n\\nGaming\\n\\nSnowman, the mobile game studio behind Alto’s Adventure and Alto’s Odyssey, launched its newest title, Laya’s Horizon, exclusively with Netflix. The wingsuit game sees players mastering the art of flying, diving off mountains, weaving across forests and gliding over rivers to unlock new abilities as they explore a vast and peaceful world.\\n\\nCross-platform game engine Unity announced layoffs of 8% of its workforce, or around 600 jobs, after laying off 500+ in January and last June.\\n\\nafter laying off 500+ in January and last June. Amazon announced that customers in the United States, Canada, Germany and the United Kingdom can now play Fortnite on their Fire TVs via its Amazon Luna cloud gaming service.\\n\\nCommerce & Food Delivery', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='Commerce & Food Delivery\\n\\nAmazon Inspire, the e-commerce giant’s in-app TikTok-like shopping feed has rolled out to all customers in the United States. The company had been experimenting since last year with the new feed, which features content creators by influencers.\\n\\nThe company had been experimenting since last year with the new feed, which features content creators by influencers. DoorDash revenue was up 40% YoY in Q1, reaching $2.04 billion, beating estimates of $1.93 billion. Its net loss also declined 3% to $162 million and orders were up 27% to 512 million.\\n\\nEtc.\\n\\nAmazon rolled out a Matter update for Alexa that includes support for Thread, setup on iOS, and a new version of its Works with Alexa program.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='Etc.\\n\\nAmazon rolled out a Matter update for Alexa that includes support for Thread, setup on iOS, and a new version of its Works with Alexa program.\\n\\nand a new version of its Works with Alexa program. Match Group posted a Q1 earnings miss with revenue down by 1% YoY to $787 million and paying users down 3% to 15.9 million. The company, however, said it’s “very possible” the recent Apple-Epic court decision could result in App Store fee relief.\\n\\nMedtech startup Healthy.io, which provides urine analysis through a mobile app, is laying off a third of its staff, or around 70 people. The company had just raised $50 million in Series D funding.\\n\\nThe company had just raised $50 million in Series D funding. Airbnb announced Rooms, a feature that focuses on the ability to book single rooms averaging $67 per night as users complain about excessive fees, onerous checkout procedures and rising Airbnb prices.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='averaging $67 per night as users complain about excessive fees, onerous checkout procedures and rising Airbnb prices. Google’s smart home app, Google Home, added support for smart garage door openers.\\n\\nSecurity\\n\\nGoogle announced that passkeys are now rolling out to Google Account users globally. Passkey let users sign in to websites and apps using the same biometrics or screen-lock PIN they use to unlock their devices.\\n\\nPasskey let users sign in to websites and apps using the same biometrics or screen-lock PIN they use to unlock their devices. Google announced that in 2022, it prevented 1.43 million policy-violating apps from being published on Google Play “in part due to new and improved security features and policy enhancements.”\\n\\nGovernment, Policy and Lawsuits', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='Government, Policy and Lawsuits\\n\\nThe EU’s Digital Markets Act (DMA) became applicable on May 2, but enforcement is not expected until spring 2024. The act focused on gatekeepers like Apple, Google, Meta and Microsoft. It limits how they can use third-party data, bans self-preferencing, introduces interoperability requirements, bans tracking users for targeted ads without consent and more. It also says app stores can’t require the use of their own payment services and permits app sideloading.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='Bipartisan U.S. lawmakers reintroduced the Kids Online Safety Act with updates aimed at fixing earlier issues. The bill says platforms have to take reasonable steps to stop the spread of posts that promote eating disorders, suicide, substance abuse and more and undergo independent analysis about their safety for minors. It now also includes protections for support services, like the National Suicide Hotline, substance abuse groups and LGBTQ+ youth centers. However, critics, including the ACLU, say the changes are not enough and they remain opposed to the increased surveillance of kids this bill would require and other matters.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='The bill says platforms have to take reasonable steps to stop the spread of posts that promote eating disorders, suicide, substance abuse and more and undergo independent analysis about their safety for minors. It now also includes protections for support services, like the National Suicide Hotline, substance abuse groups and LGBTQ+ youth centers. However, critics, including the ACLU, say the changes are not enough and they remain opposed to the increased surveillance of kids this bill would require and other matters. France’s competition watchdog announced interim measures against Meta, saying it suspects Meta of abusing its dominant position in the French market for ads on social media and across the broader (non-search-related) online ads market.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='saying it suspects Meta of abusing its dominant position in the French market for ads on social media and across the broader (non-search-related) online ads market. The U.S. Federal Trade Commission (FTC) says Meta has “repeatedly violated” privacy rules and proposed to tighten its 2020 privacy order against the company, which would completely bar it from monetizing data from anyone under 18 in any way, among other new restrictions. The FTC also accused Meta of COPPA, a children’s privacy law, by misrepresenting its Messenger Kids parental controls, which allowed group chats and group calls with unapproved contacts.\\n\\nFunding and M&A\\n\\nAmazon acquired a small audio-focused artificial intelligence firm called Snackable.AI in 2022, The Post reported. Deal terms weren’t disclosed but Mari Joller, the founder and CEO of Snackable, is now the artificial intelligence and machine learning product leader at Amazon.\\n\\nDownloads\\n\\nRTRO', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='Downloads\\n\\nRTRO\\n\\nNew social networking startup RTRO launched its app this week with the goal of connecting brands, creators and their fans and followers in a more positive environment focused on human connections and communities, not algorithm-driven content. To accomplish this, RTRO divides its social experience into two parts — on one side, you can keep up with friends or family in RTRO’s “circles.” On the other side, users can switch over to see content from creators and brands in their own space, dubbed RTRO TV.\\n\\nDistroKid\\n\\nMusic distribution service DistroKid this week launched its first mobile app, initially only for iPhone. The new app lets artists upload new releases, receive instant payment alerts, access stats from Apple and Spotify, edit metadata and more from their phones. The company said the mobile app had been the number one request from DistroKid members.', metadata={'source': '/content/new_articles/05-06-this-week-in-apps-apple-and-google-team-up-on-trackers-google-i-o-preview-apps-hit-newfronts.txt'}),\n",
       " Document(page_content='OpenAI may be synonymous with machine learning now and Google is doing its best to pick itself up off the floor, but both may soon face a new threat: rapidly multiplying open source projects that push the state of the art and leave the deep-pocketed but unwieldy corporations in their dust. This Zerg-like threat may not be an existential one, but it will certainly keep the dominant players on the defensive.\\n\\nThe notion is not new by a long shot — in the fast-moving AI community, it’s expected to see this kind of disruption on a weekly basis — but the situation was put in perspective by a widely shared document purported to originate within Google. “We have no moat, and neither does OpenAI,” the memo reads.', metadata={'source': '/content/new_articles/05-05-google-and-openai-are-walmarts-besieged-by-fruit-stands.txt'}),\n",
       " Document(page_content='I won’t encumber the reader with a lengthy summary of this perfectly readable and interesting piece, but the gist is that while GPT-4 and other proprietary models have obtained the lion’s share of attention and indeed income, the head start they’ve gained with funding and infrastructure is looking slimmer by the day.\\n\\nWhile the pace of OpenAI’s releases may seem blistering by the standards of ordinary major software releases, GPT-3, ChatGPT and GPT-4 were certainly hot on each other’s heels if you compare them to versions of iOS or Photoshop. But they are still occurring on the scale of months and years.', metadata={'source': '/content/new_articles/05-05-google-and-openai-are-walmarts-besieged-by-fruit-stands.txt'}),\n",
       " Document(page_content='What the memo points out is that in March, a leaked foundation language model from Meta, called LLaMA, was leaked in fairly rough form. Within weeks, people tinkering around on laptops and penny-a-minute servers had added core features like instruction tuning, multiple modalities and reinforcement learning from human feedback. OpenAI and Google were probably poking around the code, too, but they didn’t — couldn’t — replicate the level of collaboration and experimentation occurring in subreddits and Discords.\\n\\nCould it really be that the titanic computation problem that seemed to pose an insurmountable obstacle — a moat — to challengers is already a relic of a different era of AI development?\\n\\nSam Altman already noted that we should expect diminishing returns when throwing parameters at the problem. Bigger isn’t always better, sure — but few would have guessed that smaller was instead.\\n\\nGPT-4 is a Walmart, and nobody actually likes Walmart', metadata={'source': '/content/new_articles/05-05-google-and-openai-are-walmarts-besieged-by-fruit-stands.txt'}),\n",
       " Document(page_content='GPT-4 is a Walmart, and nobody actually likes Walmart\\n\\nThe business paradigm being pursued by OpenAI and others right now is a direct descendant of the SaaS model. You have some software or service of high value and you offer carefully gated access to it through an API or some such. It’s a straightforward and proven approach that makes perfect sense when you’ve invested hundreds of millions into developing a single monolithic yet versatile product like a large language model.\\n\\nIf GPT-4 generalizes well to answering questions about precedents in contract law, great — never mind that a huge number of its “intellect” is dedicated to being able to parrot the style of every author who ever published a work in the English language. GPT-4 is like a Walmart. No one actually wants to go there, so the company makes damn sure there’s no other option.', metadata={'source': '/content/new_articles/05-05-google-and-openai-are-walmarts-besieged-by-fruit-stands.txt'}),\n",
       " Document(page_content='But customers are starting to wonder, why am I walking through 50 aisles of junk to buy a few apples? Why am I hiring the services of the largest and most general-purpose AI model ever created if all I want to do is exert some intelligence in matching the language of this contract against a couple hundred other ones? At the risk of torturing the metaphor (to say nothing of the reader), if GPT-4 is the Walmart you go to for apples, what happens when a fruit stand opens in the parking lot?', metadata={'source': '/content/new_articles/05-05-google-and-openai-are-walmarts-besieged-by-fruit-stands.txt'}),\n",
       " Document(page_content='It didn’t take long in the AI world for a large language model to be run, in highly truncated form of course, on (fittingly) a Raspberry Pi. For a business like OpenAI, its jockey Microsoft, Google or anyone else in the AI-as-a-service world, it effectively beggars the entire premise of their business: that these systems are so hard to build and run that they have to do it for you. In fact it starts to look like these companies picked and engineered a version of AI that fit their existing business model, not vice versa!', metadata={'source': '/content/new_articles/05-05-google-and-openai-are-walmarts-besieged-by-fruit-stands.txt'}),\n",
       " Document(page_content='Once upon a time you had to offload the computation involved in word processing to a mainframe — your terminal was just a display. Of course that was a different era, and we’ve long since been able to fit the whole application on a personal computer. That process has occurred many times since as our devices have repeatedly and exponentially increased their capacity for computation. These days when something has to be done on a supercomputer, everyone understands that it’s just a matter of time and optimization.\\n\\nFor Google and OpenAI, the time came a lot quicker than expected. And they weren’t the ones to do the optimizing — and may never be at this rate.', metadata={'source': '/content/new_articles/05-05-google-and-openai-are-walmarts-besieged-by-fruit-stands.txt'}),\n",
       " Document(page_content='For Google and OpenAI, the time came a lot quicker than expected. And they weren’t the ones to do the optimizing — and may never be at this rate.\\n\\nNow, that doesn’t mean that they’re plain out of luck. Google didn’t get where it is by being the best — not for a long time, anyway. Being a Walmart has its benefits. Companies don’t want to have to find the bespoke solution that performs the task they want 30% faster if they can get a decent price from their existing vendor and not rock the boat too much. Never underestimate the value of inertia in business!', metadata={'source': '/content/new_articles/05-05-google-and-openai-are-walmarts-besieged-by-fruit-stands.txt'}),\n",
       " Document(page_content='Sure, people are iterating on LLaMA so fast that they’re running out of camelids to name them after. Incidentally, I’d like to thank the developers for an excuse to just scroll through hundreds of pictures of cute, tawny vicuñas instead of working. But few enterprise IT departments are going to cobble together an implementation of Stability’s open source derivative-in-progress of a quasi-legal leaked Meta model over OpenAI’s simple, effective API. They’ve got a business to run!\\n\\nBut at the same time, I stopped using Photoshop years ago for image editing and creation because the open source options like Gimp and Paint.net have gotten so incredibly good. At this point, the argument goes the other direction. Pay how much for Photoshop? No way, we’ve got a business to run!', metadata={'source': '/content/new_articles/05-05-google-and-openai-are-walmarts-besieged-by-fruit-stands.txt'}),\n",
       " Document(page_content='What Google’s anonymous authors are clearly worried about is that the distance from the first situation to the second is going to be much shorter than anyone thought, and there doesn’t appear to be a damn thing anybody can do about it.\\n\\nExcept, the memo argues: embrace it. Open up, publish, collaborate, share, compromise. As they conclude:', metadata={'source': '/content/new_articles/05-05-google-and-openai-are-walmarts-besieged-by-fruit-stands.txt'}),\n",
       " Document(page_content='The best way to avoid a down round is to found an AI startup\\n\\nAs we see unicorns slash staff and the prevalence of down rounds spike, it may seem that the startup ecosystem is chock-full of bad news and little else. That’s not precisely the case.\\n\\nWhile AI, and in particular the generative AI subcategory, are as hot as the sun, not all venture attention is going to the handful of names that you already know. Sure, OpenAI is able to land nine and 10-figure rounds from a murderer’s row of tech investors and mega-cap corporations. And rising companies like Hugging Face and Anthropic cannot stay out of the news, proving that smaller AI-focused startups are doing more than well.\\n\\nIn fact, new data from Carta, which provides cap table management and other services, indicates that AI-focused startups are outperforming their larger peer group at both the seed and Series A stage.', metadata={'source': '/content/new_articles/05-06-ai-startups-q1-investments.txt'}),\n",
       " Document(page_content='The dataset, which notes that AI-centered startups are raising more and at higher valuations than other startups, indicates that perhaps the best way to avoid a down round today is to build in the artificial intelligence space.\\n\\nWhat the data says\\n\\nPer Carta data relating to the first quarter of the year, seed funding to non-AI startups in the U.S. market that use its services dipped from $1.64 billion to $1.08 billion, or a decline of around 34%. That result is directionally aligned with other data that we’ve seen regarding Q1 2023 venture capital totals; the data points down.', metadata={'source': '/content/new_articles/05-06-ai-startups-q1-investments.txt'}),\n",
       " Document(page_content='The legal spats between artists and the companies training AI on their artwork show no sign of abating.\\n\\nWithin the span of a few months, several lawsuits have emerged over generative AI tech from companies including OpenAI and Stability AI, brought by plaintiffs who allege that copyrighted data — mostly art — was used without their permission to train the generative models. Generative AI models “learn” to create art, code and more by “training” on sample images and text, usually scraped indiscriminately from the web.\\n\\nIn an effort to grant artists more control over how — and where — their art’s used, Jordan Meyer and Mathew Dryhurst co-founded the startup Spawning AI. Spawning created HaveIBeenTrained, a website that allows creators to opt out of the training dataset for one art-generating AI model, Stable Diffusion v3, due to be released in the coming months.', metadata={'source': '/content/new_articles/05-03-spawning-lays-out-its-plans-for-letting-creators-opt-out-of-generative-ai-training.txt'}),\n",
       " Document(page_content='As of March, artists had used HaveIBeenTrained to remove 80 million pieces of artwork from the Stable Diffusion training set. By late April, that figure had eclipsed 1 billion.\\n\\nAs the demand for Spawning’s service grew, the company — which was entirely bootstrapped up until that point — sought an outside investment. And it got it. Spawning today announced that it raised $3 million in a seed round led by True Ventures with participation from the Seed Club Ventures, Abhay Parasnis, Charles Songhurst, Balaji Srinivisan, Jacob.eth and Noise DAO.\\n\\nSpeaking to TechCrunch via email, Meyer said that the funding will allow Spawning to continue developing “IP standards for the AI era” and establish more robust opt-out and opt-in standards.', metadata={'source': '/content/new_articles/05-03-spawning-lays-out-its-plans-for-letting-creators-opt-out-of-generative-ai-training.txt'}),\n",
       " Document(page_content='Speaking to TechCrunch via email, Meyer said that the funding will allow Spawning to continue developing “IP standards for the AI era” and establish more robust opt-out and opt-in standards.\\n\\n“We are enthusiastic about the potential of AI tooling. We developed domain expertise in the field from being passionate about new opportunities AI provides to creators, but feel that consent is a fundamental layer to make these developments something everyone can feel good about,” Meyer said.\\n\\nSpawning’s metrics speak for themselves. Clearly, there’s a demand from artists for more say in how their art’s used (or scraped, as the case may be). But beyond partnerships with art platforms like Shutterstock and ArtStation, Spawning hasn’t managed to rally the industry around a common opt-out or provenance standard.', metadata={'source': '/content/new_articles/05-03-spawning-lays-out-its-plans-for-letting-creators-opt-out-of-generative-ai-training.txt'}),\n",
       " Document(page_content='Adobe, which recently announced generative AI tools, is pursuing its own opt-out mechanisms and tooling. So is DeviantArt, which in November launched a protection that relies on HTML tags to prohibit the software robots that crawl pages for images from downloading those images for training sets. OpenAI, the generative AI giant in the room, still doesn’t offer an opt-out tool — nor has it announced plans to anytime soon.\\n\\nSpawning has also come under criticism for the opaqueness — and vagueness — of its opt-out process. As Ars Technica noted in a recent piece, the opt-out process doesn’t appear to fit the definition of consent for personal data use in Europe’s General Data Protection Regulation, which states that consent must be actively given, not assumed by default. Also unclear is how Spawning intends to legally verify the identities of artists who make opt-out requests — or indeed, if it intends to attempt this at all.', metadata={'source': '/content/new_articles/05-03-spawning-lays-out-its-plans-for-letting-creators-opt-out-of-generative-ai-training.txt'}),\n",
       " Document(page_content='Spawning’s solution is multipronged. First, it plans to make it easier for AI model trainers to honor opt-out requests and streamline the process for creators. Then, Spawning will offer more services to organizations seeking to protect the work of their artists, Meyer says.\\n\\n“We want to build the consent layer for AI, which we feel will be a fundamentally helpful piece of infrastructure moving forward,” he added. “We plan to grow Spawning to address the many different domains touched by the AI economy, as each domain has their own particular needs.”\\n\\nIn a first step toward this ambitious vision, Spawning in March enabled “domain opt-outs,” allowing creators and content partners to quickly opt-out content from whole websites. Spawning says that 30,000 domains to date have been registered in the system.', metadata={'source': '/content/new_articles/05-03-spawning-lays-out-its-plans-for-letting-creators-opt-out-of-generative-ai-training.txt'}),\n",
       " Document(page_content='April will mark the release of an API and open source Python package that’ll greatly expand the breadth of content that Spawning touches. Previously, opt-out requests through Spawning only applied to the LAION-5B dataset — the dataset used to train Stable Diffusion. As of April, any website, app or service will be able to use Spawning’s API to automatically comply with opt-outs not just for image data, but for text, audio, videos and more.\\n\\nMeyer says that Spawning will aggregate every new opt-out method (e.g. Adobe’s and DeviantArt’s) into its Python package for model trainers, with the goal of cutting down on the number of accounts model creators have to manage to comply with opt-out requests.', metadata={'source': '/content/new_articles/05-03-spawning-lays-out-its-plans-for-letting-creators-opt-out-of-generative-ai-training.txt'}),\n",
       " Document(page_content='To boost visibility, Spawning is partnering with Hugging Face, one of the larger platforms for hosting and running AI models, to add a new info box on Hugging Face that’ll alert users to the proportion of “opted-out” data within text-to-image datasets. The box will also link to a Spawning API sign-up page so that model trainers can remove opted-out images at training time.\\n\\n“We feel that once companies and developers know that the option to honor creator wishes is available, there is little reason not to honor them,” Meyer said. “We are excited about the future of generative AI, but creators and organizations alike need standards in place to have their data work in their favor.”', metadata={'source': '/content/new_articles/05-03-spawning-lays-out-its-plans-for-letting-creators-opt-out-of-generative-ai-training.txt'}),\n",
       " Document(page_content='Looking ahead, Spawning intends to release an “exact-duplicate” detection feature to match opted-out images with copies that the platform finds across the web, followed by a “near-duplicate” detection feature to notify artists when Spawning finds likely copies of their work that’ve been cropped, compressed or otherwise slightly modified.\\n\\nBeyond that, there’s plans for a Chrome extension to let creators pre-emptively opt out of their work posted anywhere on the web and a caption search on the HaveIBeenTrained website to directly search image descriptions. The site’s current search tool uses only approximate matches between text and images as well as URL searches to find content hosted on specific websites.\\n\\nSpawning — now beholden to investors — plans to make money by building services on top of its content infrastructure, although Meyer wouldn’t divulge much. How that’ll sit with content creators remains to be seen.', metadata={'source': '/content/new_articles/05-03-spawning-lays-out-its-plans-for-letting-creators-opt-out-of-generative-ai-training.txt'}),\n",
       " Document(page_content='“We’ve spoken to quite a few organizations, with many conversations being too premature to announce, and think that our funding announcement and increased visibility will go some way to offer assurances that what we are building is a robust and dependable standard to work with,” Meyer said. “After we complete these features, we’ll begin building infrastructure to support more datasets — including music, video and text.”', metadata={'source': '/content/new_articles/05-03-spawning-lays-out-its-plans-for-letting-creators-opt-out-of-generative-ai-training.txt'}),\n",
       " Document(page_content='Well that was fast. The U.K.’s competition watchdog has announced an initial review of “AI foundational models”, such as the large language models (LLMs) which underpin OpenAI’s ChatGPT and Microsoft’s New Bing. Generative AI models which power AI art platforms such as OpenAI’s DALL-E or Midjourney will also likely fall in scope.\\n\\nThe Competition and Markets Authority (CMA) said its review will look at competition and consumer protection considerations in the development and use of AI foundational models — with the aim of understanding “how foundation models are developing and producing an assessment of the conditions and principles that will best guide the development of foundation models and their use in the future”.\\n\\nIt’s proposing to publish the review in “early September”, with a deadline of June 2 for interested stakeholders to submit responses to inform its work.', metadata={'source': '/content/new_articles/05-04-cma-generative-ai-review.txt'}),\n",
       " Document(page_content='It’s proposing to publish the review in “early September”, with a deadline of June 2 for interested stakeholders to submit responses to inform its work.\\n\\n“Foundation models, which include large language models and generative artificial intelligence (AI), that have emerged over the past five years, have the potential to transform much of what people and businesses do. To ensure that innovation in AI continues in a way that benefits consumers, businesses and the UK economy, the government has asked regulators, including the [CMA], to think about how the innovative development and deployment of AI can be supported against five overarching principles: safety, security and robustness; appropriate transparency and explainability; fairness; accountability and governance; and contestability and redress,” the CMA wrote in a press release.”', metadata={'source': '/content/new_articles/05-04-cma-generative-ai-review.txt'}),\n",
       " Document(page_content='Stanford University’s Human-Centered Artificial Intelligence Center’s Center for Research on Foundation Models is credited with coining the term “foundational models”, back in 2021, to refer to AI systems that focus on training one model on a huge amount of data and adapting it to many applications.\\n\\n“The development of AI touches upon a number of important issues, including safety, security, copyright, privacy, and human rights, as well as the ways markets work. Many of these issues are being considered by government or other regulators, so this initial review will focus on the questions the CMA is best placed to address — what are the likely implications of the development of AI foundation models for competition and consumer protection?” the CMA added.\\n\\nIn a statement, its CEO, Sarah Cardell, also said:', metadata={'source': '/content/new_articles/05-04-cma-generative-ai-review.txt'}),\n",
       " Document(page_content='In a statement, its CEO, Sarah Cardell, also said:\\n\\nAI has burst into the public consciousness over the past few months but has been on our radar for some time. It’s a technology developing at speed and has the potential to transform the way businesses compete as well as drive substantial economic growth. It’s crucial that the potential benefits of this transformative technology are readily accessible to UK businesses and consumers while people remain protected from issues like false or misleading information. Our goal is to help this new, rapidly scaling technology develop in ways that ensure open, competitive markets and effective consumer protection.\\n\\nSpecifically, the U.K. competition regulator said its initial review of AI foundational models will:\\n\\nexamine how the competitive markets for foundation models and their use could evolve\\n\\nexplore what opportunities and risks these scenarios could bring for competition and consumer protection', metadata={'source': '/content/new_articles/05-04-cma-generative-ai-review.txt'}),\n",
       " Document(page_content='examine how the competitive markets for foundation models and their use could evolve\\n\\nexplore what opportunities and risks these scenarios could bring for competition and consumer protection\\n\\nproduce guiding principles to support competition and protect consumers as AI foundation models develop\\n\\nWhile it may seen early for the antitrust regulator to conduct a review of such a fast-moving emerging technology the CMA is acting on government instruction.\\n\\nAn AI white paper published in March signalled ministers’ preference to avoid setting any bespoke rules (or oversight bodies) to govern uses of artificial intelligence at this stage. However ministers said existing U.K. regulators — including the CMA, which was directly name-checked — would be expected to issue guidance to encourage safe, fair and accountable uses of AI.', metadata={'source': '/content/new_articles/05-04-cma-generative-ai-review.txt'}),\n",
       " Document(page_content='The CMA says its initial review of foundational AI models is in line with instructions in the white paper, where the government talked about existing regulators conducting “detailed risk analysis” in order to be in a position to carry out potential enforcements, i.e. on dangerous, unfair and unaccountable applications of AI, using their existing powers.\\n\\nThe regulator also points to its core mission — to support open, competitive markets — as another reason for taking a look at generative AI now.\\n\\nNotably, the competition watchdog is set to get additional powers to regulate Big Tech in the coming years, under plans taken off the back-burner by prime minister Rishi Sunak’s government last month, when ministers said it would move forward with a long-trailed (but much delayed) ex ante reform aimed at digital giants’ market power.', metadata={'source': '/content/new_articles/05-04-cma-generative-ai-review.txt'}),\n",
       " Document(page_content='The expectation is that the CMA’s Digital Markets Unit, up and running since 2021 in shadow form, will (finally) gain legislative powers in the coming years to apply pro-active “pro-competition” rules which are tailored to platforms that are deemed to have “strategic market status” (SMS). So we can speculate that providers of powerful foundational AI models may, down the line, be judged to have SMS — meaning they could expect to face bespoke rules on how they must operate vis-a-vis rivals and consumers in the U.K. market.\\n\\nThe U.K.’s data protection watchdog, the ICO, also has its eye on generative AI. It’s another existing oversight body which the government has tasked with paying special mind to AI under its plan for context-specific guidance to steer development of the tech through the application of existing laws.', metadata={'source': '/content/new_articles/05-04-cma-generative-ai-review.txt'}),\n",
       " Document(page_content='In a blog post last month, Stephen Almond, the ICO’s executive director of regulatory risk, offered some tips and a little warning for developers of generative AI when it comes to compliance with U.K. data protection rules. “Organisations developing or using generative AI should be considering their data protection obligations from the outset, taking a data protection by design and by default approach,” he suggested. “This isn’t optional — if you’re processing personal data, it’s the law.”\\n\\nOver the English Channel in the European Union, meanwhile, lawmakers are in the process of deciding a fixed set of rules that are likely to apply to generative AI.\\n\\nNegotiations toward a final text for the EU’s incoming AI rulebook are ongoing — but currently there’s a focus on how to regulate foundational models via amendments to the risk-based framework for regulating uses of AI the bloc published in draft over two years ago.', metadata={'source': '/content/new_articles/05-04-cma-generative-ai-review.txt'}),\n",
       " Document(page_content='It remains to be seen where the EU’s co-legislators will end up on what’s sometimes also referred to as general purpose AI. But, as we reported recently, parliamentarians are pushing for a layered approach to tackle safety issues with foundational models; the complexity of responsibilities across AI supply chains; and to address specific content concerns (like copyright) which are associated with generative AI.\\n\\nAdd to that, EU data protection law already applies to AI, of course. And privacy-focused investigations of models like ChatGPT are underway in the bloc — including in Italy where an intervention by the local watchdog led to OpenAI rushing out a series of privacy disclosures and controls last month.\\n\\nThe European Data Protection Board also recently set up a task force to support coordination between different data protection authorities on investigations of the AI chatbot. Others investigating ChatGPT include Spain’s privacy watchdog.', metadata={'source': '/content/new_articles/05-04-cma-generative-ai-review.txt'}),\n",
       " Document(page_content='It’s that time of week again, folks — Week in Review (WiR) time. For those new to the scene, WiR is TechCrunch’s regular newsletter that recaps the biggest tech stories over the past few days. There’s no better digest for the person on the go, we’d argue — but of course, we’re a little biased.\\n\\nBefore we get into the meat of the thing, a quick reminder that TC City Spotlight: Atlanta is fast approaching. On June 7, TechCrunch is headed to Atlanta, where we’ll host a pitch competition, a talk on the economics of equality, a panel discussion on investing in the Atlanta ecosystem and more.\\n\\nElsewhere, there’s a TechCrunch Live event with Persona and Index Ventures on May 10, which will touch on how Persona keeps pace with new threats and how Index made a prescient move to spot and back Persona early on. And we have Disrupt in San Francisco from September 19–21 — our annual conference is jam-packed with expert-led sessions and interviews with movers and shakers in the tech space.', metadata={'source': '/content/new_articles/05-06-amazon-launches-free-channels-check-marks-come-to-gmail-and-openai-raises-more-moolah.txt'}),\n",
       " Document(page_content='Now, with that out of the way, here’s the top headlines.\\n\\nmost read\\n\\nAmazon debuts free channels: Amazon is doubling down on free, ad-supported content with this week’s introduction of Fire TV Channels. The new, free and ad-supported video experience, which came to Fire TV devices this week, will be continuously updated throughout the day and integrated into several areas across the Fire TV interface, Sarah reports.\\n\\nBio update for a check: Briefly, a bug on Twitter let legacy blue check holders get their badge back by updating their bio. Readers will recall that blue checks on Twitter once signified that a user was “verified,” but now serve as an indication that they’re paying for Twitter’s premium subscription service, Twitter Blue. Verified users who chose not to pay recently faced the prospect of blue check removal — but not necessarily permanently, judging by the bug.', metadata={'source': '/content/new_articles/05-06-amazon-launches-free-channels-check-marks-come-to-gmail-and-openai-raises-more-moolah.txt'}),\n",
       " Document(page_content='Google ditches passwords for passkeys: This week Google rolled out passkeys to Google Account users globally, roughly a year after the company — alongside Apple, Microsoft and the FIDO Alliance — announced a partnership to broadly advance passkey adoption. With passkeys, users’ authentication synchronizes across devices through the cloud using cryptographic key pairs, allowing them to sign in to websites and apps using the same biometrics or screen-lock PIN they use to unlock their devices.\\n\\nMicrosoft debuts Pegasus: Microsoft this week announced that it’ll extend the Startup Founders Hub, its self-service platform that provides founders with free resources, including Azure credits, with a new incubator program called the Pegasus Program. Pegasus will select startups with products that “fill a market need” and give them up to $350,000 in Azure, GitHub and LinkedIn credits plus backing from advisors, as well as “access to the best Microsoft tech,” Microsoft says.', metadata={'source': '/content/new_articles/05-06-amazon-launches-free-channels-check-marks-come-to-gmail-and-openai-raises-more-moolah.txt'}),\n",
       " Document(page_content='Blue check marks come to Gmail: Google is going to start displaying a blue check mark next to select senders’ names on Gmail to verify their identity, the company said on Wednesday. The check marks will automatically appear next to companies that have adopted Gmail’s existing brand indicators for message identification feature, reports Aisha.\\n\\nOpenAI rakes in the dough: OpenAI, the startup behind the widely used conversational AI model ChatGPT, has picked up new backers. In an exclusive report, Jagmeet and Ingrid reveal that VC firms, including Sequoia Capital, Andreessen Horowitz, Thrive, K2 Global and Founders Fund, have put just over $300 million into OpenAI, valuing the company at between $27 billion and $29 billion.', metadata={'source': '/content/new_articles/05-06-amazon-launches-free-channels-check-marks-come-to-gmail-and-openai-raises-more-moolah.txt'}),\n",
       " Document(page_content='Apple releases security fix: On Monday, Apple released its first batch of publicly available “rapid security” patches, aimed at quickly fixing security vulnerabilities that are under active exploitation or pose significant risks to its customers. Apple says that these patches, which are enabled by default, were intended to let customers update their devices faster than a typical software upgrade.\\n\\nMusk settles for less: A defamation case brought against Tesla chief executive Elon Musk by critic Randeep Hothi is coming to a close, reportedly costing the billionaire ten big ones. Lawyers representing Hothi — a vocal member of the TSLAQ short-seller community on Twitter who rose to prominence as a skeptic of Tesla’s gigafactory plans and “full self-driving” tech — said in a statement that Musk asked to settle the nearly three-year-old case back in March.', metadata={'source': '/content/new_articles/05-06-amazon-launches-free-channels-check-marks-come-to-gmail-and-openai-raises-more-moolah.txt'}),\n",
       " Document(page_content='A new LLM for Alexa: Amazon is building a more “generalized and capable” large language model to power Alexa, said Amazon CEO Andy Jassy during the company’s first-quarter earnings call this week. He added that although Amazon has had an LLM powering Alexa, Amazon is working on one that’s more capable than the current one.\\n\\naudio', metadata={'source': '/content/new_articles/05-06-amazon-launches-free-channels-check-marks-come-to-gmail-and-openai-raises-more-moolah.txt'}),\n",
       " Document(page_content='TechCrunch’s stable of podcasts grows by the day — and it’s all quality stuff. This week, the Equity folks covered First Republic Bank, Poparazzi’s shutdown, Databricks’ acquisition, who’s going head-to-head with Stripe, the rise of down rounds and why Bluesky had them feeling less gray. Meanwhile, Found spoke with Stefan Bauer about how his company, Marker Learning, is cutting the cost of learning disability assessments by conducting them remotely. Chain Reaction interviewed Jake Chervinsky, the chief policy officer at Blockchain Association, a nonprofit organization focused on promoting “pro-innovation” policy for the digital asset world. On The TechCrunch Podcast — which, like WiR, covers the week in tech news — Devin talked about whether Meta’s cavalier approach to compliance might finally be coming to a close. And last but not least, TechCrunch Live profiled Sam Chaudhary, the founder of ClassDojo, and Chris Farmer, the founder and CEO of SignalFire, about playing the long game', metadata={'source': '/content/new_articles/05-06-amazon-launches-free-channels-check-marks-come-to-gmail-and-openai-raises-more-moolah.txt'}),\n",
       " Document(page_content='be coming to a close. And last but not least, TechCrunch Live profiled Sam Chaudhary, the founder of ClassDojo, and Chris Farmer, the founder and CEO of SignalFire, about playing the long game in edtech, investing in companies that aren’t rushing to monetize and the “outsider advantage.”', metadata={'source': '/content/new_articles/05-06-amazon-launches-free-channels-check-marks-come-to-gmail-and-openai-raises-more-moolah.txt'}),\n",
       " Document(page_content='TechCrunch+\\n\\nTC+ subscribers get access to in-depth commentary, analysis and surveys — which you know if you’re already a subscriber. If you’re not, consider signing up. Here are a few highlights from this week:\\n\\nA cloudy future: Lyft’s equity is selling off in the wake of the U.S. ride-hailing giant’s first-quarter results and its comments regarding the current quarter, and how its new strategic posture will affect its growth and economics in the coming quarters. But there’s not necessarily cause for panic. Alex and Anna write about Lyft’s new tack and the potential upsides, of which there are several.', metadata={'source': '/content/new_articles/05-06-amazon-launches-free-channels-check-marks-come-to-gmail-and-openai-raises-more-moolah.txt'}),\n",
       " Document(page_content='Down but not out: For the past year, everyone’s been predicting that the muted exit environment and bone-dry funding market would bring a reckoning for many late-stage companies. Down rounds carry a negative connotation and are often interpreted as the fault of the company or founder. But in a market where everything seems to be heading downward, they shouldn’t imply a company or its founders made a mistake — you often simply can’t help it, Rebecca writes.\\n\\nChatGPT, meet edtech: Shares of edtech company Chegg fell off a cliff this week even after the company reported Q1 results that bested analyst expectations. In its earnings call, the company’s executives noted that ChatGPT was slowing its ability to add new subscribers, not only potentially slowing growth but also throwing uncertainty into its ability to predict its future financial results. Alex and Natasha M dig deeper.', metadata={'source': '/content/new_articles/05-06-amazon-launches-free-channels-check-marks-come-to-gmail-and-openai-raises-more-moolah.txt'}),\n",
       " Document(page_content='Get your TechCrunch fix IRL. Join us at Disrupt 2023 in San Francisco this September to immerse yourself in all things startup. From headline interviews to intimate roundtables to a jam-packed startup expo floor, there’s something for everyone at Disrupt. Save up to $800 when you buy your pass now through May 15, and save 15% on top of that with promo code WIR. Learn more.', metadata={'source': '/content/new_articles/05-06-amazon-launches-free-channels-check-marks-come-to-gmail-and-openai-raises-more-moolah.txt'}),\n",
       " Document(page_content='ChatGPT: Everything you need to know about the AI-powered chatbot\\n\\nChatGPT, OpenAI’s text-generating AI chatbot, has taken the world by storm. It’s able to write essays, code and more given short text prompts, hyper-charging productivity. But it also has a more…nefarious side.\\n\\nIn any case, AI tools are not going away — and indeed has expanded dramatically since its launch just a few months ago. Major brands are experimenting with it, using the AI to generate ad and marketing copy, for example.\\n\\nAnd OpenAI is heavily investing in it. ChatGPT was recently super-charged by GPT-4, the latest language-writing model from OpenAI’s labs. Paying ChatGPT users have access to GPT-4, which can write more naturally and fluently than the model that previously powered ChatGPT. In addition to GPT-4, OpenAI recently connected ChatGPT to the internet with plugins available in alpha to users and developers on the waitlist.', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='Here’s a timeline of ChatGPT product updates and releases, starting with the latest, to be updated regularly. We also answer the most common FAQs (see below).\\n\\nTimeline of the most recent ChatGPT updates\\n\\nMay 3, 2023\\n\\nMeta said in a report on May 3 that malware posing as ChatGPT was on the rise across its platforms.The company said that since March 2023, its security teams have uncovered 10 malware families using ChatGPT (and similar themes) to deliver malicious software to users’ devices.\\n\\n“In one case, we’ve seen threat actors create malicious browser extensions available in official web stores that claim to offer ChatGPT-based tools,” said Meta security engineers Duc H. Nguyen and Ryan Victory in a blog post. “They would then promote these malicious extensions on social media and through sponsored search results to trick people into downloading malware.”\\n\\nApril 28, 2023', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='April 28, 2023\\n\\nVC firms including Sequoia Capital, Andreessen Horowitz, Thrive and K2 Global are picking up new shares, according to documents seen by TechCrunch. A source tells us Founders Fund is also investing. Altogether the VCs have put in just over $300 million at a valuation of $27 billion to $29 billion. This is separate to a big investment from Microsoft announced earlier this year, a person familiar with the development told TechCrunch, which closed in January. The size of Microsoft’s investment is believed to be around $10 billion, a figure we confirmed with our source.\\n\\nApril 25, 2023\\n\\nCalled ChatGPT Business, OpenAI describes the forthcoming offering as “for professionals who need more control over their data as well as enterprises seeking to manage their end users.”', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='Called ChatGPT Business, OpenAI describes the forthcoming offering as “for professionals who need more control over their data as well as enterprises seeking to manage their end users.”\\n\\n“ChatGPT Business will follow our API’s data usage policies, which means that end users’ data won’t be used to train our models by default,” OpenAI wrote in a blog post. “We plan to make ChatGPT Business available in the coming months.”\\n\\nApril 24, 2023\\n\\nOpenAI applied for a trademark for “GPT,” which stands for “Generative Pre-trained Transformer,” last December. Last month, the company petitioned the USPTO to speed up the process, citing the “myriad infringements and counterfeit apps” beginning to spring into existence.\\n\\nUnfortunately for OpenAI, its petition was dismissed last week. According to the agency, OpenAI’s attorneys neglected to pay an associated fee as well as provide “appropriate documentary evidence supporting the justification of special action.”', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='That means a decision could take up to five more months.\\n\\nApril 22, 2023\\n\\nAuto-GPT is an open source app created by game developer Toran Bruce Richards that uses OpenAI’s latest text-generating models, GPT-3.5 and GPT-4, to interact with software and services online, allowing it to “autonomously” perform tasks.\\n\\nDepending on what objective the tool’s provided, Auto-GPT can behave in very… unexpected ways. One Reddit user claims that, given a budget of $100 to spend within a server instance, Auto-GPT made a wiki page on cats, exploited a flaw in the instance to gain admin-level access and took over the Python environment in which it was running — and then “killed” itself.\\n\\nApril 18, 2023\\n\\nFTC chair Lina Khan and fellow commissioners warned House representatives of the potential for modern AI technologies, like ChatGPT, to be used to “turbocharge” fraud in a congressional hearing.', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='FTC chair Lina Khan and fellow commissioners warned House representatives of the potential for modern AI technologies, like ChatGPT, to be used to “turbocharge” fraud in a congressional hearing.\\n\\n“AI presents a whole set of opportunities, but also presents a whole set of risks,” Khan told the House representatives. “And I think we’ve already seen ways in which it could be used to turbocharge fraud and scams. We’ve been putting market participants on notice that instances in which AI tools are effectively being designed to deceive people can place them on the hook for FTC action,” she stated.\\n\\nApril 17, 2023', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='April 17, 2023\\n\\nThe company behind the popular iPhone customization app Brass, sticker maker StickerHub and others is out today with a new AI chat app called SuperChat, which allows iOS users to chat with virtual characters powered by OpenAI’s ChatGPT. However, what makes the app different from the default ChatGPT experience or the dozens of generic AI chat apps now available are the characters offered which you can use to engage with SuperChat’s AI features.\\n\\nApril 12, 2023\\n\\nItaly’s data protection watchdog has laid out what OpenAI needs to do for it to lift an order against ChatGPT issued at the end of last month — when it said it suspected the AI chatbot service was in breach of the EU’s GSPR and ordered the U.S.-based company to stop processing locals’ data.\\n\\nThe DPA has given OpenAI a deadline — of April 30 — to get the regulator’s compliance demands done. (The local radio, TV and internet awareness campaign has a slightly more generous timeline of May 15 to be actioned.)', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='April 12, 2023\\n\\nA study co-authored by scientists at the Allen Institute for AI shows that assigning ChatGPT a “persona” — for example, “a bad person,” “a horrible person” or “a nasty person” — through the ChatGPT API increases its toxicity sixfold. Even more concerning, the co-authors found having ChatGPT pose as certain historical figures, gendered people and members of political parties also increased its toxicity — with journalists, men and Republicans in particular causing the machine learning model to say more offensive things than it normally would.\\n\\nThe research was conducted using the latest version of ChatGPT, but not the model currently in preview based on OpenAI’s GPT-4.\\n\\nApril 4, 2023\\n\\nYC Demo Day’s Winter 2023 batch features no fewer than four startups that claim to be building “ChatGPT for X.” They’re all chasing after a customer service software market that’ll be worth $58.1 billion by 2023, assuming the rather optimistic prediction from Acumen Research comes true.', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='Here are the YC-backed startups that caught our eye:\\n\\nYuma, whose customer demographic is primarily Shopify merchants, provides ChatGPT-like AI systems that integrate with help desk software, suggesting drafts of replies to customer tickets.\\n\\nBaselit, which uses one of OpenAI’s text-understanding models to allow businesses to embed chatbot-style analytics for their customers.\\n\\nLasso customers send descriptions or videos of the processes they’d like to automate and the company combines ChatGPT-like interface with robotic process automation (RPA) and a Chrome extension to build out those automations.\\n\\nBerriAI, whose platform is designed to help developers spin up ChatGPT apps for their organization data through various data connectors.\\n\\nApril 1, 2023\\n\\nOpenAI has started geoblocking access to its generative AI chatbot, ChatGPT, in Italy.', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='April 1, 2023\\n\\nOpenAI has started geoblocking access to its generative AI chatbot, ChatGPT, in Italy.\\n\\nItaly’s data protection authority has just put out a timely reminder that some countries do have laws that already apply to cutting edge AI: it has ordered OpenAI to stop processing people’s data locally with immediate effect. The Italian DPA said it’s concerned that the ChatGPT maker is breaching the European Union’s General Data Protection Regulation (GDPR), and is opening an investigation.\\n\\nMarch 29, 2023\\n\\nThe letter’s signatories include Elon Musk, Steve Wozniak and Tristan Harris of the Center for Humane Technology, among others. The letter calls on “all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4.”\\n\\nThe letter reads:', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='The letter reads:\\n\\nContemporary AI systems are now becoming human-competitive at general tasks,[3] and we must ask ourselves: Should we let machines flood our information channels with propaganda and untruth? Should we automate away all the jobs, including the fulfilling ones? Should we develop nonhuman minds that might eventually outnumber, outsmart, obsolete and replace us? Should we risk loss of control of our civilization? Such decisions must not be delegated to unelected tech leaders. Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable.\\n\\nMarch 23, 2023', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='March 23, 2023\\n\\nOpenAI launched plugins for ChatGPT, extending the bots functionality by granting it access to third-party knowledge sources and databases, including the web. Available in alpha to ChatGPT users and developers on the waitlist, OpenAI says that it’ll initially prioritize a small number of developers and subscribers to its premium ChatGPT Plus plan before rolling out larger-scale and API access.\\n\\nMarch 14, 2023\\n\\nGPT-4 is a powerful image- and text-understanding AI model from OpenAI. Released March 14, GPT-4 is available for paying ChatGPT Plus users and through a public API. Developers can sign up on a waitlist to access the API.\\n\\nMarch 9, 2023\\n\\nChatGPT is generally available through the Azure OpenAI Service, Microsoft’s fully managed, corporate-focused offering. Customers, who must already be “Microsoft managed customers and partners,” can apply here for special access.\\n\\nMarch 1, 2023', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='March 1, 2023\\n\\nOpenAI makes another move toward monetization by launching a paid API for ChatGPT. Instacart, Snap (Snapchat’s parent company) and Quizlet are among its initial customers.\\n\\nFebruary 7, 2023\\n\\nAt a press event in Redmond, Washington, Microsoft announced its long-rumored integration of OpenAI’s GPT-4 model into Bing, providing a ChatGPT-like experience within the search engine. The announcement spurred a 10x increase in new downloads for Bing globally, indicating a sizable consumer demand for new AI experiences.\\n\\nOther companies beyond Microsoft joined in on the AI craze by implementing ChatGPT, including OkCupid, Kaito, Snapchat and Discord — putting the pressure on Big Tech’s AI initiatives, like Google.\\n\\nFebruary 1, 2023\\n\\nAfter ChatGPT took the internet by storm, OpenAI launched a new pilot subscription plan for ChatGPT called ChatGPT Plus, aiming to monetize the technology starting at $20 per month.\\n\\nDecember 8, 2022', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='December 8, 2022\\n\\nA week after ChatGPT was released into the wild, two developers — Steven Tey and Dom Eccleston — made a Chrome extension called ShareGPT to make it easier to capture and share the AI’s answers with the world.\\n\\nNovember 30, 2022\\n\\nGPT-3.5 broke cover with ChatGPT, a fine-tuned version of GPT-3.5 that’s essentially a general-purpose chatbot. ChatGPT can engage with a range of topics, including programming, TV scripts and scientific concepts.\\n\\nWriters everywhere rolled their eyes at the new technology, much like artists did with OpenAI’s DALL-E model, but the latest chat-style iteration seemingly broadened its appeal and audience.\\n\\nFAQs:\\n\\nWhat is ChatGPT? How does it work?\\n\\nChatGPT is a general-purpose chatbot that uses artificial intelligence to generate text after a user enters a prompt, developed by tech startup OpenAI. The chatbot uses GPT-4, a large language model that uses deep learning to produce human-like text.\\n\\nWhen did ChatGPT get released?', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='When did ChatGPT get released?\\n\\nNovember 30, 2022 is when ChatGPT was released for public use.\\n\\nWhat is the latest version of ChatGPT?\\n\\nBoth the free version of ChatGPT and the paid ChatGPT Plus are regularly updated with new GPT models. The most recent model is GPT-4.\\n\\nIs ChatGPT free?\\n\\nThere is a free version of ChatGPT that only requires a sign-in in addition to the paid version, ChatGPT Plus.\\n\\nWho uses ChatGPT?\\n\\nAnyone can use ChatGPT! More and more tech companies and search engines are utilizing the chatbot to automate text or quickly answer user questions/concerns.\\n\\nWhat is the difference between ChatGPT and a chatbot?\\n\\nA chatbot can be any software/system that holds dialogue with you/a person but doesn’t necessarily have to be AI-powered. For example, there are chatbots that are rules-based in the sense that they’ll give canned responses to questions.\\n\\nChatGPT is AI-powered and utilizes LLM technology to generate text after a prompt.\\n\\nCan ChatGPT write essays?\\n\\nYes.', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='ChatGPT is AI-powered and utilizes LLM technology to generate text after a prompt.\\n\\nCan ChatGPT write essays?\\n\\nYes.\\n\\nCan ChatGPT commit libel?\\n\\nDue to the nature of how these models work, they don’t know or care whether something is true, only that it looks true. That’s a problem when you’re using it to do your homework, sure, but when it accuses you of a crime you didn’t commit, that may well at this point be libel.\\n\\nWe will see how handling troubling statements produced by ChatGPT will play out over the next few months as tech and legal experts attempt to tackle the fastest moving target in the industry.\\n\\nDoes ChatGPT have an app?\\n\\nThere is not an app available for iPhone or Android, but users have options to enable the chatbot on their mobile devices via their browser or a third-party app that uses ChatGPT’s public API.\\n\\nWhat is the ChatGPT character limit?', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='What is the ChatGPT character limit?\\n\\nIt’s not documented anywhere that ChatGPT has a character limit. However, users have noted that there are some character limitations after around 500 words.\\n\\nDoes ChatGPT have an API?\\n\\nYes, it was released March 1, 2023.\\n\\nWhat are some sample everyday uses for ChatGPT?\\n\\nEveryday examples include programing, scripts, email replies, listicles, blog ideas, summarization, etc.\\n\\nWhat are some advanced uses for ChatGPT?\\n\\nAdvanced use examples include debugging code, programming languages, scientific concepts, complex problem solving, etc.\\n\\nHow good is ChatGPT at writing code?\\n\\nIt depends on the nature of the program. While ChatGPT can write workable Python code, it can’t necessarily program an entire app’s worth of code. That’s because ChatGPT lacks context awareness — in other words, the generated code isn’t always appropriate for the specific context in which it’s being used.\\n\\nCan you save a ChatGPT chat?', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='Can you save a ChatGPT chat?\\n\\nYes. OpenAI allows users to save chats in the ChatGPT interface, stored in the sidebar of the screen. There are no built-in sharing features yet.\\n\\nAre there alternatives to ChatGPT?\\n\\nYes. There are multiple AI-powered chatbot competitors such as Together, Google’s Bard and Anthropic’s Claude, and developers are creating open source alternatives. But the latter are harder — if not impossible — to run today.\\n\\nHow does ChatGPT handle data privacy?\\n\\nOpenAI has said that individuals in “certain jurisdictions” (such as the EU) can object to the processing of their personal information by its AI models by filling out this form. This includes the ability to make requests for deletion of AI-generated references about you. Although OpenAI notes it may not grant every request since it must balance privacy requests against freedom of expression “in accordance with applicable laws”.', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='The web form for making a deletion of data about you request is entitled “OpenAI Personal Data Removal Request”.\\n\\nIn its privacy policy, the ChatGPT maker makes a passing acknowledgement of the objection requirements attached to relying on “legitimate interest” (LI), pointing users towards more information about requesting an opt out — when it writes: “See here for instructions on how you can opt out of our use of your information to train our models.”\\n\\nWhat controversies have surrounded ChatGPT?\\n\\nRecently, Discord announced that it had integrated OpenAI’s technology into its bot named Clyde where two users tricked Clyde into providing them with instructions for making the illegal drug methamphetamine (meth) and the incendiary mixture napalm.\\n\\nAn Australian mayor has publicly announced he may sue OpenAI for defamation due to ChatGPT’s false claims that he had served time in prison for bribery. This would be the first defamation lawsuit against the text-generating service.', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='CNET found itself in the midst of controversy after Futurism reported the publication was publishing articles under a mysterious byline completely generated by AI. The private equity company that owns CNET, Red Ventures, was accused of using ChatGPT for SEO farming, even if the information was incorrect.\\n\\nSeveral major school systems and colleges, including New York City Public Schools, have banned ChatGPT from their networks and devices. They claim that the AI impedes the learning process by promoting plagiarism and misinformation, a claim that not every educator agrees with.\\n\\nThere have also been cases of ChatGPT accusing individuals of false crimes.\\n\\nWhere can I find examples of ChatGPT prompts?\\n\\nSeveral marketplaces host and provide ChatGPT prompts, either for free or for a nominal fee. One is PromptBase. Another is ChatX. More launch every day.\\n\\nCan ChatGPT be detected?\\n\\nPoorly. Several tools claim to detect ChatGPT-generated text, but in our tests, they’re inconsistent at best.', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='Can ChatGPT be detected?\\n\\nPoorly. Several tools claim to detect ChatGPT-generated text, but in our tests, they’re inconsistent at best.\\n\\nAre ChatGPT chats public?\\n\\nNo. But OpenAI recently disclosed a bug, since fixed, that exposed the titles of some users’ conversations to other people on the service.\\n\\nWho owns the copyright on ChatGPT-created content or media?\\n\\nThe user who requested the input from ChatGPT is the copyright owner.\\n\\nWhat lawsuits are there surrounding ChatGPT?\\n\\nNone specifically targeting ChatGPT. But OpenAI is involved in at least one lawsuit that has implications for AI systems trained on publicly available data, which would touch on ChatGPT.\\n\\nAre there issues regarding plagiarism with ChatGPT?\\n\\nYes. Text-generating AI models like ChatGPT have a tendency to regurgitate content from their training data.', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content=\"Vint Cerf on the ‘exhilarating mix’ of thrill and hazard at the frontiers of tech 'That's always an exciting place to be — a place where nobody's ever been before.'\\n\\nVint Cerf has been a near-constant influence on the internet since the days when he was helping create it in the first place. Today he wears many hats, among them VP and chief internet evangelist at Google. He is to be awarded the IEEE’s Medal of Honor at a gala in Atlanta, and ahead of the occasion he spoke with TechCrunch in a wide-ranging interview touching on his work, AI, accessibility and interplanetary internet.\\n\\nTechCrunch: To start out with, can you tell us how Google has changed in your time there?\", metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='TechCrunch: To start out with, can you tell us how Google has changed in your time there?\\n\\nCerf: Well, when I joined the company in 2005, there were 5,000 people already, which is pretty damn big. And of course, my normal attire is three piece suits. The important thing is that I thought I would be raising the sartorial quotient of the company by joining. And now, almost 18 years later, there are 170-some-odd thousand people, and I have failed miserably. So I hope you don’t mind if I take my jacket off.\\n\\nGo right ahead.', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='Go right ahead.\\n\\nSo as you might have noticed, Sergey has come back to do a little bit more on the artificial intelligence side of things, which is something he’s always been interested in; I would say historically, we’ve always had an interest in artificial intelligence. But that has escalated significantly over the past decade or so. The acquisition of DeepMind was a brilliant choice. And you can see some of the outcomes first of the spectacular stuff, like playing Go and winning. And then the more productive stuff, like figuring out how 200 million proteins are folded up.', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='Then there’s the large language models and the chatbots. And I think we’re still in a very peculiar period of time, where we’re trying to characterize what these things can and can’t do, and how they go off the rails, and how do you take advantage of them to do useful work? How do we get them to distinguish fact from fiction? All of that is in my view open territory, but then that’s always an exciting place to be — a place where nobody’s ever been before. The thrill of discovery and the risk of hazard create a fairly exciting mix — an exhilarating mix.\\n\\nYou gave a talk recently about, I don’t want to say the dangers of the large language models, but…\\n\\nWell, I did say there are hazards there. I was talking to a bunch of investment bankers, or VCs, and I said, you know, don’t try to sell stuff to your investors just because it’s flashy and shiny. Be cautious about going too fast and trying to apply it without figuring out how to put guardrails in place.', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='I raised a question of hazard and wanting people to be more thoughtful about which applications made sense. I even suggested an analogy: you know how the Society of Automotive Engineers, they have different risk levels for the self driving cars — a risk level idea could apply to artificial intelligence and machine learning.\\n\\nFor entertainment purposes, perhaps it’s not too concerning, unless it goes down some dark path, in which case, you might want to put some friction into the system to deal with that, especially a younger user. But then, as you get to the point where you’re training these things to do medical diagnosis or make investment advice, or make decisions about whether somebody gets out of jail… now suddenly, the risk factors are extremely high.\\n\\nWe shouldn’t be unaware of those risk factors. We can, as we build applications, be prepared to detect excursions away from safe territory, so that we don’t accidentally inflict some harm by the use of these kinds of technologies.', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='So we need some kind of guardrails.\\n\\nAgain, I’m not expert in this space, but I am beginning to wonder whether we need something kind of like that in order to provide a “super-ego” for the natural language network. So when it starts to go off the rails somewhere, we can observe that that’s happening. And a second network that’s observing both the input and the output might intervene, somehow, and stop the the production of the output.\\n\\nSort of a conscience function?\\n\\nWell, it’s not quite conscience, it’s closer to executive function — the prefrontal cortex. I want to be careful, I’m only reasoning by metaphor here.\\n\\nI know that Microsoft has embarked on something like this. Their version of GPT-4 has an intermediary model like that, they call it Prometheus.', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='I know that Microsoft has embarked on something like this. Their version of GPT-4 has an intermediary model like that, they call it Prometheus.\\n\\nPurely as an observation, I had the impression that the Prometheus natural language model would detect and intervene if it thought that the interactions were going down with dark path. I thought that they would implement it in such a way that before you actually say something to the interlocutor that is going down the dark path, you intervene and prevent it from going there at all.\\n\\nMy impression, though, is that it actually produces the output and then discovers that it’s produced it, but and then it says, “Oh, I shouldn’t have done that. Oh, dear, I take that back,” or “I don’t want to talk to you anymore about that.” It’s a little bit like the email that you get occasionally from the Microsoft Outlook system that says, “This person would like to withdraw the message.”', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='I love when that happens… it makes me want to read the original message so badly, even if I wouldn’t have before.\\n\\nYeah, exactly. It’s sort of like putting a big red flag in there saying, boy there’s something juicy in here.\\n\\nYou mentioned the AI models, that it’s an interesting place to work. Do you get the same sort of foundational flavor that you got from working on protocols and other big shared things over the years?\\n\\nWell, what we are seeing is emergent properties of these large language models, that are not necessarily anticipated. And there have been emergent properties showing up in the protocol world. Flow control in particular is a vast headache in the online packet switch environment, and people have been tackling these problems inside and outside of Google for years.', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='One of the examples of emergent properties that I think very few of us thought about is the domain name business. Once they had value, suddenly, all kinds of emergent properties show up, people with interests that conflict and have to be resolved. Same for internet address space, it’s an even more weird environment where people actually buy IPv4 addresses for like $50 each.\\n\\nI confess to you that as I watched the auctions for IPv4 address space, I was thinking how stupid I was. When I was at the Defense Department in charge of all this, I should have allocated the slash eight, which is 16 million addresses, to myself, and just sit on it, you know, for 50 years, then sell it and retire.', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='Even simple systems have the ability to surprise you. Especially when you have simple systems when a large number of them are interacting with each other. I’ve found myself not necessarily recognizing when these emergent properties will come, but I will say that whenever something gets monetized, you should anticipate there will be emergent properties and possibly unexpected behavior, all driven by greed.\\n\\nLet me ask you about some some other stuff you’re working on. I’m always happy when I see cutting-edge tech being applied to people who need it, people with disabilities, people who like just have not been addressed by the current use cases of tech. Are you still working in the accessibility community?', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='I am very active in the accessibility space. At Google, we have a number of what we call employee resource groups, or ERGs. Yeah, some of them I, executive sponsor for one for Googlers who have hearing problems. And there is a disabilities oriented group, which involves employees who either have disabilities or family members that have disabilities, and they share their stories with each other because often people have similar problems, but don’t know what the solutions were for other people. Also, it’s just nice to know that you’re not alone in some of these challenges. There’s another group called the Grayglers for people that have a little gray in their hair, and I’m the executive sponsor for that. And of course, the focus of attention there is the challenges that arise as you get older, even as you think about retirement and things like that.', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='When a lot of so-called Web 2.0 stuff came out 10 years ago, it was totally inaccessible, broke all the screen readers, all this kind of stuff. Somebody has to step in and say, look, we need to have this standard, or else you’re leaving out millions of people. So I’m always interested to hear about what interesting projects or organizations or people are out there.\\n\\nWhat I have come to believe is that engineers, being just given a set of specs that say if you do it this way, it will meet this level of the standard… that doesn’t necessarily produce intuition. You really have to have some intuition in order to make things accessible.', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='So I’ve come to the conclusion that what we really need is to show people examples of something which is not accessible, and something that is, and let them ingest as many examples as we can give them, because their neural networks will eventually figure out, what is it about this design that makes it accessible? And how do I apply that insight into the next design that I do? So, seeing what works and what doesn’t work is really important. And you often learn a lot more from what doesn’t work than you do from what does.', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='There’s a guy named Gregg Vanderheiden, who’s at the University of Maryland, he and I did a two-day event [the Future of Interface Workshop] looking at research on accessibility and trying to frame what this is going to look like over the next 10 or 20 years. It really is quite astonishing what the technology might be able to do to act as an augmenting capability for people that that need assistance. There’s great excitement, but at the same time great disappointment, because we haven’t used it as effectively as I think we could have. It’s kind of like how Alexander Graham Bell invented a telephone that can’t be used by people who are deaf, which is why he was working on it in the first place.', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='It is a funny contradiction of priorities. One thing where I do see some of the the large language and multimodal AI models helping out is that they can describe what they are seeing, even if you can’t see it. I know that one of GPT-4’s first applications was in an application for blind people to view the world around them.\\n\\nWe’re experiencing something close to that right this minute. Since I wear hearing aids, I’m making use of the captioning capability. And at the moment since this is Zoom rather than a Google Meet, there isn’t any setting on this one for closed captioning. I’m exercising the Zoom application through the Chrome browser, and Google has developed a capability for the Chrome browser to detect speech in the incoming sound.', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='So packets are coming in and they’re known to be sound, it passes through an identification system that produces a caption bar, which you can move around on the screen. And that’s been super helpful for me. For cases like this, where the application doesn’t have captioning, or for random video streaming video that might be coming in and hasn’t been captioned, the caption window automatically pops up. In theory, I think we can do this in 100 different languages, although I don’t know that we’ve activated it for more than four or five. As you say, these tools will become more and more normal, and as time goes on, people will expect the system to adapt to their needs.', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='So language translation, and speech recognition is quite powerful, but I do want to mention something that I found vaguely unsettling. Recently, I encountered an example of a conversation between a reporter and a chatbot. But he chose deliberately to take the output of the chat bot and have it spoken by the system. And he chose the style of a famous British explorer [David Attenborough].\\n\\nThe text itself was quite well formed, but coming with Attenborough’s accent just added to the weight of the assertions even when they were wrong. The confidence levels, as I’m sure you’ve seen, are very high, even when the thing doesn’t know what it’s talking about.', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='The reason I bring this up is that we are allowing in these indicators of, how should we say this, of quality, to fool us. Because in the past, they really did mean it was David Attenborough. But here it’s not, it’s just his voice. I got to thinking about this, and I realized there was an ancient example of exactly this problem that showed up 50 years ago at Xerox PARC.', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='They had a laser printer, and they had the Alto workstation, and the Bravo text editor, it meant the first draft of anything you type to be printed out beautifully formatted with lovely forms and everything else. Normally, you would never see that production quality until after everything had been edited, you know, wrestled with by everybody to get the text formatted, picture-perfect stuff. That meant the first draft stuff came out looking like it was final draft. People didn’t didn’t understand that they were nuts, that they were seeing first-round stuff, and that it wasn’t complete, or necessarily even satisfactory.\\n\\nSo it occurred to me that we’ve reached a point now where technology is fooling us into giving it more weight than it deserves, because of certain indicia that used to be indicative of the investment made in producing it. And… I’m not quite sure what to do about that.\\n\\nI don’t think anyone is!', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='I don’t think anyone is!\\n\\nI think somehow or another, we need to make it clear what the provenance is of the thing that we’re looking at. Like how we needed to say this is first-draft material, you know, don’t make any assumptions. So provenance turns out to be a very important concept, especially in a world where we have the ability to imbue content with attributes that we would normally interpret in one way. Like, it’s David Attenborough speaking, and we should listen to that. And yet, which have to be, we have to think more critically about them. Because in fact, the attribute is being delivered artificially.\\n\\nAnd perhaps maliciously.', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='And perhaps maliciously.\\n\\nCertainly that too. And this is why critical thinking has become an important skill. But it doesn’t work very well, unless you have enough information to understand the provenance of the material that you’re looking at. I think we are going to have to invest more in provenance and identity in order to evaluate the quality of that which we are experiencing.\\n\\nI wanted to ask you about interplanetary internet, because that whole area is extremely interesting to me.\\n\\nWell, this one, of course, gets started way back in 1998. But I’m a science fiction reader from way back way to age 10 or something, so I got quite excited when it was possible to even think about the possibility of designing and building a communication system that would span the solar system.', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='The team got started very small, and now 25 years later involves many of the space agencies around the world: JAXA, the Korean Space Agency, NASA and so on. And a growing team of people who are either government funded to do space-based research, or volunteers. There’s a special interest group called the interplanetary networking Special Interest Group, which is part of the Internet Society — that thing got started in 1998. But it has now grown to like 900 people around the world who are interested in this stuff.\\n\\nWe’ve standardized this stuff, we’re on version seven of it, we’re running it up in the International Space Station. It’s intended to be available for the return to the moon and Artemis missions. I’m not going to see the end result of all this, but I’m going to see the first couple of chapters. And I’m very excited about that, because it’s not crazy to actually think about. Like all my other projects, it takes a long time. Patience and persistence!', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='For something like this it must have been a real challenge, but also a very familiar one. In some ways building something like this is what you’ve been doing your whole career. This is just a different set of restraints and capabilities.\\n\\nYou put your finger on it, exactly right. This is in a different parametric space than the one that works for TCP/IP. And we’re still bumping into some really interesting problems, especially where you have TCP/IP networks running on the moon, for example, locally and interconnecting with other internets on other planets, going through the interplanetary protocol. What does that look like? You know, which IP addresses should be used? We have to figure out, well, how the hell does the Domain Name System work in the context of internets that aren’t on the planet? And it’s really fun!', metadata={'source': '/content/new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt'}),\n",
       " Document(page_content='Partners of 3one4 Capital, a venture capital firm in India, recently went on a road show to raise a new fund. Within two and a half months, at the height of the worsening global economy, they had secured $200 million. It’s the fourth marquee fund for the Bengaluru-headquartered fund, whose portfolio includes four unicorn startups.\\n\\nThe fund, sixth overall for 3one4 Capital, was oversubscribed to $250 million but the firm is accepting only $200 million to keep itself lean and disciplined, said Pranav Pai, co-founder and partner at 3one4 Capital. The firm’s decision to limit the fund size is emblematic of its strategic choices, which have set it apart from other Indian venture firms.', metadata={'source': '/content/new_articles/05-07-3one4-capital-driven-by-contrarian-bets-raises-200-million-new-fund.txt'}),\n",
       " Document(page_content='“We are known to give good returns. Our performance has been benchmarked among the best leading performing funds in the space. So we asked ourselves the hard questions, can we continue our performance with a larger fund size? Do we even need that much capital for the early-stage?” said Pai in an interview with TechCrunch.\\n\\nIn recent years, a surge of venture capital firms in India have raised unprecedentedly large funds, sparking concerns about the responsible allocation of this capital, particularly for early-stage startups. Critics question whether there are enough viable companies in the Indian market to absorb and effectively utilize such significant investments.', metadata={'source': '/content/new_articles/05-07-3one4-capital-driven-by-contrarian-bets-raises-200-million-new-fund.txt'}),\n",
       " Document(page_content='Pai, pictured above, asserts that there is ample room for more Indian companies to pursue IPOs, as the nation’s IPO market has proven successful and well-regulated for institutional investors. He anticipates a transformation in India’s stock index, with an increasing number of tech companies, apps, services, fintech, and payment solutions becoming part of the index.\\n\\nDespite this, Pai acknowledges that the Indian market has yet to fully realize its potential for mergers and acquisitions. Although there has been growth in M&A activity—increasing three to four times in the past five years—it remains below expectations. For the Indian market to flourish, Pai emphasizes the need for a more robust M&A landscape.', metadata={'source': '/content/new_articles/05-07-3one4-capital-driven-by-contrarian-bets-raises-200-million-new-fund.txt'}),\n",
       " Document(page_content='Over the last half-decade, numerous Indian venture firms have shifted their attention to early-stage investments. Despite this increased focus, the market continues to depend on international investors to support mid- and growth-stage deals, highlighting the need for further growth in India’s venture capital ecosystem. “We have high performing mutual funds and PEs. We hope that more of these firms will launch dedicated funds for Indian startups,” he said.\\n\\nHalf of the capital in the new fund for 3one4 has come from Indian investors, another aspect that differentiates the firm from many of its peers. All the systemically important Indian banks, and the top five local banks by market cap overall have invested in the new fund. Eight of the top 10 mutual fund operators are also LPs in the new fund, said Pai. “We are also proud to have leading global endowments, sovereigns and insurance companies as LPs,” he said.', metadata={'source': '/content/new_articles/05-07-3one4-capital-driven-by-contrarian-bets-raises-200-million-new-fund.txt'}),\n",
       " Document(page_content='“We want to be India’s leading homegrown venture capital firm. We are based here, we invest here – we don’t want to invest in Southeast Asia – and our fund size and strategy are aligned with opportunities in India. As our companies have IPO-ed over the years, we have seen the importance of having India’s largest institutions working with us to help build those companies. It would be difficult if we didn’t have banks to help our companies from everything from revenue collection to payrolls. And mutual funds are buyers, book runners and market makers for IPOs and them buying the stock gives a vote of confidence to the market,” he said.', metadata={'source': '/content/new_articles/05-07-3one4-capital-driven-by-contrarian-bets-raises-200-million-new-fund.txt'}),\n",
       " Document(page_content='3one4, which focuses largely on early-stage and in sectors including direct-to-consumer tech, media and content, fintech, deep technology and SaaS and enterprise automation, today manages about $750 million in AUM and its portfolio includes HR platform Darwinbox, business-to-business focused neobank Open, consumer-focused neobank Jupiter, Licious, a direct-to-consumer brand that sells meat, local social networks Koo and Lokal, entertainment service Kuku FM, fintech Raise Financial, and gaming firm Loco.\\n\\n3one4 Capital has gained a reputation for its contrarian investment approach, as exemplified by its early investment in Licious. Over five years ago, the prevailing opinion held that India’s price-sensitive market would not pay a premium for online meat delivery. However, Licious has since grown into one of South Asia’s largest direct-to-consumer brands, with a presence in approximately two dozen cities across India.', metadata={'source': '/content/new_articles/05-07-3one4-capital-driven-by-contrarian-bets-raises-200-million-new-fund.txt'}),\n",
       " Document(page_content='Another example of 3one4’s daring investments is Darwinbox, a bet made at a time when most investors doubted the ability of Indian SaaS companies to expand internationally or garner sufficient local business subscriptions.\\n\\n3one4 Capital’s contrarian approach extends to the investments it has deliberately avoided as well. In 2021, amidst a frenzy of investment activity in the crypto space, nearly every fund in India sought opportunities and backed crypto startups. However, 3one4 Capital, after thorough evaluation of the sector, chose not to make any investments in crypto.', metadata={'source': '/content/new_articles/05-07-3one4-capital-driven-by-contrarian-bets-raises-200-million-new-fund.txt'}),\n",
       " Document(page_content='The firm, which employs 28 people, is also focusing on setting new standards in transparency and governance for itself. It’s the first VC to be a signatory to UN PRI, it said. “We have to report, behave, act and look a certain way. We have to look like the fiduciary of best institutions in the world, and then and only then we quality to tell our portfolio founders that this is how we want to create best in class companies with you,” said Pai.', metadata={'source': '/content/new_articles/05-07-3one4-capital-driven-by-contrarian-bets-raises-200-million-new-fund.txt'}),\n",
       " Document(page_content='Databricks today announced that it has acquired Okera, a data governance platform with a focus on AI. The two companies did not disclose the purchase price. According to Crunchbase, Okera previously raised just under $30 million. Investors include Felicis, Bessemer Venture Partners, Cyber Mentor Fund, ClearSky and Emergent Ventures.', metadata={'source': '/content/new_articles/05-03-databricks-acquires-ai-centric-data-governance-platform-okera.txt'}),\n",
       " Document(page_content='Data governance was already a hot topic, but the recent focus on AI has highlighted some of the shortcomings of the previous approach to it, Databricks notes in today’s announcement. “Historically, data governance technologies, regardless of sophistication, rely on enforcing control at some narrow waist layer and require workloads to fit into the ‘walled garden’ at this layer,” the company explains in a blog post. That approach doesn’t work anymore in the age of large language models (LLMs) because the number of assets is growing too quickly (in part because so much of it is machine-generated) and because the overall AI landscape is changing so quickly, standard access controls aren’t able to capture these changes quickly enough.\\n\\nOkera then uses an AI-powered system that can automatically discover and classify personally identifiable information, tag it and apply rules to this (with a focus on the metadata), using a no-code interface.', metadata={'source': '/content/new_articles/05-03-databricks-acquires-ai-centric-data-governance-platform-okera.txt'}),\n",
       " Document(page_content='As the Databricks team stressed, that’s one of the reasons the company was interested in acquiring Okera, but the other is the service’s isolation technology, which can enforce governance control on arbitrary workloads without any major overhead. This technology is still in private preview but was likely one of the major reasons Databricks acquired the company.\\n\\nDatabricks, which launched its own LLM a few weeks ago, plans to integrate Okera’s technology into its Unity Catalog, its existing governance solution of data and AI assets. The company also noted that the acquisition will enable Databricks to expose additional APIs that its own data governance partners will be able to use to provide solutions to their customers.', metadata={'source': '/content/new_articles/05-03-databricks-acquires-ai-centric-data-governance-platform-okera.txt'}),\n",
       " Document(page_content='With this acquisition, Databricks is also bringing Okera co-founder and CEO Nong Li on board. Li created the Apache Parquet data storage format and was actually briefly an engineer at Databricks between working at Cloudera and before starting Okera, where he was the founding CTO and became the CEO in February 2022.', metadata={'source': '/content/new_articles/05-03-databricks-acquires-ai-centric-data-governance-platform-okera.txt'}),\n",
       " Document(page_content='“As data continues to grow in volume, velocity, and variety across different applications, CIOs, CDOs, and CEOs across the board have to balance those two often conflicting initiatives – not to mention that historically, managing access policies across multiple clouds has been painful and time-consuming,” writes Li in today’s announcement. “Many organizations don’t have enough technical talent to manage access policies at scale, especially with the explosion of LLMs. What they need is a modern, AI-centric governance solution. We could not be more excited to join the Databricks team and to bring our expertise in building secure, scalable and simple governance solutions for some of the world’s most forward-thinking enterprises.”\\n\\nIf you know more about this acquisition, you can contact Frederic on Signal at (860) 208-3416 or by email (frederic@techcrunch.com). You can also reach us via SecureDrop.', metadata={'source': '/content/new_articles/05-03-databricks-acquires-ai-centric-data-governance-platform-okera.txt'}),\n",
       " Document(page_content='AI startup Hugging Face and ServiceNow Research, ServiceNow’s R&D division, have released StarCoder, a free alternative to code-generating AI systems along the lines of GitHub’s Copilot.\\n\\nCode-generating systems like DeepMind’s AlphaCode; Amazon’s CodeWhisperer; and OpenAI’s Codex, which powers Copilot, provide a tantalizing glimpse at what’s possible with AI within the realm of computer programming. Assuming the ethical, technical and legal issues are someday ironed out (and AI-powered coding tools don’t cause more bugs and security exploits than they solve), they could cut development costs substantially while allowing coders to focus on more creative tasks.', metadata={'source': '/content/new_articles/05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}),\n",
       " Document(page_content='According to a study from the University of Cambridge, at least half of developers’ efforts are spent debugging and not actively programming, which costs the software industry an estimated $312 billion per year. But so far, only a handful of code-generating AI systems have been made freely available to the public — reflecting the commercial incentives of the organizations building them (see: Replit).\\n\\nStarCoder, which by contrast is licensed to allow for royalty-free use by anyone, including corporations, was trained on over 80 programming languages as well as text from GitHub repositories, including documentation and programming notebooks. StarCoder integrates with Microsoft’s Visual Studio Code code editor and, like OpenAI’s ChatGPT, can follow basic instructions (e.g., “create an app UI”) and answer questions about code.', metadata={'source': '/content/new_articles/05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}),\n",
       " Document(page_content='Congratulations to all the @BigCodeProject contributors that worked tirelessly over the last 6+ months to bring the vision of releasing a responsibly developed 15B parameter Code LLM to fruition. We cannot thank you enough for the collaboration & contributions to the community. https://t.co/282sCRJq3k — ServiceNow Research (@ServiceNowRSRCH) May 4, 2023\\n\\nLeandro von Werra, a machine learning engineer at Hugging Face and a co-lead on StarCoder, claims that StarCoder matches or outperforms the AI model from OpenAI that was used to power initial versions of Copilot.', metadata={'source': '/content/new_articles/05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}),\n",
       " Document(page_content='“One thing we learned from releases such as Stable Diffusion last year is the creativity and capability of the open-source community,” von Werra told TechCrunch in an email interview. “Within weeks of the release the community had built dozens of variants of the model as well as custom applications. Releasing a powerful code generation model allows anybody to fine-tune and adapt it to their own use-cases and will enable countless downstream applications.”\\n\\nBuilding a model\\n\\nStarCoder is a part of Hugging Face’s and ServiceNow’s over-600-person BigCode project, launched late last year, which aims to develop “state-of-the-art” AI systems for code in an “open and responsible” way. Hugging Face supplied an in-house compute cluster of 512 Nvidia V100 GPUs to train the StarCoder model.', metadata={'source': '/content/new_articles/05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}),\n",
       " Document(page_content='Various BigCode working groups focus on subtopics like collecting datasets, implementing methods for training code models, developing an evaluation suite and discussing ethical best practices. For example, the Legal, Ethics and Governance working group explored questions on data licensing, attribution of generated code to original code, the redaction of personally identifiable information (PII), and the risks of outputting malicious code.\\n\\nInspired by Hugging Face’s previous efforts to open source sophisticated text-generating systems, BigCode seeks to address some of the controversies arising around the practice of AI-powered code generation. The nonprofit Software Freedom Conservancy among others has criticized GitHub and OpenAI for using public source code, not all of which is under a permissive license, to train and monetize Codex. Codex is available through OpenAI’s and Microsoft’s paid APIs, while GitHub recently began charging for access to Copilot.', metadata={'source': '/content/new_articles/05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}),\n",
       " Document(page_content='For their parts, GitHub and OpenAI assert that Codex and Copilot — protected by the doctrine of fair use, at least in the U.S. — don’t run afoul of any licensing agreements.\\n\\n“Releasing a capable code-generating system can serve as a research platform for institutions that are interested in the topic but don’t have the necessary resources or know-how to train such models,” von Werra said. “We believe that in the long run this leads to fruitful research on safety, capabilities and limits of code-generating systems.”\\n\\nUnlike Copilot, the 15-billion-parameter StarCoder was trained over the course of several days on an open source dataset called The Stack, which has over 19 million curated, permissively licensed repositories and more than six terabytes of code in over 350 programming languages. In machine learning, parameters are the parts of an AI system learned from historical training data and essentially define the skill of the system on a problem, such as generating code.', metadata={'source': '/content/new_articles/05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}),\n",
       " Document(page_content='Because it’s permissively licensed, code from The Stack can be copied, modified and redistributed. But the BigCode project also provides a way for developers to “opt out” of The Stack, similar to efforts elsewhere to let artists remove their work from text-to-image AI training datasets.\\n\\nThe BigCode team also worked to remove PII from The Stack, such as names, usernames, email and IP addresses, and keys and passwords. They created a separate dataset of 12,000 files containing PII, which they plan to release to researchers through “gated access.”\\n\\nBeyond this, the BigCode team used Hugging Face’s malicious code detection tool to remove files from The Stack that might be considered “unsafe,” such as those with known exploits.', metadata={'source': '/content/new_articles/05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}),\n",
       " Document(page_content='Beyond this, the BigCode team used Hugging Face’s malicious code detection tool to remove files from The Stack that might be considered “unsafe,” such as those with known exploits.\\n\\nThe privacy and security issues with generative AI systems, which for the most part are trained on relatively unfiltered data from the web, are well-established. ChatGPT once volunteered a journalist’s phone number. And GitHub has acknowledged that Copilot may generate keys, credentials and passwords seen in its training data on novel strings.\\n\\n“Code poses some of the most sensitive intellectual property for most companies,” von Werra said. “In particular, sharing it outside their infrastructure poses immense challenges.”', metadata={'source': '/content/new_articles/05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}),\n",
       " Document(page_content='“Code poses some of the most sensitive intellectual property for most companies,” von Werra said. “In particular, sharing it outside their infrastructure poses immense challenges.”\\n\\nTo his point, some legal experts have argued that code-generating AI systems could put companies at risk if they were to unwittingly incorporate copyrighted or sensitive text from the tools into their production software. As Elaine Atwell notes in a piece on Kolide’s corporate blog, because systems like Copilot strip code of its licenses, it’s difficult to tell which code is permissible to deploy and which might have incompatible terms of use.\\n\\nIn response to the criticisms, GitHub added a toggle that lets customers prevent suggested code that matches public, potentially copyrighted content from GitHub from being shown. Amazon, following suit, has CodeWhisperer highlight and optionally filter the license associated with functions it suggests that bear a resemblance to snippets found in its training data.', metadata={'source': '/content/new_articles/05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}),\n",
       " Document(page_content='Commercial drivers\\n\\nSo what does ServiceNow, a company that deals mostly in enterprise automation software, get out of this? A “strong-performing model and a responsible AI model license that permits commercial use,” said Harm de Vries, the lead of the Large Language Model Lab at ServiceNow Research and the co-lead of the BigCode project.\\n\\nOne imagines that ServiceNow will eventually build StarCoder into its commercial products. The company wouldn’t reveal how much, in dollars, it’s invested in the BigCode project, save that the amount of donated compute was “substantial.”', metadata={'source': '/content/new_articles/05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}),\n",
       " Document(page_content='“The Large Language Models Lab at ServiceNow Research is building up expertise on the responsible development of generative AI models to ensure the safe and ethical deployment of these powerful models for our customers,” de Vries said. “The open-scientific research approach to BigCode provides ServiceNow developers and customers with full transparency into how everything was developed and demonstrates ServiceNow’s commitment to making socially responsible contributions to the community.”\\n\\nStarCoder isn’t open source in the strictest sense. Rather, it’s being released under a licensing scheme, OpenRAIL-M, that includes “legally enforceable” use case restrictions that derivatives of the model — and apps using the model — are required to comply with.', metadata={'source': '/content/new_articles/05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}),\n",
       " Document(page_content='For example, StarCoder users must agree not to leverage the model to generate or distribute malicious code. While real-world examples are few and far between (at least for now), researchers have demonstrated how AI like StarCoder could be used in malware to evade basic forms of detection.\\n\\nWhether developers actually respect the terms of the license remains to be seen. Legal threats aside, there’s nothing at the base technical level to prevent them from disregarding the terms to their own ends.\\n\\nThat’s what happened with the aforementioned Stable Diffusion, whose similarly restrictive license was ignored by developers who used the generative AI model to create pictures of celebrity deepfakes.\\n\\nBut the possibility hasn’t discouraged von Werra, who feels the downsides of not releasing StarCoder aren’t outweighed by the upsides.', metadata={'source': '/content/new_articles/05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}),\n",
       " Document(page_content='But the possibility hasn’t discouraged von Werra, who feels the downsides of not releasing StarCoder aren’t outweighed by the upsides.\\n\\n“At launch, StarCoder will not ship as many features as GitHub Copilot, but with its open-source nature, the community can help improve it along the way as well as integrate custom models,” he said.\\n\\nThe StarCoder code repositories, model training framework, dataset-filtering methods, code evaluation suite and research analysis notebooks are available on GitHub as of this week. The BigCode project will maintain them going forward as the groups look to develop more capable code-generating models, fueled by input from the community.\\n\\nThere’s certainly work to be done. In the technical paper accompanying StarCoder’s release, Hugging Face and ServiceNow say that the model may produce inaccurate, offensive, and misleading content as well as PII and malicious code that managed to make it past the dataset filtering stage.', metadata={'source': '/content/new_articles/05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}),\n",
       " Document(page_content=\"Microsoft doubles down on AI with new Bing features The company's betting the farm on generative AI\\n\\nMicrosoft is embarking on the next phase of Bing’s expansion. And — no surprise — it heavily revolves around AI.\\n\\nAt a preview event this week in New York City, Microsoft execs including Yusuf Mehdi, the CVP and consumer chief marketing officer, gave members of the press, including this reporter, a look at the range of features heading to Bing over the next few days, weeks and months.\\n\\nThey don’t so much reinvent the wheel as they build on what Microsoft has injected into the Bing experience over the past three months or so. Since launching Bing Chat, its AI-powered chatbot powered by OpenAI’s GPT-4 and DALL-E 2 models, Microsoft says that visitors to Bing — which has grown to exceed 100 million daily active users — have engaged in over half a billion chats and created more than 200 million images.\", metadata={'source': '/content/new_articles/05-04-microsoft-doubles-down-on-ai-with-new-bing-features.txt'}),\n",
       " Document(page_content=\"Looking ahead, Bing will become more visual, thanks to more image- and graphic-centric answers in Bing Chat. It’ll also become more personalized, with capabilities that’ll allow users to export their Bing Chat histories and draw in content from third-party plugins (more on those later). And it’ll embrace multimodality, at least in the sense that Bing Chat will be able to answer questions within the context of images.\\n\\n“I think it’s safe to say that we’re underway with the transformation of search,” Mehdi said in prepared remarks. “In our minds, we think that today will be the start of the next generation of this ‘search mission.'”\\n\\nOpen, and visual\\n\\nAs of today, the new Bing — the one with Bing Chat — is now available waitlist-free. Anyone can try it out by signing in with a Microsoft Account.\", metadata={'source': '/content/new_articles/05-04-microsoft-doubles-down-on-ai-with-new-bing-features.txt'}),\n",
       " Document(page_content='Open, and visual\\n\\nAs of today, the new Bing — the one with Bing Chat — is now available waitlist-free. Anyone can try it out by signing in with a Microsoft Account.\\n\\nIt’s more or less the experience that launched several months ago. But as alluded to earlier, Bing Chat will soon respond with images — at least where it makes sense. Answers to questions (e.g. “Where is Machu Picchu?”) will be accompanied by relevant images if any exist, much like the standard Bing search flow but condensed into a card-like interface.\\n\\nIn a demo at the event, a spokesperson typed the question “Does the saguaro cactus grow flowers?” and Bing Chat pulled up a paragraph-long response alongside an image of the cactus in question. For me, it evoked the “knowledge panels” in Google Search.\\n\\nMicrosoft isn’t saying which categories of content, exactly, might trigger an image. But it does have filtering in place to prevent explicit images from appearing — or so it claims.', metadata={'source': '/content/new_articles/05-04-microsoft-doubles-down-on-ai-with-new-bing-features.txt'}),\n",
       " Document(page_content='Microsoft isn’t saying which categories of content, exactly, might trigger an image. But it does have filtering in place to prevent explicit images from appearing — or so it claims.\\n\\nSarah Bird, the head of responsible AI at Microsoft, told me that Bing Chat benefits from the filtering and moderation already in place with Bing search. Beyond this, Bing Chat uses a combination of “toxicity classifiers,” or AI models trained to detect potentially harmful prompts, and blacklists to keep the chat relatively clean.\\n\\nThose measures didn’t prevent Bing Chat from going off the rails when it first rolled out in preview in early February, it’s worth noting. Our coverage found the chatbot spouting vaccine misinformation and writing a hateful screed from the perspective of Adolf Hitler. Other reporters got it to make threats, claim multiple identities and even shame them for admonishing it.', metadata={'source': '/content/new_articles/05-04-microsoft-doubles-down-on-ai-with-new-bing-features.txt'}),\n",
       " Document(page_content='In another knock against Microsoft, the company just a few months ago laid off the ethics and society team within its larger AI organization. The move left Microsoft without a dedicated team to ensure its AI principles are closely tied to product design.\\n\\nBird, though, asserts that meaningful progress has been made and that these sorts of AI issues aren’t solved overnight — public though Bing Chat may be. Among other measures, a team of human moderators is in place to watch for abuse, she said, such as users attempting to use Bing Chat to generate phishing emails.\\n\\nBut — as members of the press weren’t given the chance to interact with the latest version of Bing beyond curated demos — I can’t say to what extent all that’s made a difference. It’ll doubtless become clear once more folks get their hands on it.', metadata={'source': '/content/new_articles/05-04-microsoft-doubles-down-on-ai-with-new-bing-features.txt'}),\n",
       " Document(page_content='One aspect of Bing Chat that is improving is the transparency around its responses — specifically responses of a fact-based nature. Soon, when asked to summarize a document or about the contents a document (e.g. “what does this page say about the Brooklyn Bridge?”), whether a 20-page PDF or a Wikipedia article, Bing Chat will include citations indicating from where in the text the information came from. Clicking on them will highlight the corresponding passage.\\n\\nProductivity emergent\\n\\nIn another new feature on the visual front, Bing Chat will be able to create charts and graphs when fed the right prompt and data. Previously, asking something like “Which are the most populous cities in Brazil?” would yield a basic list of results. But in a near-future preview, Bing Chat will present those results visually and in the chart type of a user’s choosing.', metadata={'source': '/content/new_articles/05-04-microsoft-doubles-down-on-ai-with-new-bing-features.txt'}),\n",
       " Document(page_content='This seemingly represents a step for Bing toward a full-blown productivity platform, particularly when paired with the enhanced text-to-image generation capabilities coming down the pipeline.\\n\\nIn the coming weeks, Bing Image Creator — Microsoft’s tool that can generate images from text prompts, powered by DALL-E 2 — will understand more languages aside from English (over 100 total). As with English, users will be able to refine the images they generate with follow-up prompts (e.g. “Make an image of a bunny rabbit,” followed by “now make the fur pink”).\\n\\nGenerative art AI has been in the headlines a lot, lately — and not for the most optimistic of reasons necessarily.', metadata={'source': '/content/new_articles/05-04-microsoft-doubles-down-on-ai-with-new-bing-features.txt'}),\n",
       " Document(page_content='Generative art AI has been in the headlines a lot, lately — and not for the most optimistic of reasons necessarily.\\n\\nPlaintiffs have brought several lawsuits against OpenAI and its rival vendors, alleging that copyrighted data — mostly art — was used without their permission to train generative models like DALL-E 2. Generative models “learn” to create art and more by “training” on sample images and text, usually scraped indiscriminately from the public web.\\n\\nI asked Bird about whether Microsoft is exploring ways to compensate creators whose work was swept up in training data, even if the company’s official position is that it’s a matter of fair use. Several platforms launching generative AI tools, including Shutterstock, have kick-started creators funds along these lines. Others, like Spawning, are creating mechanisms to let artists opt out of AI model training altogether.', metadata={'source': '/content/new_articles/05-04-microsoft-doubles-down-on-ai-with-new-bing-features.txt'}),\n",
       " Document(page_content='Bird implied that these issues will eventually have to be confronted — and that content creators deserve some form of recompense. But she wasn’t willing to commit to anything concrete this week.\\n\\nMultimodal search\\n\\nElsewhere on the image front, Bing Chat is gaining the ability to understand images as well as text. Users will be able to upload images and search the web for related content, for example copying a link to an image of a crocheted octopus and asking Bing Chat the question “how do I make that?” to get step-by-step instructions.\\n\\nMultimodality powers the new page context function in the Edge app for mobile, as well. Users will be able to ask questions in Bing Chat related to the mobile page they’re viewing.', metadata={'source': '/content/new_articles/05-04-microsoft-doubles-down-on-ai-with-new-bing-features.txt'}),\n",
       " Document(page_content='Multimodality powers the new page context function in the Edge app for mobile, as well. Users will be able to ask questions in Bing Chat related to the mobile page they’re viewing.\\n\\nMicrosoft wouldn’t say either way, but it seems likely that these new multimodal abilities stem from GPT-4, which can understand images in addition to text. When OpenAI announced GPT-4, it didn’t make the model’s image understanding capabilities available to all customers — and still hasn’t. I’d wager that Microsoft, though, being a major investor in and close collaborator with OpenAI, has some sort of privileged access.\\n\\nAny image upload tool can be abused, of course, which is why Microsoft is employing automated filtering and hashing to block illicit uploads, according to Bird. The jury’s out on how well these work, though — we weren’t given the chance to test image uploads ourselves.\\n\\nNew chat features\\n\\nMultimodality and new visual features aren’t all that’s coming to Bing Chat.', metadata={'source': '/content/new_articles/05-04-microsoft-doubles-down-on-ai-with-new-bing-features.txt'}),\n",
       " Document(page_content='New chat features\\n\\nMultimodality and new visual features aren’t all that’s coming to Bing Chat.\\n\\nSoon, Bing Chat will store users’ chat histories, letting them pick up where they left off and return to previous chats when they wish. It’s an experience akin to the chat history feature OpenAI recently brought to ChatGPT, showing a list of chats and the bot’s responses to each of those chats.\\n\\nThe specifics of the chat history feature have yet to be ironed out, like how long chats will be stored, exactly. But users will be able to delete their history at any time regardless, Microsoft says — addressing the criticisms several European Union governments had against ChatGPT.', metadata={'source': '/content/new_articles/05-04-microsoft-doubles-down-on-ai-with-new-bing-features.txt'}),\n",
       " Document(page_content='Bing Chat will also gain export and share functionalities, letting users share conversations on social media or to a Word document. Dena Saunders, a partner GM in Microsoft’s web experiences team, told TechCrunch that a more robust copy-and-paste system is in the works — but not in preview just yet — for graphs and images created through Bing Chat.\\n\\nPerhaps the most transformative addition to Bing Chat, though, is plugins. From partners like OpenTable and Wolfram Alpha, plugins greatly extend what Bing Chat can do, for example helping users book a reservation or create visualizations and get answers to challenging science and math questions.\\n\\nLike chat history, the not-yet-live plugins functionality is in the very preliminary stages. There’s no plugins marketplace to speak of; plugins can be toggled on or off from the Bing Chat web interface.', metadata={'source': '/content/new_articles/05-04-microsoft-doubles-down-on-ai-with-new-bing-features.txt'}),\n",
       " Document(page_content='Saunders hinted, but wouldn’t confirm, that the Bing Chat plugins scheme was associated with — or perhaps identical to — OpenAI’s recently introduced plugins for ChatGPT. That’d certainly make sense, given the similarities between the two.\\n\\nEdge, refreshed\\n\\nBing Chat is available through Edge as well as the web, of course. And Edge is getting a fresh coat of paint alongside Bing Chat.\\n\\nFirst previewed in February, the new and improved Edge features rounded corners in line with Microsoft’s Windows 11 design philosophy. Elements in the browser are now more “containerized,” as one Microsoft spokesperson put it, and there’s subtle tweaks throughout, like the Microsoft Account image moving left-of-center.', metadata={'source': '/content/new_articles/05-04-microsoft-doubles-down-on-ai-with-new-bing-features.txt'}),\n",
       " Document(page_content=\"In Compose, Edge’s Bing Chat-powered tool that can write emails and more given a basic prompt (e.g. “write an invitation to my dog’s birthday party”), a new option lets users adjust the length, phrasing and tone of the generated text to nearly anything they’d like. Type in the desired tone, and Bing Chat will write a message to match — Bird says filters are in place to prevent the use of clearly problematic tones, like “hateful” or “racist.”\\n\\nFar more intriguing than Compose, though — at least to me — are actions in Edge, which translate certain Bing Chat prompts into automations.\\n\\nTyping a command like “bring my passwords from another browser” in Bing Chat in the Edge sidebar opens Edge’s browsing data settings page, while the prompt “play ‘The Devil Wears Prada'” pulls up a list of streaming options including Vudu and (predictably) the Microsoft Store. There’s even an action that automatically organizes — and color-coordinates — browsing tabs.\", metadata={'source': '/content/new_articles/05-04-microsoft-doubles-down-on-ai-with-new-bing-features.txt'}),\n",
       " Document(page_content='Actions are in a primitive stage at present. But it’s clear where Microsoft’s going, here. One imagines actions eventually expanding beyond Edge to reach other Microsoft products, like Office 365, and perhaps one day the whole Windows desktop.\\n\\nSaunders wouldn’t confirm or deny that this is the endgame. “Stay tuned for Microsoft Build,” she told me, referring to Microsoft’s upcoming developer conference. We shall.', metadata={'source': '/content/new_articles/05-04-microsoft-doubles-down-on-ai-with-new-bing-features.txt'}),\n",
       " Document(page_content=\"Generative AI is pretty impressive in terms of its fidelity these days, as viral memes like Balenciaga Pope would suggest. The latest systems can conjure up scenescapes from city skylines to cafes, creating images that appear startlingly realistic — at least on first glance.\\n\\nBut one of the longstanding weaknesses of text-to-image AI models is, ironically, text. Even the best models struggle to generate images with legible logos, much less text, calligraphy or fonts.\\n\\nBut that might change.\\n\\nLast week, DeepFloyd, a research group backed by Stability AI, unveiled DeepFloyd IF, a text-to-image model that can “smartly” integrate text into images. Trained on a dataset of more than a billion images and text, DeepFloyd IF, which requires a GPU with at least 16GB of RAM to run, can create an image from a prompt like “a teddy bear wearing a shirt that reads ‘Deep Floyd'” — optionally in a range of styles.\", metadata={'source': '/content/new_articles/05-05-with-deepfloyd-generative-ai-art-gets-a-text-upgrade.txt'}),\n",
       " Document(page_content='DeepFloyd IF is available in open source, licensed in a way that prohibits commercial use — for now. The restriction was likely motivated by the current tenuous legal status of generative AI art models. Several commercial model vendors are under fire from artists who allege the vendors are profiting from their work without compensating them by scraping that work from the web without permission.\\n\\nBut NightCafe, the generative art platform, was granted early access to DeepFloyd IF.\\n\\nNightCafe CEO Angus Russell spoke to TechCrunch about what makes DeepFloyd IF different from other text-to-image models and why it might represent a significant step forward for generative AI.\\n\\nAccording to Russell, DeepFloyd IF’s design was heavily inspired by Google’s Imagen model, which was never released publicly. In contrast to models like OpenAI’s DALL-E 2 and Stable Diffusion, DeepFloyd IF uses multiple different processes stacked together in a modular architecture to generate images.', metadata={'source': '/content/new_articles/05-05-with-deepfloyd-generative-ai-art-gets-a-text-upgrade.txt'}),\n",
       " Document(page_content='With a typical diffusion model, the model learns how to gradually subtract noise from a starting image made almost entirely of noise, moving it closer step by step to the target prompt. DeepFloyd IF performs diffusion not once but several times, generating a 64x64px image then upscaling the image to 256x256px and finally to 1024x1024px.\\n\\nWhy the need for multiple diffusion steps? DeepFloyd IF works directly with pixels, Russell explained. Diffusion models are for the most part latent diffusion models, which essentially means they work in a lower-dimensional space that represents a lot more pixels but in a less accurate way.', metadata={'source': '/content/new_articles/05-05-with-deepfloyd-generative-ai-art-gets-a-text-upgrade.txt'}),\n",
       " Document(page_content='The other key difference between DeepFloyd IF and models such as Stable Diffusion and DALL-E 2 is that the former uses a large language model to understand and represent prompts as a vector, a basic data structure. Due to the size of the large language model embedded in DeepFloyd IF’s architecture, the model is particularly good at understanding complex prompts and even spatial relationships described in prompts (e.g. “a red cube on top of a pink sphere”).\\n\\n“It’s also very good at generating legible and correctly spelled text in images, and can even understand prompts in multiple languages,” Russell added. “Of these capabilities, the ability to generate legible text in images is perhaps the biggest breakthrough to make DeepFloyd IF stand out from other algorithms.”', metadata={'source': '/content/new_articles/05-05-with-deepfloyd-generative-ai-art-gets-a-text-upgrade.txt'}),\n",
       " Document(page_content='Because DeepFloyd IF can pretty capably generate text in images, Russell expects it to unlock a wave of new generative art possibilities — think logo design, web design, posters, billboards and even memes. The model should also be much better at generating things like hands, he says, and — because it can understand prompts in other languages — it might be able to create text in those languages, too.\\n\\n“NightCafe users are excited about DeepFloyd IF largely because of the possibilities that are unlocked by generating text in images,” Russell said. “Stable Diffusion XL was the first open source algorithm to make headway on generating text — it can accurately generate one or two words some of the time — but it’s still not good enough at it for use cases where text is important.”', metadata={'source': '/content/new_articles/05-05-with-deepfloyd-generative-ai-art-gets-a-text-upgrade.txt'}),\n",
       " Document(page_content='That’s not to suggest DeepFloyd IF is the holy grail of text-to-image models. Russell notes that the base model doesn’t generate images that are quite as aesthetically pleasing as some diffusion models, although he expects fine-tuning will improve that.\\n\\nBut the bigger question, to me, is to what degree DeepFloyd IF suffers from the same flaws as its generative AI brethren.\\n\\nA growing body of research has turned up racial, ethnic, gender and other forms of stereotyping in image-generating AI, including Stable Diffusion. Just this month, researchers at AI startup Hugging Face and Leipzig University published a tool demonstrating that models including Stable Diffusion and OpenAI’s DALL-E 2 tend to produce images of people that look white and male, especially when asked to depict people in positions of authority.\\n\\nThe DeepFloyd team, to their credit, note the potential for biases in the fine print accompanying DeepFloyd IF:', metadata={'source': '/content/new_articles/05-05-with-deepfloyd-generative-ai-art-gets-a-text-upgrade.txt'}),\n",
       " Document(page_content='The DeepFloyd team, to their credit, note the potential for biases in the fine print accompanying DeepFloyd IF:\\n\\nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. This affects the overall output of the model, as white and western cultures are often set as the default.\\n\\nAside from this, DeepFloyd IF, like other open source generative models, could be used for harm, like generating pornographic celebrity deepfakes and graphic depictions of violence. On the official webpage for DeepFloyd IF, the DeepFloyd team says that they used “custom filters” to remove watermarked, “NSFW” and “other inappropriate content” from the training data.\\n\\nBut it’s unclear exactly which content was removed — and how much might’ve been missed. Ultimately, time will tell.', metadata={'source': '/content/new_articles/05-05-with-deepfloyd-generative-ai-art-gets-a-text-upgrade.txt'}),\n",
       " Document(page_content='Welcome to The Interchange! If you received this in your inbox, thank you for signing up and your vote of confidence. If you’re reading this as a post on our site, sign up here so you can receive it directly in the future. Every week, we’ll take a look at the hottest fintech news of the previous week. This will include everything from funding rounds to trends to an analysis of a particular space to hot takes on a particular company or phenomenon. There’s a lot of fintech news out there and it’s our job to stay on top of it — and make sense of it — so you can stay in the know. — Mary Ann and Christine\\n\\nBusy, busy, busy\\n\\nIt was a busy week in startup and venture lands, and the fintech space was no exception.', metadata={'source': '/content/new_articles/05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt'}),\n",
       " Document(page_content='Busy, busy, busy\\n\\nIt was a busy week in startup and venture lands, and the fintech space was no exception.\\n\\nIn the venture world, I reported on Peter Ackerson’s departure from Fin Capital earlier this year and the fact that he has since started a new venture firm called Audere Capital. The circumstances around his departure remain fuzzy, but one source speculated that tension arose between Ackerson and Fin founding partner Logan Allin over some of the goings-on at alternative financing startup Pipe last year. More details here.', metadata={'source': '/content/new_articles/05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt'}),\n",
       " Document(page_content='We also wrote about Tellus, a startup that raised $16 million in an Andreessen Horowitz–led seed round of funding last year that is now being scrutinized by the U.S. government. When I interviewed the company’s co-founder, Rocky Lee, last year, I admit I was a little bit skeptical of any company that would bet on people agreeing to high-interest mortgage rates to upgrade their homes (think 9%!) and using customer savings deposits to fund such loans. When I asked Lee if this was risky, he admitted it was but insisted that Tellus utilized “very strict underwriting criteria” and had not yet seen any defaults “because the majority of its borrowers go on to soon refinance their loans at more favorable terms.” Well, last week U.S. Senator Sherrod Brown, chairman of the U.S. Senate Committee on Banking, Housing, and Urban Affairs, wrote a letter to FDIC chairman Martin Gruenberg expressing concerns about Tellus’s claims. In that letter, Brown pressed the FDIC to review Tellus’s business', metadata={'source': '/content/new_articles/05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt'}),\n",
       " Document(page_content='on Banking, Housing, and Urban Affairs, wrote a letter to FDIC chairman Martin Gruenberg expressing concerns about Tellus’s claims. In that letter, Brown pressed the FDIC to review Tellus’s business practices “to ensure that customers are protected from financial fraud and abuse.” In a twist, I discovered that Lee was married to a16z general partner Connie Chan (not sure if he still is). Neither he nor the venture firm commented on the senator’s concerns but Tellus CEO/CTO Jeromee Johnson did provide me with a statement via email. Read more here.', metadata={'source': '/content/new_articles/05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt'}),\n",
       " Document(page_content='Infrastructure continues to be resilient, even in a downturn. This week alone, I wrote about two payments infrastructure companies making moves, and my colleague Ingrid Lunden wrote about Stripe’s latest customer win. For starters, I covered Finix officially becoming a payments processor — a natural evolution really for a company that has slowly been expanding its offerings. In case you forgot, Finix is a startup that Sequoia backtracked on investing in after Stripe (an existing portfolio company) expressed concerns about being too competitive. (Finix got to keep its $21 million, though!) Now that it directly connects to all major U.S. card networks — American Express, Discover, Mastercard and Visa — and no longer relies on a third-party processor, Finix says it’s able to offer businesses “instant onboarding, improved economics and opportunities for lowering interchange fees.” I talked with CEO and co-founder Richie Serna all about it, and why he thinks what Finix has built is', metadata={'source': '/content/new_articles/05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt'}),\n",
       " Document(page_content='“instant onboarding, improved economics and opportunities for lowering interchange fees.” I talked with CEO and co-founder Richie Serna all about it, and why he thinks what Finix has built is different from what legacy players and Stripe have on the market. I also wrote about Liquido, a Mountain View, California–based startup aiming to be the “Stripe of Latin America,” and more. Index Ventures’ Mark Fiorentino led two funding rounds totaling $26 million into the company in 2021. Interestingly, prior to joining Index, Fiorentino helped build and lead business strategy and finance at Stripe from 2015 to 2019. And Ingrid wrote about Stripe landing Uber as a customer, which was a bit unexpected considering that rival Lyft has been a longtime marquee customer of the company.', metadata={'source': '/content/new_articles/05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt'}),\n",
       " Document(page_content='And, last but not least, corporate card and spend management startup Brex announced last week a global expansion of its Empower product into new markets so that companies that are its customers now “can spend globally and operate locally” in countries such as Brazil, Canada, Israel, Japan, Mexico, Singapore, South Africa, and the Philippines, as well as in 36 European countries. In an interview with TechCrunch, Brex co-founder and co-CEO Henrique Dubugras said that the company believes the move “will really open up TAM” for Brex since so many existing and prospective U.S. clients “have some sort of global operations.”', metadata={'source': '/content/new_articles/05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt'}),\n",
       " Document(page_content='“One of the big problems that companies have when they operate globally is that they actually need to open up an account in all these different countries where they might have employees. It becomes really complicated to set up all your financial systems on a country by country basis,” he added. “Now, if you use Brex, you can actually operate as if you were a local company with a local card.”\\n\\nIn other words, companies using Brex that have employees who work in other countries are giving those workers the ability to use a corporate card freely in their home countries, while also giving the company the ability to pay the statements in local currency from the local bank.\\n\\n“It’s something that we’ve been trying to do for a while,” Dubugras added, noting that insurtech Lemonade is a customer. — Mary Ann\\n\\nOther weekly news', metadata={'source': '/content/new_articles/05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt'}),\n",
       " Document(page_content='“It’s something that we’ve been trying to do for a while,” Dubugras added, noting that insurtech Lemonade is a customer. — Mary Ann\\n\\nOther weekly news\\n\\nChristine, Mary Ann and Natasha Mascarenhas teamed up to write about the collapse of First Republic Bank, speaking with tech founders and investors who had money in the bank about what happens next. We also spoke with an FRB competitor about what all of these startup bank collapses mean for business. More here.\\n\\nReports Carly Page: “Hackers have published a trove of sensitive data stolen from payment software company AvidXchange after the company fell victim to ransomware for the second time this year. AvidXchange provides cloud-based software that helps organizations automate invoice processing and payment management processes. A ransomware group called RansomHouse claimed responsibility for the recent cyberattack on AvidXchange.” More here.', metadata={'source': '/content/new_articles/05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt'}),\n",
       " Document(page_content='Christine wrote about the launch of former Bolt CEO Ryan Breslow’s new company, Love, which is a wellness marketplace that features an initial 200 curated products, like supplements, health testing kits and essential oils, among such categories as reducing stress and gut health. All of the products on the site pass a set of compliance processes and reviews developed in partnership with clinical trials company Radicle Science, which Breslow said is unique to the company. More here.\\n\\nBritish neobank Revolut launched in Brazil, its first country in Latin America, offering customers a global bank account and crypto investments, Silicon Republic reported. The company already had a presence in the country after hiring Glauber Mota as the CEO of its Brazil business in March 2022. Alex Wilhelm and Anna Heim reported in April that Revolut “saw its valuation decline by some 46% in the eyes of one of its backers.” More here.', metadata={'source': '/content/new_articles/05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt'}),\n",
       " Document(page_content='Tage Kene-Okafor reported on Fingo, a YC-backed Kenyan fintech, which launched a neobank — the first of its kind in the East African country, according to the company — in collaboration with Pan-African financial institution Ecobank Kenya. “It’s taken a while for Fingo to get here since CEO Kiiru Muhoya and his co-founders James da Costa, Ian Njuguna and Gitari Tirima founded the Kenyan outfit in January 2021 to provide financial services that appeal to a fast-growing African youthful population that happens to be the youngest globally but the most financially marginalized. After a $200,000 pre-seed round, Fingo got into YC S21 and raised $4 million in seed funding toward the end of that year.” More here.', metadata={'source': '/content/new_articles/05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt'}),\n",
       " Document(page_content='Manish Singh reported that Paytm, India’s leading mobile payments firm, reported a 13.2% surge in revenue to $285.7 million in the quarter ending March and pared its loss by 57% to $20.5 million “in a sharp turnaround for the company that is increasingly trying to become profitable following a tremulous year and a half after its public debut.” More here.\\n\\nMore headlines\\n\\nApple and fintechs like Robinhood chase yield-hungry depositors as Fed rate hikes continue. Similarly, Arta Finance, a company providing access to alternative assets, debuted the Harvest Treasuries AI-Managed Portfolio, which offers a 4.62% APY (annual percentage yield), and Wealthfront’s cash account now offers 4.55% for all clients and 5.05% APY for clients who refer a friend.\\n\\nFintech projected to become a $1.5 trillion industry by 2030, according to a new report from Boston Consulting Group and QED Investors\\n\\nOpendoor tech earnings beat by $0.77, revenue topped estimates', metadata={'source': '/content/new_articles/05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt'}),\n",
       " Document(page_content='Fintech projected to become a $1.5 trillion industry by 2030, according to a new report from Boston Consulting Group and QED Investors\\n\\nOpendoor tech earnings beat by $0.77, revenue topped estimates\\n\\nEveree joins Visa’s Fintech Fast Track Program with launch of Everee Visa® pay card\\n\\nFunding and M&A\\n\\nSeen on TechCrunch\\n\\nAfrican payment service provider Nomba raises $30M, backed by Base10 Partners and Shopify\\n\\nBend is taking on Brex and Ramp with a green twist and a $2.5M seed round\\n\\nAnd elsewhere', metadata={'source': '/content/new_articles/05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt'}),\n",
       " Document(page_content='African payment service provider Nomba raises $30M, backed by Base10 Partners and Shopify\\n\\nBend is taking on Brex and Ramp with a green twist and a $2.5M seed round\\n\\nAnd elsewhere\\n\\nDigital wallet for insurance Marble bags $4.2M. Speaking about the raise to TechCrunch, CEO Stuart Winchester said via email, “American households are under a lot of financial strain right now, and insurance expenses are no small part of that. We will continue to put out features that make it easier to not only save money and maximize value, but also to reduce the mental load of managing multiple insurance policies. We expect to see the insurance industry in general adopt more of the consumer friendly features that we’ve helped pioneer.”\\n\\nInsurtech startup Novidea raises $50 million Series C\\n\\nExclusive: Former Venmo COO raises $20M for Vera Equity\\n\\nTarabut Gateway raises $32 million to expand Saudi open banking\\n\\nMusic financing startup Duetti raises $32 million to buy old songs', metadata={'source': '/content/new_articles/05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt'}),\n",
       " Document(page_content='Exclusive: Former Venmo COO raises $20M for Vera Equity\\n\\nTarabut Gateway raises $32 million to expand Saudi open banking\\n\\nMusic financing startup Duetti raises $32 million to buy old songs\\n\\nBilling platform Inbox Health raises $22.5M and more digital health fundings\\n\\nGoogle’s VC firm just led a $12 million Series A investment in Range, a startup that’s training AI to give financial advice\\n\\nOpenEnvoy raises $15 million to grow AP automation solution\\n\\nMiami-based startup Kiddie Kredit raises $1.4M with support from Dwyane Wade and Baron Davis\\n\\nBlack-owned tech firm Greenwood acquires digital banking rival. TechCrunch covered Greenwood’s last raise in March of 2021 here.', metadata={'source': '/content/new_articles/05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt'}),\n",
       " Document(page_content='Black-owned tech firm Greenwood acquires digital banking rival. TechCrunch covered Greenwood’s last raise in March of 2021 here.\\n\\nJoin us at TechCrunch Disrupt 2023 in San Francisco this September as we explore the impact of fintech on our world today. New this year, we will have a whole day dedicated to all things fintech featuring some of today’s leading fintech figures. Save up to $800 when you buy your pass now through May 15, and save 15% on top of that with promo code INTERCHANGE. Learn more.\\n\\nWe are done for this week and it’s a good thing because we are also TIRED! See you next week — same time, same place. Until then, take good care! xoxo, Mary Ann and Christine', metadata={'source': '/content/new_articles/05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt'}),\n",
       " Document(page_content='SpaceX’s super-heavy launch system Starship is poised to fundamentally reshape the space economy. The 394-foot-tall vehicle, which took to the skies for the first time last month, is designed to carry a staggering amount of mass to low Earth orbit and into deep space.\\n\\nTechCrunch+ spoke with three pure-play space VCs — Space Capital founder and managing partner Chad Anderson, Space.VC founder and general partner Jonathan Lacoste and E2MC Ventures founder Raphael Roettgen — to learn more about how they advise founders to think through Starship’s super-heavy implications.\\n\\nWhile the trio diverges on many fine points, they all agreed that founders should be thinking now about how Starship could affect their operations, for better or worse.\\n\\n“Starship has such high importance to the space sector that probably almost everyone who has a space company has to war game what that means for their business,” Roettgen said.\\n\\nChanging the face of launch …', metadata={'source': '/content/new_articles/05-07-spacex-starship-startups-future.txt'}),\n",
       " Document(page_content='Changing the face of launch …\\n\\nThe most obvious way in which Starship is likely to revolutionize the industry is by continuing the trend SpaceX firmly established with the debut of Falcon 9: further lowering the cost of launching mass to space. Starship will be capable of carrying 100 to 150 tons of stuff to orbit, a paradigm-shifting quantity that far outstrips the payload capacity of any rocket that humans have ever designed.', metadata={'source': '/content/new_articles/05-07-spacex-starship-startups-future.txt'}),\n",
       " Document(page_content='In the must-watch final season of “Succession,” Kendall Roy enters a conference room with his siblings. As the scene opens, he takes a seat and declares: “Who will be the successor? Me.”\\n\\nOf course, that scene didn’t appear on HBO’s hit show, but it’s a good illustration of generative AI’s level of sophistication compared to the real thing. Yet as the Writers Guild of America goes on strike in pursuit of livable working conditions and better streaming residuals, the networks won’t budge on writers’ demands to regulate the use of AI in writers’ rooms.\\n\\n“Our proposal is that we not be required to adapt something that’s output by AI, and that the output of an AI not be considered writers’ work,” comedy writer Adam Conover told TechCrunch. “That doesn’t entirely exclude that technology from the production process, but it does mean that our working conditions wouldn’t be undermined by AI.”', metadata={'source': '/content/new_articles/05-03-ai-replace-tv-writers-strike.txt'}),\n",
       " Document(page_content='But the Alliance of Motion Picture and Television Producers (AMPTP) refused to engage with that proposal, instead offering a yearly meeting to discuss “advances in technology.”\\n\\n“When we first put [the proposal] in, we thought we were covering our bases — you know, some of our members are worried about this, the area is moving quickly, we should get ahead of it,” Conover said. “We didn’t think it’d be a contentious issue because the fact of the matter is, the current state of the text-generation technology is completely incapable of writing any work that could be used in a production.”', metadata={'source': '/content/new_articles/05-03-ai-replace-tv-writers-strike.txt'}),\n",
       " Document(page_content='The text-generating algorithms behind tools like ChatGPT are not built to entertain us. Instead, they analyze patterns in massive datasets to respond to requests by determining what is most likely the desired output. So, ChatGPT knows that “Succession” is about an aging media magnate’s children fighting for control of his company, but it is unlikely to come up with any dialogue more nuanced than, “Who will be the successor? Me.”\\n\\nAccording to Ben Zhao, a University of Chicago professor and faculty lead of art anti-mimicry tool Glaze, AI advancements can be used as an excuse for corporations to devalue human labor.\\n\\n“It’s to the advantage of the studios and bigger corporations to basically over-claim ChatGPT’s abilities, so they can, in negotiations at least, undermine and minimize the role of human creatives,” Zhao told TechCrunch. “I’m not sure how many people at these larger companies actually believe what they’re saying.”', metadata={'source': '/content/new_articles/05-03-ai-replace-tv-writers-strike.txt'}),\n",
       " Document(page_content='Conover emphasized that some parts of a writer’s job are less obvious than literal scriptwriting but equally difficult to replicate with AI.\\n\\n“It’s going and meeting with the set decoration department that says, ‘Hey, we can’t actually build this prop that you’re envisioning, could you do this instead?’ and then you talk to them and go back and rewrite,” he said. “This is a human enterprise that involves working with other people, and that simply cannot be done by an AI.”\\n\\nComedian Yedoye Travis sees how AI could be useful in a writers’ room.\\n\\n“What we do in writers’ rooms is ultimately bouncing ideas around,” he told TechCrunch. “Even if it’s not good per se, an AI can throw together a script in however many minutes, compared to a week for human writers, and then it’s easier to edit than to write.”', metadata={'source': '/content/new_articles/05-03-ai-replace-tv-writers-strike.txt'}),\n",
       " Document(page_content='But even if there may be some promise for how humans can leverage this technology, he worries that studios see it merely as a way to demand more from writers over a shorter period of time.\\n\\n“It says to me that they’re only concerned with things being made,” Travis said. “They’re not concerned with people being paid for things being made.”\\n\\nWriters are also advocating to regulate the use of AI in entertainment because it remains a legal grey area.\\n\\n“It’s not clear that the work that it outputs is copyrightable, and a movie studio is not going to spend $50 to $100 million shooting a script that they don’t know that they own the copyright to,” Conover said. “So we figured this would be an easy give for [the AMPTP], but they completely stonewalled on it.”', metadata={'source': '/content/new_articles/05-03-ai-replace-tv-writers-strike.txt'}),\n",
       " Document(page_content='As the Writers Guild of America strikes for the first time since its historic 100-day action in 2007, Conover said he thinks the debate over AI technology is a “red herring.” With generative AI in such a rudimentary stage, writers are more immediately concerned with dismal streaming residuals and understaffed writing teams. Yet studios’ pushback on the union’s AI-related requests only further reinforces the core issue: The people who power Hollywood aren’t being paid their fair share.\\n\\n“I’m not worried about the technology,” Conover said. “I’m worried about the companies using technology, that is not in fact very good, to undermine our working conditions.”', metadata={'source': '/content/new_articles/05-03-ai-replace-tv-writers-strike.txt'}),\n",
       " Document(page_content='After Google cut all but three of the projects at its in-house incubator Area 120 and shifted it to work on AI projects across Google, one of the legacy efforts — coincidentally also an AI project — is now officially exiting to Google. Checks, an AI-powered tool to check mobile apps for compliance with various privacy rules and regulations, is moving into Google proper as a privacy product aimed at mobile developers.\\n\\nChecks originally made its debut in February 2022, although it was in development for some time before that. In its time at Area 120, it became one of the largest projects in the group, co-founders Fergus Hurley and Nia Castelly told me, with 10 people fully dedicated to it and a number of others contributing less formally. The founders’ job titles under Google will now be GM and Legal Lead, respectively, for Checks.', metadata={'source': '/content/new_articles/05-03-checks-the-ai-powered-data-protection-project-incubated-in-area-120-officially-exits-to-google.txt'}),\n",
       " Document(page_content='The amount that Google invested in the project was never disclosed, nor was the valuation of the exit to the parent company from the incubator, but the company has confirmed that there was a valuation and that it had grown since launch.\\n\\nThe company is not disclosing how many customers it has in total but notes that they are in the sectors of gaming, health, finance, education and retail. A sampling includes Miniclip, Rovio, Kongregate, Crayola and Yousician and in total the number of customers represented by its customers is over 3 billion.\\n\\nChecks will sit in the Developer X division. “What Fergus, Nia, and the entire Google Checks team have accomplished is one of the hardest things to do. Their focus on customer needs and nimble execution has served them well, and we’re eager to push ahead in this next phase of Checks,” said Jeanine Banks in a statement.', metadata={'source': '/content/new_articles/05-03-checks-the-ai-powered-data-protection-project-incubated-in-area-120-officially-exits-to-google.txt'}),\n",
       " Document(page_content='Checks is one of those ideas that feels incredibly timely in that it speaks to an issue that’s growing in importance for consumers — who will vote with their feet when they feel that their privacy is in jeopardy. That in turn also puts more pressure on developers to get things right on the privacy front. App publishers these days are faced with a growing array of rules and regulations around data protection and privacy, not just rules like GDPR in Europe and CCPA in California (and the U.S.) set across different countries and jurisdictions, but also by companies that operate platforms within their own compliance efforts.\\n\\nWhen translated into how those regulations impact apps, there are potential issues at the front end, as well as on the back end, with how apps are coded and information moves from one place to another to consider. It’s a spaghetti bowl of issues, with fixes in one area potentially impacting another and making user experience less smooth to boot.', metadata={'source': '/content/new_articles/05-03-checks-the-ai-powered-data-protection-project-incubated-in-area-120-officially-exits-to-google.txt'}),\n",
       " Document(page_content='Checks leans on artificial intelligence and machine learning to scan apps and their code to identify areas where there might be violations of privacy and data protection rules, and provides remediation to suggest how to fix it — tasks that would be far more difficult for a team of humans to execute on their own. It’s already integrated with Google’s large language models and what it describes as “app understanding technologies” to power what it identifies and make suggestions for fixing issues.\\n\\nA dashboard lets users monitor and triage issues in the areas of compliance monitoring, data monitoring and store disclosure support (which is focused specifically on Google Play data safety). With the service also aimed at iOS developers, it’s not clear if it will add Apple App Store data safety at any point into that mix. All of this can be monitored in real time on live apps, as well as when they are still in development.', metadata={'source': '/content/new_articles/05-03-checks-the-ai-powered-data-protection-project-incubated-in-area-120-officially-exits-to-google.txt'}),\n",
       " Document(page_content='We have reached out to Google to get an update on the status of the other two projects that were spared all-out closure after Area 120 changed focus. They include video dubbing solution Aloud and an as-yet unnamed consumer product from the team that had previously built a bookmarking app Liist (which got acquired by Google).\\n\\nAs of right now, Liist’s co-founder David Friedl still describes himself on LinkedIn as working on a stealth product at Area 120, and Aloud is still using an Area 120 URL, so it seems that they remain in a holding pattern. (We’ll update this if and when we hear more.)', metadata={'source': '/content/new_articles/05-03-checks-the-ai-powered-data-protection-project-incubated-in-area-120-officially-exits-to-google.txt'}),\n",
       " Document(page_content='In the meantime, Area 120 itself is also seeing some revolving doors. Clay Bavor, who was running Area 120 among other things and who messaged the big changes to staff in January, was out the door just a month later. He has now teamed up with Bret Taylor — another ex-Googler who has an outsized track record that includes being the CTO of Facebook and the co-CEO of Salesforce — to work on a mystery startup.\\n\\nUpdated with more information about Checks’ valuation and quote from Google.', metadata={'source': '/content/new_articles/05-03-checks-the-ai-powered-data-protection-project-incubated-in-area-120-officially-exits-to-google.txt'}),\n",
       " Document(page_content='Slack has evolved from a pure communications platform to one that enables companies to link directly to enterprise applications without having to resort to dreaded task switching. Today, at the Salesforce World Tour event in NYC, the company announced the next step in its platform’s evolution where it will be putting AI at the forefront of the user experience, making it easier to get information and build workflows.\\n\\nIt’s important to note that these are announcements, and many of these features are not available yet.\\n\\nRob Seaman says that rather than slapping on an AI cover, they are working to incorporate it in a variety of ways across the platform. That started last month with a small step, a partnership with OpenAI to bring a ChatGPT app into Slack, the first piece of a much broader vision for AI on the platform. That part is in beta at the moment.', metadata={'source': '/content/new_articles/05-04-slack-updates-aim-to-put-ai-at-the-center-of-the-user-experience.txt'}),\n",
       " Document(page_content='Today’s announcement involves several new integrations, including SlackGPT, the company’s own flavor of generative AI built on top of the Slack platform, which users and developers can tap into to build AI-driven experiences. The content in Slack provides a starting point for building models related to the platform.\\n\\n“We think Slack has a unique advantage when it comes to generative AI. A lot of the institutional knowledge on every topic, team, work item and project is already in Slack through the messages, the files and the clips that are shared every day,” he said.\\n\\nWhen you combine that with Slack’s Partner ecosystem and platform, customers have a lot of options for integrating AI into their workflows. He says that Slack is thinking about this in three ways right now.', metadata={'source': '/content/new_articles/05-04-slack-updates-aim-to-put-ai-at-the-center-of-the-user-experience.txt'}),\n",
       " Document(page_content='“For starters, Slack is going to bring AI natively into the user experience with SlackGPT to help customers work faster, communicate better, learn faster, etc. And an example of that is AI-powered conversation summaries and writing assistance for composition that’s going to be directly available in Slack,” he said.\\n\\nThe former could as an example help employees get caught up on a long thread without having to read every message to get the gist of what was being discussed. The latter could help generate Slack messages or content for linked Slack applications. That’s a little less obvious use case. It’s probably easier to write a Slack message yourself unless it’s an automated message that’s part of a workflow, but if you are creating content for Slack Canvas, you could let the generative AI help you.', metadata={'source': '/content/new_articles/05-04-slack-updates-aim-to-put-ai-at-the-center-of-the-user-experience.txt'}),\n",
       " Document(page_content='Developers can get in on the action too, building AI steps into workflows, giving them the option of tapping into external apps and large language models to build generative AI experiences themselves. Just last week the company made its updated developer experience generally available, and this should make it easier to incorporate generative AI into the platform in customized ways, Seaman says.\\n\\n“So this gives us the foundation to give users choice and flexibility to bring AI into their work in their business whenever they’re ready, and however they like. We’ve got 2,600 apps in the ecosystem right now, which includes a lot of the leading LLMs, and we see a lot of customers already choosing to integrate generative AI into Slack themselves,” he said.', metadata={'source': '/content/new_articles/05-04-slack-updates-aim-to-put-ai-at-the-center-of-the-user-experience.txt'}),\n",
       " Document(page_content='Finally EinsteinGPT, the Salesforce flavor of generative AI announced in March, will also be incorporated into Slack, letting employees ask questions directly about Salesforce content, like the users most likely to churn or the accounts most likely to buy, and so forth. This is really about more directly integrating Salesforce content into Slack, the company Salesforce paid $27 billion for a couple of years ago.\\n\\n“Slack is really becoming the conversational interface for Salesforce. So that’s bringing those EinsteinGPT-powered insights from the real-time customer data that exists in Salesforce into Slack to enrich every team’s understanding of the customer,” he said.', metadata={'source': '/content/new_articles/05-04-slack-updates-aim-to-put-ai-at-the-center-of-the-user-experience.txt'}),\n",
       " Document(page_content='As with most of the generative AI tooling we’ve seen being added to enterprise software, Slack is announcing these capabilities long before they release them, but this should give customers a sense of what’s coming, and how AI could be transforming Slack in the future. SlackGPT and EinsteinGPT integration are still in the development phase, but developers can build custom integrations with a variety of LLMs, today. Workflow Builder with SlackGPT AI connectors (which will allow customers to instantly connect ChatGPT or Claude to a workflow or build custom connectors that plug in their own LLMs) will be available this summer.', metadata={'source': '/content/new_articles/05-04-slack-updates-aim-to-put-ai-at-the-center-of-the-user-experience.txt'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='The event, which kicks off May 10 at 10 AM PT will be a big showcase for everything that’s on the way for Android 14. The company has, arguably, missed a step when it comes to the current generative AI land rush — hell, who could have predicted after all of these years that Bing would finally have a moment?\\n\\nCEO Sundar Pichai will no doubt be making the case that the company continues to lead the way in the world of artificial intelligence. There’s always been a fair bit of the stuff at the event largely focused on practical real-world applications like mobile imaging and dealing with customer service. This year, however, I’d say it’s safe to say the company is going to go bonkers with the stuff.', metadata={'source': '/content/new_articles/05-05-google-i-o-2023-is-next-week-heres-what-were-expecting.txt'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Hardware, meanwhile, is always a bit of a crapshoot at developer conferences. But after an off-year for the industry at large, a deluge of rumors are aligning, pointing to what’s likely to be an unusually consumer electronics-focused keynote. Given the fact that the last bit is my focus at TechCrunch, I’m going to start the list there.\\n\\nThe Pixel 7a is about as sure as bets get. Google has settled into a comfortable release cadence: releasing a flagship in the fall, followed by a budget device in the spring. The former is designed to be an ideal showcase for its latest mobile operating system and first-party silicon, while the latter makes some compromises for price, while maintaining as many of its predecessors as possible.\\n\\nHow to show excitement without shouting? Asking for a friend Coming to @Flipkart on 11th May. pic.twitter.com/il6GUx3MmR — Google India (@GoogleIndia) May 2, 2023', metadata={'source': '/content/new_articles/05-05-google-i-o-2023-is-next-week-heres-what-were-expecting.txt'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import embeddings\n",
    "persist_directory = 'db'\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=text,\n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persiste the db to disk\n",
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal.\n",
    "vectordb = Chroma(persist_directory=persist_directory,\n",
    "                  embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"How much money did Microsoft raise?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='April 28, 2023\\n\\nVC firms including Sequoia Capital, Andreessen Horowitz, Thrive and K2 Global are picking up new shares, according to documents seen by TechCrunch. A source tells us Founders Fund is also investing. Altogether the VCs have put in just over $300 million at a valuation of $27 billion to $29 billion. This is separate to a big investment from Microsoft announced earlier this year, a person familiar with the development told TechCrunch, which closed in January. The size of Microsoft’s investment is believed to be around $10 billion, a figure we confirmed with our source.\\n\\nApril 25, 2023\\n\\nCalled ChatGPT Business, OpenAI describes the forthcoming offering as “for professionals who need more control over their data as well as enterprises seeking to manage their end users.”', metadata={'source': '/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt'}),\n",
       " Document(page_content='The amount that Google invested in the project was never disclosed, nor was the valuation of the exit to the parent company from the incubator, but the company has confirmed that there was a valuation and that it had grown since launch.\\n\\nThe company is not disclosing how many customers it has in total but notes that they are in the sectors of gaming, health, finance, education and retail. A sampling includes Miniclip, Rovio, Kongregate, Crayola and Yousician and in total the number of customers represented by its customers is over 3 billion.\\n\\nChecks will sit in the Developer X division. “What Fergus, Nia, and the entire Google Checks team have accomplished is one of the hardest things to do. Their focus on customer needs and nimble execution has served them well, and we’re eager to push ahead in this next phase of Checks,” said Jeanine Banks in a statement.', metadata={'source': '/content/new_articles/05-03-checks-the-ai-powered-data-protection-project-incubated-in-area-120-officially-exits-to-google.txt'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'similarity'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever.search_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 2}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever.search_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(client=<class 'openai.api_resources.completion.Completion'>, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the chain to answer questions\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(),\n",
    "                                  chain_type=\"stuff\",\n",
    "                                  retriever=retriever,\n",
    "                                  return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cite sources\n",
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Around $10 billion.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt\n",
      "/content/new_articles/05-03-checks-the-ai-powered-data-protection-project-incubated-in-area-120-officially-exits-to-google.txt\n"
     ]
    }
   ],
   "source": [
    "# full example\n",
    "query = \"How much money did Microsoft raise?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pando has raised $30 million in a Series B round, bringing its total raised to $45 million. The new capital will be put toward expanding Pando’s global sales, marketing and delivery capabilities.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/new_articles/05-03-ai-powered-supply-chain-startup-pando-lands-30m-investment.txt\n",
      "/content/new_articles/05-03-ai-powered-supply-chain-startup-pando-lands-30m-investment.txt\n"
     ]
    }
   ],
   "source": [
    "# break it down\n",
    "query = \"What is the news about Pando?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleteing the DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: db/ (stored 0%)\n",
      "  adding: db/chroma.sqlite3 (deflated 42%)\n",
      "  adding: db/340c51aa-f60a-48b6-b4fd-2fea61654443/ (stored 0%)\n",
      "  adding: db/340c51aa-f60a-48b6-b4fd-2fea61654443/length.bin (deflated 74%)\n",
      "  adding: db/340c51aa-f60a-48b6-b4fd-2fea61654443/data_level0.bin (deflated 100%)\n",
      "  adding: db/340c51aa-f60a-48b6-b4fd-2fea61654443/header.bin (deflated 61%)\n",
      "  adding: db/340c51aa-f60a-48b6-b4fd-2fea61654443/link_lists.bin (stored 0%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r db.zip ./db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To cleanup, you can delete the collection\n",
    "vectordb.delete_collection()\n",
    "vectordb.persist()\n",
    "\n",
    "# delete the directory\n",
    "!rm -rf db/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting again loading the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  db.zip\n",
      "   creating: db/\n",
      "  inflating: db/chroma.sqlite3       \n",
      "   creating: db/340c51aa-f60a-48b6-b4fd-2fea61654443/\n",
      "  inflating: db/340c51aa-f60a-48b6-b4fd-2fea61654443/length.bin  \n",
      "  inflating: db/340c51aa-f60a-48b6-b4fd-2fea61654443/data_level0.bin  \n",
      "  inflating: db/340c51aa-f60a-48b6-b4fd-2fea61654443/header.bin  \n",
      " extracting: db/340c51aa-f60a-48b6-b4fd-2fea61654443/link_lists.bin  \n"
     ]
    }
   ],
   "source": [
    "!unzip db.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weaviate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://console.weaviate.cloud/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting weaviate-client\n",
      "  Downloading weaviate_client-3.25.2-py3-none-any.whl (120 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/120.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m112.6/120.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.3/120.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.30.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (2.31.0)\n",
      "Collecting validators<1.0.0,>=0.21.2 (from weaviate-client)\n",
      "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Collecting authlib<2.0.0,>=1.2.1 (from weaviate-client)\n",
      "  Downloading Authlib-1.2.1-py2.py3-none-any.whl (215 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=3.2 in /usr/local/lib/python3.10/dist-packages (from authlib<2.0.0,>=1.2.1->weaviate-client) (41.0.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (2023.7.22)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.2->authlib<2.0.0,>=1.2.1->weaviate-client) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.2->authlib<2.0.0,>=1.2.1->weaviate-client) (2.21)\n",
      "Installing collected packages: validators, authlib, weaviate-client\n",
      "Successfully installed authlib-1.2.1 validators-0.22.0 weaviate-client-3.25.2\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.0.330-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
      "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.52 (from langchain)\n",
      "  Downloading langsmith-0.0.57-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
      "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.330 langsmith-0.0.57 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
      "Collecting openai\n",
      "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
      "Installing collected packages: openai\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llmx 0.0.15a0 requires cohere, which is not installed.\n",
      "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed openai-0.28.1\n"
     ]
    }
   ],
   "source": [
    "!pip install weaviate-client\n",
    "!pip install langchain\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\"\n",
    "WEAVIATE_API_KEY = \"\"\n",
    "WEAVIATE_CLUSTER = \"https://\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured\n",
      "  Downloading unstructured-0.10.28-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
      "Collecting filetype (from unstructured)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting python-magic (from unstructured)\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.3)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.11.2)\n",
      "Collecting emoji (from unstructured)\n",
      "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.1)\n",
      "Collecting python-iso639 (from unstructured)\n",
      "  Downloading python_iso639-2023.6.15-py3-none-any.whl (275 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.1/275.1 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langdetect (from unstructured)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.23.5)\n",
      "Collecting rapidfuzz (from unstructured)\n",
      "  Downloading rapidfuzz-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting backoff (from unstructured)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.5.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2023.7.22)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=82449007d49f233240cf5be86b735b1ae7083ac0ad3088afcdd55e86ae1a21ad\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
      "Successfully built langdetect\n",
      "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, langdetect, emoji, backoff, unstructured\n",
      "Successfully installed backoff-2.2.1 emoji-2.8.0 filetype-1.2.0 langdetect-1.0.9 python-iso639-2023.6.15 python-magic-0.4.27 rapidfuzz-3.5.2 unstructured-0.10.28\n",
      "Requirement already satisfied: unstructured[pdf] in /usr/local/lib/python3.10/dist-packages (0.10.28)\n",
      "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (5.2.0)\n",
      "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.4.27)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (4.9.3)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (3.8.1)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (4.11.2)\n",
      "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (2.8.0)\n",
      "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.6.1)\n",
      "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (2023.6.15)\n",
      "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.0.9)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.23.5)\n",
      "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (3.5.2)\n",
      "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (4.5.0)\n",
      "Collecting onnx (from unstructured[pdf])\n",
      "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pdf2image (from unstructured[pdf])\n",
      "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
      "Collecting pdfminer.six (from unstructured[pdf])\n",
      "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting unstructured-inference==0.7.10 (from unstructured[pdf])\n",
      "  Downloading unstructured_inference-0.7.10-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting unstructured.pytesseract>=0.3.12 (from unstructured[pdf])\n",
      "  Downloading unstructured.pytesseract-0.3.12-py3-none-any.whl (14 kB)\n",
      "Collecting layoutparser[layoutmodels,tesseract] (from unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-multipart (from unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub (from unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.10->unstructured[pdf]) (4.8.0.76)\n",
      "Collecting onnxruntime<1.16 (from unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers>=4.25.1 (from unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[pdf]) (23.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[pdf]) (9.4.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[pdf]) (2.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[pdf]) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[pdf]) (0.9.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured[pdf]) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (4.66.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx->unstructured[pdf]) (3.20.3)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[pdf]) (3.3.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[pdf]) (41.0.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pdf]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pdf]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pdf]) (2023.7.22)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (1.16.0)\n",
      "Collecting coloredlogs (from onnxruntime<1.16->unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.10->unstructured[pdf]) (23.5.26)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.10->unstructured[pdf]) (1.12)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.10->unstructured[pdf]) (3.12.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.10->unstructured[pdf]) (6.0.1)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers>=4.25.1->unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.25.1->unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured-inference==0.7.10->unstructured[pdf]) (2023.6.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[pdf]) (1.0.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf]) (1.11.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf]) (1.5.3)\n",
      "Collecting iopath (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pdfplumber (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading pdfplumber-0.10.3-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf]) (2.1.0+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf]) (0.16.0+cu118)\n",
      "Collecting effdet (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytesseract (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (2.21)\n",
      "Collecting huggingface-hub (from unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<1.16->unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting timm>=0.9.2 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading timm-0.9.10-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf]) (2.0.7)\n",
      "Collecting omegaconf>=2.0 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf]) (3.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf]) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf]) (2.1.0)\n",
      "Collecting portalocker (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf]) (2023.3.post1)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading pypdfium2-4.23.1-py3-none-manylinux_2_17_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<1.16->unstructured-inference==0.7.10->unstructured[pdf]) (1.3.0)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf])\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf]) (3.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf]) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf]) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf]) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf]) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured[pdf]) (3.1.1)\n",
      "Building wheels for collected packages: iopath, antlr4-python3-runtime\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31530 sha256=d4af088ff361a088ed5e96c438ce9cc8432b8a275d61858de051d67e22baa646\n",
      "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=e58adff98e19e0f4d25d0c8c5a8dde588fb58305915e96284f097f2ae037d798\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "Successfully built iopath antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, unstructured.pytesseract, safetensors, python-multipart, pytesseract, pypdfium2, portalocker, pdf2image, onnx, omegaconf, humanfriendly, iopath, huggingface-hub, coloredlogs, tokenizers, pdfminer.six, onnxruntime, transformers, timm, pdfplumber, layoutparser, effdet, unstructured-inference\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "lida 0.0.10 requires fastapi, which is not installed.\n",
      "lida 0.0.10 requires kaleido, which is not installed.\n",
      "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.9.3 coloredlogs-15.0.1 effdet-0.4.1 huggingface-hub-0.17.3 humanfriendly-10.0 iopath-0.1.10 layoutparser-0.3.4 omegaconf-2.3.0 onnx-1.15.0 onnxruntime-1.15.1 pdf2image-1.16.3 pdfminer.six-20221105 pdfplumber-0.10.3 portalocker-2.8.2 pypdfium2-4.23.1 pytesseract-0.3.10 python-multipart-0.0.6 safetensors-0.4.0 timm-0.9.10 tokenizers-0.14.1 transformers-4.35.0 unstructured-inference-0.7.10 unstructured.pytesseract-0.3.12\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "pydevd_plugins"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install unstructured\n",
    "!pip install \"unstructured[pdf]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\"./data\",glob = \"**/*.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='You Only Look Once (YOLO): Unified, Real-Time Object Detection\\n\\nPresenter: Shivang Singh\\n\\nSept 2nd, 2021\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n1\\n\\nProblem Addressed: Object Detection\\n\\n❖ Object detection is the problem of both\\n\\nlocating AND classifying objects\\n\\n❖ Goal of YOLO algorithm is to do object\\n\\ndetection both fast AND with high\\n\\naccuracy\\n\\n“Deep Learning for Vision Systems” (Elgendy)\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\nObject Detection vs Classification\\n\\n2\\n\\nImportance of Object Detection for Robotics\\n\\n❖ Visual modality is very powerful\\n\\n❖ Humans are able to detect objects and do\\n\\nVision based vs LIDAR (self driving)\\n\\nperception using just this modality in real time\\n\\n(not needing radar)\\n\\n❖ If we want responsive robot systems that\\n\\nwork in real time (without specialized\\n\\nsensors) almost real time vision based object\\n\\ndetection can help greatly\\n\\nTesla Investor Day Presentation\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n3\\n\\nPrevious Object Detection Paradigm This pipeline was used in nearly all SOTA Object Detection prior:\\n\\nLabel + confidence\\n\\nImage Classifier\\n\\nhat - 0.92 racket - 0.2 ball - 0.23\\n\\nStep 1: Scan the image to generate candidate bounding boxes\\n\\nStep 2: Run the bounding box through a classifier\\n\\nStep 3: Conduct post-processing (filtering out redundant bounding boxes)\\n\\nDiagram developed by presenter\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n4\\n\\nKey Insights\\n\\nPrevious Approaches\\n\\n❖ A separate model for generating\\n\\nbounding boxes and for classification\\n\\n(more complicated model pipeline)\\n\\n❖ Need to run classification many\\n\\ntimes (expensive computation)\\n\\n❖ Looks at limited part of the image\\n\\n(lacks contextual information for\\n\\ndetection)\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\nYOLO algorithm\\n\\n❖ A single neural network for\\n\\nlocalization and for classification\\n\\n(less complicated pipeline)\\n\\n❖ Need to inference only once\\n\\n(efficient computation)\\n\\n❖ Looks at the entire image each time\\n\\nleading to less false positives (has\\n\\ncontextual information for detection)\\n\\n5\\n\\nFormal Problem Setting\\n\\n❖ Given an image generate bounding boxes, one for\\n\\neach detectable object in image\\n\\n❖ For each bounding box, output 5 predictions: x, y, w,\\n\\nh, confidence. Also output class\\n\\n❖ x, y (coordinates for center of bounding box)\\n\\n❖ w,h (width and height)\\n\\n❖ confidence (probability bounding box has object)\\n\\n❖ class (classification of object in bounding box)\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n6\\n\\nRelated Work\\n\\nR-CNN or Region Based Convolutional Network (Girshick et al. 2014):\\n\\n\\n\\n-\\n\\nUsed the sliding window approach from earlier, with Selective Search, a smarter way to select candidates (which means there is less computation) Still feeds a limited part of the image to the classifier Drawbacks: Large pipeline, slow, too many false positives\\n\\nFast and Faster R-CNN:\\n\\nOptimize parts of the pipeline described earlier - Drawbacks: loses accuracy\\n\\nDeep Multibox (Szegedy et. al 2014):\\n\\n-\\n\\nTrain a CNN to find areas of interest Drawbacks: Doesn’t address classification only localization\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n7\\n\\nRelated Work\\n\\nMultiGrasp (Redmon et. al 2014)\\n\\nSimilar to YOLO - A much simpler task (only needs to predict object not multiple objects)\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n8\\n\\nYOLO overview\\n\\n❖ First, image is split into a SxS grid ❖ For each grid square, generate B bounding boxes ❖ For each bounding box, there are 5 predictions: x, y, w, h,\\n\\nconfidence\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\nS = 3, B = 2\\n\\n9\\n\\nYOLO Training\\n\\n❖ YOLO is a regression algorithm. What is\\n\\nX? What is Y?\\n\\n❖ X is simple, just an image width (in\\n\\npixels) * height (in pixels) * RGB values ❖ Y is a tensor of size S * S * (B * 5 + C) ❖ B*5 + C term represents the predictions\\n\\n+ class predicted distribution for a grid\\n\\nblock\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\nFor each grid block, we have a\\n\\nvector like this. For this example\\n\\nB is 2 and C is 2\\n\\nGT label\\n\\nexample:\\n\\n10\\n\\nYOLO Architecture\\n\\nNow that we know the input and output, we can discuss the model\\n\\nWe are given 448 by 448 by 3 as\\n\\n\\n\\nour input. Implementation uses 7 convolution layers\\n\\nPaper parameters: S = 7, B = 2,\\n\\nC = 20\\n\\nOutput is S*S*(5B+C) = 7*7*(5*2+20) = 7*7*30\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n11\\n\\nYOLO Prediction\\n\\n❖ We then use the output to make final detections ❖ Use a threshold to filter out bounding boxes with\\n\\nlow P(Object)\\n\\n❖ In order to know the class for the bounding box\\n\\ncompute score take argmax over the distribution\\n\\nPr(Class|Object) for the grid the bounding box’s\\n\\ncenter is in\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n12\\n\\nNon-maximal suppression\\n\\n❖ Most of the time objects fall in one grid,\\n\\nhowever it is still possible to get redundant boxes (rare case as object must be close to multiple grid cells for this to happen) ❖ Discard bounding box with high overlap (keeping the bounding box with highest confidence)\\n\\n❖ Adds 2-3% on final mAP score\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n13\\n\\nYOLO Objective Function\\n\\n❖ For YOLO, we need to minimize the following loss ❖ Sum squared error is used\\n\\nCoordinate Loss: Minimize the difference between x,y,w,h pred and x,y,w,h ground truth. ONLY IF object exists in grid box and if bounding box is resp for pred\\n\\nConfidence Loss: Loss based on confidence ONLY IF there is object\\n\\nNo Object Loss based on confidence if there is no object\\n\\nClass loss, minimize loss between true class of object in grid box\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n14\\n\\nExperimental Setup\\n\\n❖ Authors compare YOLO against the previous work described above on PASCAL VOC 2007, and\\n\\nVOC 2012 as well as out of domain art dataset\\n\\n❖ Correct if IOU metric above .5 and class is correct\\n\\n❖ Use two performance metrics:\\n\\n➢ mAP score: mean average precision\\n\\n➢ FPS:\\n\\nframes per second\\n\\n❖ Add FAST YOLO: which has less parameters\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n15\\n\\nExperimental Results\\n\\n❖ Baseline YOLO outperform\\n\\nreal time detectors by large\\n\\namount\\n\\n❖ Do better than most less than\\n\\nreal time as well\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n16\\n\\nExperimental Results\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n17\\n\\nExperimental Results - Error Analysis Exper\\n\\nMakes far less background errors (less likely to predict false positives on background) IOU is VERY small with any ground truth label\\n\\n\\n\\nBut far more localization errors\\n\\n\\n\\nCorrect class, IOU is somewhat small\\n\\nLocalization error\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\nBackground error\\n\\n18\\n\\nExperimental Results - Out of Domain\\n\\n❖ Ran YOLO + competitors\\n\\n(trained on natural images) on art\\n\\n❖ Does well on artistic datasets where more having global context greatly helps\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n19\\n\\nDiscussion of Results\\n\\n❖ Pro: YOLO is a lot faster than the other algorithms for image detection\\n\\n❖ Pro: YOLO’s use of global information rather than only local information allows it to understand\\n\\ncontextual information when doing object detection\\n\\n➢ Does better in domains such as artwork due to this\\n\\n❖ Con: YOLO lagged behind the SOTA models in object detection\\n\\n➢ This is attributed to making many localization errors and unable to detect small object\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n20\\n\\nCritique / Limitations / Open Issues\\n\\n❖ Performance lags behind SOTA\\n\\n❖ Requires data to be labeled with bounding boxes, hard to collect for many classes\\n\\n➢ Previous work could generalize better since it used image classifier\\n\\n➢ 2014 COCO dataset (very large dataset) addressed this somewhat\\n\\n❖ Regarding experiments: number of classes predicted is very limited\\n\\n➢ Not convinced that YOLO v1 is generalizable\\n\\n❖ Confidence output of YOLO not confidence of class but P(Object), lowers interpretability\\n\\n❖ Another limitation of YOLO is that it imposed spatial constraints on the objects in the image since\\n\\nonly B boxes can be predicted on an SxS grid\\n\\n❖ Since the architecture only predicts boxes, this might make it less useful for irregular shapes\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n21\\n\\nFuture Work for Paper / Reading\\n\\n❖ One extension of this work would be to look\\n\\nat image segmentation and see if the insights\\n\\ncarry over\\n\\nYOLOACT (Boyla et al 2019): Real\\n\\ntime image segmentation ❖ YOLO has been upgraded 2 times\\n\\nSolves a lot of issues relating to\\n\\ndetecting small objects,\\n\\ngeneralizability, and localization\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\nYOLOACT example\\n\\n22\\n\\nExtended Readings\\n\\n❖ YOLO v2 (https://arxiv.org/abs/1506.02640) (extends on the work greatly) (Redmond et al 2016)\\n\\n➢ Deals with the generalizability problem, has 9000 classes\\n\\n➢ Class probability distribution per bounding box, not per grid\\n\\n➢ High resolution classifier (finetune on high resolution)\\n\\n➢ Batch norm\\n\\n➢ Trained on MSCOCO (released after YOLO v1 paper)\\n\\n❖ YOLO v3 (https://arxiv.org/abs/1804.02767)\\n\\n➢ “Incremental Improvement”\\n\\n➢ Uses independent logistic classifiers for class\\n\\n■ Allows for more specificity in classes\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n23\\n\\nSummary\\n\\n❖ Object detection is the problem of detecting multiple objects in an image ❖ Almost real time object detection can make highly responsive robot systems without complex sensors ❖ Prior work relies on a large architecture with numerous parts to optimize ❖ YOLO proposes a unified architecture, which does all the tasks in one model and by one inference\\n\\nover the entire image\\n\\n❖ They show enormous speed improvement and show that they can beat most other prior work in terms\\n\\nof mAPs\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n24', metadata={'source': 'data/yolo.pdf'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='You Only Look Once (YOLO): Unified, Real-Time Object Detection\\n\\nPresenter: Shivang Singh\\n\\nSept 2nd, 2021\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n1\\n\\nProblem Addressed: Object Detection\\n\\n❖ Object detection is the problem of both\\n\\nlocating AND classifying objects\\n\\n❖ Goal of YOLO algorithm is to do object\\n\\ndetection both fast AND with high\\n\\naccuracy\\n\\n“Deep Learning for Vision Systems” (Elgendy)\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\nObject Detection vs Classification\\n\\n2\\n\\nImportance of Object Detection for Robotics\\n\\n❖ Visual modality is very powerful\\n\\n❖ Humans are able to detect objects and do\\n\\nVision based vs LIDAR (self driving)\\n\\nperception using just this modality in real time\\n\\n(not needing radar)\\n\\n❖ If we want responsive robot systems that\\n\\nwork in real time (without specialized\\n\\nsensors) almost real time vision based object\\n\\ndetection can help greatly\\n\\nTesla Investor Day Presentation\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n3', metadata={'source': 'data/yolo.pdf'}),\n",
       " Document(page_content='3\\n\\nPrevious Object Detection Paradigm This pipeline was used in nearly all SOTA Object Detection prior:\\n\\nLabel + confidence\\n\\nImage Classifier\\n\\nhat - 0.92 racket - 0.2 ball - 0.23\\n\\nStep 1: Scan the image to generate candidate bounding boxes\\n\\nStep 2: Run the bounding box through a classifier\\n\\nStep 3: Conduct post-processing (filtering out redundant bounding boxes)\\n\\nDiagram developed by presenter\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n4\\n\\nKey Insights\\n\\nPrevious Approaches\\n\\n❖ A separate model for generating\\n\\nbounding boxes and for classification\\n\\n(more complicated model pipeline)\\n\\n❖ Need to run classification many\\n\\ntimes (expensive computation)\\n\\n❖ Looks at limited part of the image\\n\\n(lacks contextual information for\\n\\ndetection)\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\nYOLO algorithm\\n\\n❖ A single neural network for\\n\\nlocalization and for classification\\n\\n(less complicated pipeline)\\n\\n❖ Need to inference only once\\n\\n(efficient computation)\\n\\n❖ Looks at the entire image each time', metadata={'source': 'data/yolo.pdf'}),\n",
       " Document(page_content='leading to less false positives (has\\n\\ncontextual information for detection)\\n\\n5\\n\\nFormal Problem Setting\\n\\n❖ Given an image generate bounding boxes, one for\\n\\neach detectable object in image\\n\\n❖ For each bounding box, output 5 predictions: x, y, w,\\n\\nh, confidence. Also output class\\n\\n❖ x, y (coordinates for center of bounding box)\\n\\n❖ w,h (width and height)\\n\\n❖ confidence (probability bounding box has object)\\n\\n❖ class (classification of object in bounding box)\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n6\\n\\nRelated Work\\n\\nR-CNN or Region Based Convolutional Network (Girshick et al. 2014):\\n\\n\\n\\n-\\n\\nUsed the sliding window approach from earlier, with Selective Search, a smarter way to select candidates (which means there is less computation) Still feeds a limited part of the image to the classifier Drawbacks: Large pipeline, slow, too many false positives\\n\\nFast and Faster R-CNN:\\n\\nOptimize parts of the pipeline described earlier - Drawbacks: loses accuracy\\n\\nDeep Multibox (Szegedy et. al 2014):\\n\\n-', metadata={'source': 'data/yolo.pdf'}),\n",
       " Document(page_content='-\\n\\nTrain a CNN to find areas of interest Drawbacks: Doesn’t address classification only localization\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n7\\n\\nRelated Work\\n\\nMultiGrasp (Redmon et. al 2014)\\n\\nSimilar to YOLO - A much simpler task (only needs to predict object not multiple objects)\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n8\\n\\nYOLO overview\\n\\n❖ First, image is split into a SxS grid ❖ For each grid square, generate B bounding boxes ❖ For each bounding box, there are 5 predictions: x, y, w, h,\\n\\nconfidence\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\nS = 3, B = 2\\n\\n9\\n\\nYOLO Training\\n\\n❖ YOLO is a regression algorithm. What is\\n\\nX? What is Y?\\n\\n❖ X is simple, just an image width (in\\n\\npixels) * height (in pixels) * RGB values ❖ Y is a tensor of size S * S * (B * 5 + C) ❖ B*5 + C term represents the predictions\\n\\n+ class predicted distribution for a grid\\n\\nblock\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\nFor each grid block, we have a\\n\\nvector like this. For this example\\n\\nB is 2 and C is 2\\n\\nGT label\\n\\nexample:\\n\\n10', metadata={'source': 'data/yolo.pdf'}),\n",
       " Document(page_content='example:\\n\\n10\\n\\nYOLO Architecture\\n\\nNow that we know the input and output, we can discuss the model\\n\\nWe are given 448 by 448 by 3 as\\n\\n\\n\\nour input. Implementation uses 7 convolution layers\\n\\nPaper parameters: S = 7, B = 2,\\n\\nC = 20\\n\\nOutput is S*S*(5B+C) = 7*7*(5*2+20) = 7*7*30\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n11\\n\\nYOLO Prediction\\n\\n❖ We then use the output to make final detections ❖ Use a threshold to filter out bounding boxes with\\n\\nlow P(Object)\\n\\n❖ In order to know the class for the bounding box\\n\\ncompute score take argmax over the distribution\\n\\nPr(Class|Object) for the grid the bounding box’s\\n\\ncenter is in\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n12\\n\\nNon-maximal suppression\\n\\n❖ Most of the time objects fall in one grid,\\n\\nhowever it is still possible to get redundant boxes (rare case as object must be close to multiple grid cells for this to happen) ❖ Discard bounding box with high overlap (keeping the bounding box with highest confidence)\\n\\n❖ Adds 2-3% on final mAP score', metadata={'source': 'data/yolo.pdf'}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)\\n\\n13\\n\\nYOLO Objective Function\\n\\n❖ For YOLO, we need to minimize the following loss ❖ Sum squared error is used\\n\\nCoordinate Loss: Minimize the difference between x,y,w,h pred and x,y,w,h ground truth. ONLY IF object exists in grid box and if bounding box is resp for pred\\n\\nConfidence Loss: Loss based on confidence ONLY IF there is object\\n\\nNo Object Loss based on confidence if there is no object\\n\\nClass loss, minimize loss between true class of object in grid box\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n14\\n\\nExperimental Setup\\n\\n❖ Authors compare YOLO against the previous work described above on PASCAL VOC 2007, and\\n\\nVOC 2012 as well as out of domain art dataset\\n\\n❖ Correct if IOU metric above .5 and class is correct\\n\\n❖ Use two performance metrics:\\n\\n➢ mAP score: mean average precision\\n\\n➢ FPS:\\n\\nframes per second\\n\\n❖ Add FAST YOLO: which has less parameters\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n15\\n\\nExperimental Results\\n\\n❖ Baseline YOLO outperform', metadata={'source': 'data/yolo.pdf'}),\n",
       " Document(page_content='real time detectors by large\\n\\namount\\n\\n❖ Do better than most less than\\n\\nreal time as well\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n16\\n\\nExperimental Results\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n17\\n\\nExperimental Results - Error Analysis Exper\\n\\nMakes far less background errors (less likely to predict false positives on background) IOU is VERY small with any ground truth label\\n\\n\\n\\nBut far more localization errors\\n\\n\\n\\nCorrect class, IOU is somewhat small\\n\\nLocalization error\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\nBackground error\\n\\n18\\n\\nExperimental Results - Out of Domain\\n\\n❖ Ran YOLO + competitors\\n\\n(trained on natural images) on art\\n\\n❖ Does well on artistic datasets where more having global context greatly helps\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n19\\n\\nDiscussion of Results\\n\\n❖ Pro: YOLO is a lot faster than the other algorithms for image detection\\n\\n❖ Pro: YOLO’s use of global information rather than only local information allows it to understand\\n\\ncontextual information when doing object detection', metadata={'source': 'data/yolo.pdf'}),\n",
       " Document(page_content='➢ Does better in domains such as artwork due to this\\n\\n❖ Con: YOLO lagged behind the SOTA models in object detection\\n\\n➢ This is attributed to making many localization errors and unable to detect small object\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n20\\n\\nCritique / Limitations / Open Issues\\n\\n❖ Performance lags behind SOTA\\n\\n❖ Requires data to be labeled with bounding boxes, hard to collect for many classes\\n\\n➢ Previous work could generalize better since it used image classifier\\n\\n➢ 2014 COCO dataset (very large dataset) addressed this somewhat\\n\\n❖ Regarding experiments: number of classes predicted is very limited\\n\\n➢ Not convinced that YOLO v1 is generalizable\\n\\n❖ Confidence output of YOLO not confidence of class but P(Object), lowers interpretability\\n\\n❖ Another limitation of YOLO is that it imposed spatial constraints on the objects in the image since\\n\\nonly B boxes can be predicted on an SxS grid\\n\\n❖ Since the architecture only predicts boxes, this might make it less useful for irregular shapes', metadata={'source': 'data/yolo.pdf'}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)\\n\\n21\\n\\nFuture Work for Paper / Reading\\n\\n❖ One extension of this work would be to look\\n\\nat image segmentation and see if the insights\\n\\ncarry over\\n\\nYOLOACT (Boyla et al 2019): Real\\n\\ntime image segmentation ❖ YOLO has been upgraded 2 times\\n\\nSolves a lot of issues relating to\\n\\ndetecting small objects,\\n\\ngeneralizability, and localization\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\nYOLOACT example\\n\\n22\\n\\nExtended Readings\\n\\n❖ YOLO v2 (https://arxiv.org/abs/1506.02640) (extends on the work greatly) (Redmond et al 2016)\\n\\n➢ Deals with the generalizability problem, has 9000 classes\\n\\n➢ Class probability distribution per bounding box, not per grid\\n\\n➢ High resolution classifier (finetune on high resolution)\\n\\n➢ Batch norm\\n\\n➢ Trained on MSCOCO (released after YOLO v1 paper)\\n\\n❖ YOLO v3 (https://arxiv.org/abs/1804.02767)\\n\\n➢ “Incremental Improvement”\\n\\n➢ Uses independent logistic classifiers for class\\n\\n■ Allows for more specificity in classes\\n\\nCS391R: Robot Learning (Fall 2021)', metadata={'source': 'data/yolo.pdf'}),\n",
       " Document(page_content='23\\n\\nSummary\\n\\n❖ Object detection is the problem of detecting multiple objects in an image ❖ Almost real time object detection can make highly responsive robot systems without complex sensors ❖ Prior work relies on a large architecture with numerous parts to optimize ❖ YOLO proposes a unified architecture, which does all the tasks in one model and by one inference\\n\\nover the entire image\\n\\n❖ They show enormous speed improvement and show that they can beat most other prior work in terms\\n\\nof mAPs\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n24', metadata={'source': 'data/yolo.pdf'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key= OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base='', openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization='', allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6, request_timeout=None, headers=None, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Database Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from langchain.vectorstores import Weaviate\n",
    "\n",
    "#Connect to weaviate Cluster\n",
    "auth_config = weaviate.auth.AuthApiKey(api_key = WEAVIATE_API_KEY)\n",
    "WEAVIATE_URL = WEAVIATE_CLUSTER\n",
    "\n",
    "client = weaviate.Client(\n",
    "    url = WEAVIATE_URL,\n",
    "    additional_headers = {\"X-OpenAI-Api-key\": OPENAI_API_KEY},\n",
    "    auth_client_secret = auth_config,\n",
    "    startup_period = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input structure\n",
    "client.schema.delete_all()\n",
    "client.schema.get()\n",
    "schema = {\n",
    "    \"classes\": [\n",
    "        {\n",
    "            \"class\": \"Chatbot\",\n",
    "            \"description\": \"Documents for chatbot\",\n",
    "            \"vectorizer\": \"text2vec-openai\",\n",
    "            \"moduleConfig\": {\"text2vec-openai\": {\"model\": \"ada\", \"type\": \"text\"}},\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"dataType\": [\"text\"],\n",
    "                    \"description\": \"The content of the paragraph\",\n",
    "                    \"moduleConfig\": {\n",
    "                        \"text2vec-openai\": {\n",
    "                            \"skip\": False,\n",
    "                            \"vectorizePropertyName\": False,\n",
    "                        }\n",
    "                    },\n",
    "                    \"name\": \"content\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "client.schema.create(schema)\n",
    "vectorstore = Weaviate(client, \"Chatbot\", \"content\", attributes=[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9c496404-7515-4b3c-8b8e-88e2dc09dfc8',\n",
       " '0c6883df-8242-4011-a51d-13efd4d6dd39',\n",
       " '3ad7f0e4-d5b9-4ffc-9fb0-1d4a4b31f610',\n",
       " '4845d89b-4e65-4ec3-ac46-e182793e9b68',\n",
       " '32df9a02-fdf1-4e71-81b5-1103264f7499',\n",
       " '6e2be198-6bb7-4374-8bf8-bc3f42aefd17',\n",
       " '5074cbc0-870c-416e-a37c-63a8f28a06bd',\n",
       " '59fd3d13-29f3-4001-af59-7fc48a4e1bec',\n",
       " 'cb52baa4-bd78-4491-801e-21b074f6aa8d',\n",
       " '631ccd1a-e35d-4eb4-94ae-8beb3c1f99e4']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load text into the vectorstore\n",
    "text_meta_pair = [(doc.page_content, doc.metadata) for doc in docs]\n",
    "texts, meta = list(zip(*text_meta_pair))\n",
    "vectorstore.add_texts(texts, meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is a yolo?\"\n",
    "\n",
    "# retrieve text related to the query\n",
    "docs = vectorstore.similarity_search(query, top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='You Only Look Once (YOLO): Unified, Real-Time Object Detection\\n\\nPresenter: Shivang Singh\\n\\nSept 2nd, 2021\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n1\\n\\nProblem Addressed: Object Detection\\n\\n❖ Object detection is the problem of both\\n\\nlocating AND classifying objects\\n\\n❖ Goal of YOLO algorithm is to do object\\n\\ndetection both fast AND with high\\n\\naccuracy\\n\\n“Deep Learning for Vision Systems” (Elgendy)\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\nObject Detection vs Classification\\n\\n2\\n\\nImportance of Object Detection for Robotics\\n\\n❖ Visual modality is very powerful\\n\\n❖ Humans are able to detect objects and do\\n\\nVision based vs LIDAR (self driving)\\n\\nperception using just this modality in real time\\n\\n(not needing radar)\\n\\n❖ If we want responsive robot systems that\\n\\nwork in real time (without specialized\\n\\nsensors) almost real time vision based object\\n\\ndetection can help greatly\\n\\nTesla Investor Day Presentation\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n3', metadata={'source': 'data/yolo.pdf'}),\n",
       " Document(page_content='23\\n\\nSummary\\n\\n❖ Object detection is the problem of detecting multiple objects in an image ❖ Almost real time object detection can make highly responsive robot systems without complex sensors ❖ Prior work relies on a large architecture with numerous parts to optimize ❖ YOLO proposes a unified architecture, which does all the tasks in one model and by one inference\\n\\nover the entire image\\n\\n❖ They show enormous speed improvement and show that they can beat most other prior work in terms\\n\\nof mAPs\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n24', metadata={'source': 'data/yolo.pdf'}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)\\n\\n13\\n\\nYOLO Objective Function\\n\\n❖ For YOLO, we need to minimize the following loss ❖ Sum squared error is used\\n\\nCoordinate Loss: Minimize the difference between x,y,w,h pred and x,y,w,h ground truth. ONLY IF object exists in grid box and if bounding box is resp for pred\\n\\nConfidence Loss: Loss based on confidence ONLY IF there is object\\n\\nNo Object Loss based on confidence if there is no object\\n\\nClass loss, minimize loss between true class of object in grid box\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n14\\n\\nExperimental Setup\\n\\n❖ Authors compare YOLO against the previous work described above on PASCAL VOC 2007, and\\n\\nVOC 2012 as well as out of domain art dataset\\n\\n❖ Correct if IOU metric above .5 and class is correct\\n\\n❖ Use two performance metrics:\\n\\n➢ mAP score: mean average precision\\n\\n➢ FPS:\\n\\nframes per second\\n\\n❖ Add FAST YOLO: which has less parameters\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n15\\n\\nExperimental Results\\n\\n❖ Baseline YOLO outperform', metadata={'source': 'data/yolo.pdf'}),\n",
       " Document(page_content='➢ Does better in domains such as artwork due to this\\n\\n❖ Con: YOLO lagged behind the SOTA models in object detection\\n\\n➢ This is attributed to making many localization errors and unable to detect small object\\n\\nCS391R: Robot Learning (Fall 2021)\\n\\n20\\n\\nCritique / Limitations / Open Issues\\n\\n❖ Performance lags behind SOTA\\n\\n❖ Requires data to be labeled with bounding boxes, hard to collect for many classes\\n\\n➢ Previous work could generalize better since it used image classifier\\n\\n➢ 2014 COCO dataset (very large dataset) addressed this somewhat\\n\\n❖ Regarding experiments: number of classes predicted is very limited\\n\\n➢ Not convinced that YOLO v1 is generalizable\\n\\n❖ Confidence output of YOLO not confidence of class but P(Object), lowers interpretability\\n\\n❖ Another limitation of YOLO is that it imposed spatial constraints on the objects in the image since\\n\\nonly B boxes can be predicted on an SxS grid\\n\\n❖ Since the architecture only predicts boxes, this might make it less useful for irregular shapes', metadata={'source': 'data/yolo.pdf'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define chain\n",
    "chain = load_qa_chain(\n",
    "    OpenAI(openai_api_key = OPENAI_API_KEY,temperature=0),\n",
    "    chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' YOLO is an algorithm for object detection that is unified, real-time, and has high accuracy. It is presented by Shivang Singh in the CS391R: Robot Learning (Fall 2021) course on Sept 2nd, 2021.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create answer\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_faiss = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Simple similarity\n",
    "faiss_q1 = db_faiss.similarity_search(query1)\n",
    "print(faiss_q1[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_query1 = db_faiss.similarity_search_with_score(query1)\n",
    "similar_query1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_q2 = db_faiss.similarity_search(query2)\n",
    "chain.run(input_documents = faiss_q2, question = query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
